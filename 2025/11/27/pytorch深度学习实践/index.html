<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>Pytorch深度学习实践 | klklkl's blogs</title><meta name="author" content="klklkl"><meta name="copyright" content="klklkl"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="《PyTorch深度学习实践》完结合集 PyTorch深度学习快速入门教程  线性模型 穷举法 12import numpy as npimport matplotlib.pyplot as plt 12x_data &#x3D; [1.0, 2.0, 3.0]y_data &#x3D; [2.0, 4.0, 6.0] 1234567def forward(x):    return w*xdef loss(x,y">
<meta property="og:type" content="article">
<meta property="og:title" content="Pytorch深度学习实践">
<meta property="og:url" content="https://klklkl10086.github.io/klklkl10086.github.io/2025/11/27/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/index.html">
<meta property="og:site_name" content="klklkl&#39;s blogs">
<meta property="og:description" content="《PyTorch深度学习实践》完结合集 PyTorch深度学习快速入门教程  线性模型 穷举法 12import numpy as npimport matplotlib.pyplot as plt 12x_data &#x3D; [1.0, 2.0, 3.0]y_data &#x3D; [2.0, 4.0, 6.0] 1234567def forward(x):    return w*xdef loss(x,y">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pic2.zhimg.com/v2-f9b32b95385eedbdb58389bbbce5de39_r.jpg">
<meta property="article:published_time" content="2025-11-27T14:15:20.000Z">
<meta property="article:modified_time" content="2025-12-07T09:18:58.852Z">
<meta property="article:author" content="klklkl">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="Pytorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pic2.zhimg.com/v2-f9b32b95385eedbdb58389bbbce5de39_r.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://klklkl10086.github.io/klklkl10086.github.io/2025/11/27/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: {"limitDay":500,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Pytorch深度学习实践',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-12-07 17:18:58'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="klklkl's blogs" type="application/atom+xml">
<link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://pic2.zhimg.com/v2-f9b32b95385eedbdb58389bbbce5de39_r.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">22</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">26</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">8</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://pic1.zhimg.com/80/v2-908b61a41ec4bebe17a04468dcf5d834_720w.webp')"><nav id="nav"><span id="blog-info"><a href="/" title="klklkl's blogs"><img class="site-icon" src="https://pic2.zhimg.com/v2-f9b32b95385eedbdb58389bbbce5de39_r.jpg# image"/><span class="site-name">klklkl's blogs</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Pytorch深度学习实践</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-11-27T14:15:20.000Z" title="发表于 2025-11-27 22:15:20">2025-11-27</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-12-07T09:18:58.852Z" title="更新于 2025-12-07 17:18:58">2025-12-07</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%A1%A5%E5%AE%8C/">研究生补完</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Pytorch深度学习实践"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Y7411d7Ys/?p=3&amp;share_source=copy_web&amp;vd_source=777aaa8a415b68222e598d976e64642c">《PyTorch深度学习实践》完结合集</a></p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1hE411t7RN/?p=22&amp;share_source=copy_web&amp;vd_source=777aaa8a415b68222e598d976e64642c">PyTorch深度学习快速入门教程</a></p>
</blockquote>
<h1 id="线性模型">线性模型</h1>
<h2 id="穷举法">穷举法</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> w*x</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">x,y</span>):</span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred-y)**<span class="number">2</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">w_list = []</span><br><span class="line">mse_list = []</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> np.arange(<span class="number">0.0</span>,<span class="number">4.1</span>,<span class="number">0.1</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;w=&quot;</span>,w)</span><br><span class="line">    l_sum=<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x_val,y_val <span class="keyword">in</span> <span class="built_in">zip</span>(x_data,y_data):</span><br><span class="line">        y_pred_val = forward(x_val)</span><br><span class="line">        loss_val = loss(x_val,y_val)</span><br><span class="line">        l_sum+=loss_val</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;\t&#x27;</span>,x_val,y_val,y_pred_val,loss_val)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;MSE=&#x27;</span>,l_sum/<span class="number">3</span>)</span><br><span class="line">    w_list.append(w)</span><br><span class="line">    mse_list.append(l_sum/<span class="number">3</span>)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">plt.plot(w_list,mse_list)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;Loss&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;w&#x27;</span>)</span><br><span class="line">plt.show() </span><br></pre></td></tr></table></figure>
<pre><code>&lt;IPython.core.display.Javascript object&gt;
</code></pre>
<div id='dfa97de9-b458-411d-8eb9-4a902ee3d358'></div>
<h2 id="课后作业">课后作业</h2>
<p>作业题目：实现线性模型（y=wx+b）并输出loss的3D图像。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline  <span class="comment">#告诉Jupyter Notebook在输出单元格中内嵌显示matplotlib绘制的图形，而不是在一个单独的窗口中打开。</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> w*x+b</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">x,y</span>):</span><br><span class="line">    y_pred = forward(x)</span><br><span class="line">    <span class="keyword">return</span> (y_pred-y)**<span class="number">2</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设y=3x+2</span></span><br><span class="line">x_data = [<span class="number">1.0</span>,<span class="number">2.0</span>,<span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">5.0</span>,<span class="number">8.0</span>,<span class="number">11.0</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">w_list=np.arange(<span class="number">0</span>,<span class="number">4.1</span>,<span class="number">0.1</span>)</span><br><span class="line">b_list=np.arange(<span class="number">0</span>,<span class="number">4.1</span>,<span class="number">0.1</span>)</span><br><span class="line"></span><br><span class="line">[w,b]=np.meshgrid(w_list,b_list)<span class="comment">#创建二维网格,w和b均为二维数组</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#广播机制 计算</span></span><br><span class="line">loss_sum=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> x_val,y_val <span class="keyword">in</span> <span class="built_in">zip</span>(x_data,y_data):</span><br><span class="line">    y_pred_val = forward(x_val)</span><br><span class="line">    loss_sum+=loss(x_val,y_val)</span><br><span class="line"></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(loss_sum)</span><br></pre></td></tr></table></figure>
<pre><code>[[210.   199.34 188.96 ...   1.76   1.74   2.  ]
 [205.23 194.69 184.43 ...   1.55   1.65   2.03]
 [200.52 190.1  179.96 ...   1.4    1.62   2.12]
 ...
 [ 70.92  64.82  59.   ...  35.96  40.5   45.32]
 [ 68.43  62.45  56.75 ...  38.03  42.69  47.63]
 [ 66.    60.14  54.56 ...  40.16  44.94  50.  ]]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">fig = plt.figure()</span><br><span class="line">ax = fig.add_subplot(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>, projection=<span class="string">&#x27;3d&#x27;</span>)  <span class="comment"># 推荐的方法</span></span><br><span class="line">ax.plot_surface(w, b, loss_sum/<span class="number">3</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">&quot;w&quot;</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">&quot;b&quot;</span>)</span><br><span class="line">ax.set_zlabel(<span class="string">&quot;loss&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="01_%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B_15_0.png" alt="png"></p>
<h1 id="梯度下降">梯度下降</h1>
<p>只能找到局部最优,无法确保找到全局最优</p>
<p>鞍点问题比较重要</p>
<h2 id="批量梯度下降">批量梯度下降</h2>
<ol>
<li>使用整个训练集计算梯度</li>
<li>每次迭代稳定但计算量大</li>
<li>适合小型数据集</li>
</ol>
<h2 id="随机梯度下降">随机梯度下降</h2>
<ol>
<li>每次使用单个样本</li>
<li>收敛快但震荡大</li>
<li>适合在线学习</li>
</ol>
<h2 id="小批量梯度下降">小批量梯度下降</h2>
<p>上述两种的折中</p>
<ol>
<li>平衡了计算效率和稳定性</li>
<li>最常用的方法</li>
<li>batch_size 是重要超参数</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">w = <span class="number">1.0</span> <span class="comment">#随机初始化权重</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 批量梯度下降 </span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> w*x</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cost</span>(<span class="params">xs,ys</span>):</span><br><span class="line">    cost = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x,y <span class="keyword">in</span> <span class="built_in">zip</span>(xs,ys):</span><br><span class="line">        cost+=(y-forward(x))**<span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> cost/<span class="built_in">len</span>(xs)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient</span>(<span class="params">xs,ys</span>):</span><br><span class="line">    grad = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> x,y <span class="keyword">in</span> <span class="built_in">zip</span>(xs,ys):</span><br><span class="line">        grad+=<span class="number">2</span>*(x*w-y)*x</span><br><span class="line">    <span class="keyword">return</span> grad/<span class="built_in">len</span>(xs)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">epoch_list=[]</span><br><span class="line">cost_list=[]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;predict (before training)&#x27;</span>,<span class="number">4</span>,forward(<span class="number">4</span>))</span><br></pre></td></tr></table></figure>
<pre><code>predict (before training) 4 4.0
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    cost_val = cost(x_data,y_data) <span class="comment">#计算整个批次的平均损失</span></span><br><span class="line">    grad_val = gradient(x_data,y_data)<span class="comment">#计算整个批次的平均梯度</span></span><br><span class="line">    w -=<span class="number">0.01</span>*grad_val</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;epoch:&#x27;</span>, epoch, <span class="string">&#x27;w=&#x27;</span>, w, <span class="string">&#x27;loss=&#x27;</span>, cost_val)</span><br><span class="line">    epoch_list.append(epoch)</span><br><span class="line">    cost_list.append(cost_val)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;predict (after training)&#x27;</span>, <span class="number">4</span>, forward(<span class="number">4</span>))</span><br><span class="line">plt.plot(epoch_list,cost_list)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;cost&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;epoch&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<hr>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#随机梯度下降</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> w*x</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">x,y</span>):</span><br><span class="line">    <span class="keyword">return</span> (y-forward(x))**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gradient</span>(<span class="params">x,y</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">2</span>*x*(w*x-y)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">w=<span class="number">1</span></span><br><span class="line">epoch_list=[]</span><br><span class="line">cost_list=[]</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;predict (before training)&#x27;</span>,<span class="number">4</span>,forward(<span class="number">4</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="keyword">for</span> x,y <span class="keyword">in</span> <span class="built_in">zip</span>(x_data,y_data):</span><br><span class="line">        y_hat=forward(x)</span><br><span class="line">        loss_val=loss(x,y)</span><br><span class="line">        grad_val=gradient(x,y)</span><br><span class="line">        w-=<span class="number">0.01</span>*grad_val</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;\tgrad:&quot;</span>, x, y,grad_val)</span><br><span class="line">    epoch_list.append(epoch)</span><br><span class="line">    cost_list.append(loss_val)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;progress:&quot;</span>,epoch,<span class="string">&quot;w=&quot;</span>,w,<span class="string">&quot;loss=&quot;</span>,loss)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;predict (after training)&#x27;</span>, <span class="number">4</span>, forward(<span class="number">4</span>))</span><br><span class="line">plt.plot(epoch_list,cost_list)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;loss&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;epoch&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h1 id="反向传播">反向传播</h1>
<h2 id="tensor类与计算图">Tensor类与计算图</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">w = torch.tensor([<span class="number">1.0</span>])</span><br><span class="line">w.requires_grad=<span class="literal">True</span> <span class="comment"># 需要计算梯度</span></span><br><span class="line"><span class="built_in">print</span>(w)</span><br><span class="line"><span class="built_in">print</span>(w.data)</span><br><span class="line"><span class="built_in">print</span>(w.<span class="built_in">type</span>())<span class="comment">#Tensor类</span></span><br><span class="line"><span class="built_in">print</span>(w.data.<span class="built_in">type</span>())<span class="comment">#data也是一个Tensor</span></span><br><span class="line"><span class="built_in">print</span>(w.grad)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(w.grad))</span><br></pre></td></tr></table></figure>
<pre><code>tensor([1.], requires_grad=True)
tensor([1.])
torch.FloatTensor
torch.FloatTensor
None
&lt;class 'NoneType'&gt;
</code></pre>
<h2 id="反向传播">反向传播</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">w = torch.tensor([<span class="number">1.0</span>])</span><br><span class="line">w.requires_grad = <span class="literal">True</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> w*x</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">x,y</span>):</span><br><span class="line">    <span class="keyword">return</span> (y-forward(x))**<span class="number">2</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="string">&quot;predict (before training)&quot;</span>, <span class="number">4</span>, forward(<span class="number">4</span>).item())<span class="comment"># 张量转为数字</span></span><br></pre></td></tr></table></figure>
<pre><code>predict (before training) 4 4.0
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="keyword">for</span> x,y <span class="keyword">in</span> <span class="built_in">zip</span>(x_data,y_data):</span><br><span class="line">        l = loss(x,y)<span class="comment">#前向传播</span></span><br><span class="line">        l.backward()<span class="comment">#反向传播</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;\tgrad:&#x27;</span>, x, y, w.grad.item())</span><br><span class="line">        w.data = w.data - <span class="number">0.01</span>*w.grad.data</span><br><span class="line">        w.grad.data.zero_()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;progress:&#x27;</span>, epoch, l.item())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;predict (after training)&quot;</span>, <span class="number">4</span>, forward(<span class="number">4</span>).item())</span><br></pre></td></tr></table></figure>
<h2 id="作业">作业</h2>
<p>以y = w*x+b 为例子实现反向传播</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">w = torch.Tensor([<span class="number">1.0</span>])</span><br><span class="line">w.requires_grad=<span class="literal">True</span></span><br><span class="line">b = torch.Tensor([<span class="number">0</span>])</span><br><span class="line">b.requires_grad=<span class="literal">True</span></span><br><span class="line"></span><br><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> w*x+b</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">x,y</span>):</span><br><span class="line">    <span class="keyword">return</span> (y-forward(x))**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="keyword">for</span> x,y <span class="keyword">in</span> <span class="built_in">zip</span>(x_data,y_data):</span><br><span class="line">        l = loss(x,y)</span><br><span class="line">        l.backward()</span><br><span class="line">        w.data=w.data-<span class="number">0.01</span>*w.grad.data</span><br><span class="line">        b.data=b.data-<span class="number">0.01</span>*b.grad.data</span><br><span class="line">        w.grad.data.zero_()</span><br><span class="line">        b.grad.data.zero_()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;progress:&#x27;</span>, epoch, l.item())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;predict (after training)&quot;</span>, <span class="number">4</span>, forward(<span class="number">4</span>).item())</span><br><span class="line">        </span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<hr>
<p>y=w1x²+w2x+b为例子实现反向传播</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">w1 = torch.Tensor([<span class="number">1.0</span>])</span><br><span class="line">w1.requires_grad=<span class="literal">True</span></span><br><span class="line">w2 = torch.Tensor([<span class="number">1.0</span>])</span><br><span class="line">w2.requires_grad=<span class="literal">True</span></span><br><span class="line">b = torch.Tensor([<span class="number">0</span>])</span><br><span class="line">b.requires_grad=<span class="literal">True</span></span><br><span class="line"></span><br><span class="line">x_data = [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span><br><span class="line">y_data = [<span class="number">2.0</span>, <span class="number">4.0</span>, <span class="number">6.0</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">x</span>):</span><br><span class="line">    <span class="keyword">return</span> w1*x*x+w2*x+b</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">x,y</span>):</span><br><span class="line">    <span class="keyword">return</span> (y-forward(x))**<span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="keyword">for</span> x,y <span class="keyword">in</span> <span class="built_in">zip</span>(x_data,y_data):</span><br><span class="line">        l = loss(x,y)</span><br><span class="line">        l.backward()</span><br><span class="line">        w1.data=w1.data-<span class="number">0.01</span>*w1.grad.data</span><br><span class="line">        w2.data=w2.data-<span class="number">0.01</span>*w2.grad.data</span><br><span class="line">        b.data=b.data-<span class="number">0.01</span>*b.grad.data</span><br><span class="line">        w1.grad.data.zero_()</span><br><span class="line">        w2.grad.data.zero_()</span><br><span class="line">        b.grad.data.zero_()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&#x27;progress:&#x27;</span>, epoch, l.item())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;predict (after training)&quot;</span>, <span class="number">4</span>, forward(<span class="number">4</span>).item())</span><br><span class="line">        </span><br><span class="line">    </span><br></pre></td></tr></table></figure>
<h1 id="pytorch实现线性回归">Pytorch实现线性回归</h1>
<h2 id="init与call函数">init与call函数</h2>
<ol>
<li><strong>init</strong>: 类的初始化函数，类似于c++的构造函数</li>
<li><strong>call</strong>_: 使得类对象具有类似函数的功能。</li>
</ol>
<h3 id="init"><strong>init</strong></h3>
<p>作用：用于初始化模型对象，定义模型的结构和参数。</p>
<ol>
<li>定义网络层：在这里，你声明模型将由哪些层组成（例如，线性层 nn.Linear、卷积层 nn.Conv2d、激活函数 nn.ReLU 等）。</li>
<li>初始化参数：这些层内部的权重（weights）和偏置（biases）参数会被自动创建并初始化。</li>
<li>执行必要的设置：调用父类的 <strong>init</strong>() 方法等。</li>
<li><strong>init</strong> 负责搭建模型的骨架，它回答了&quot;这个模型由哪些部分组成？&quot;的问题。它只会在你创建模型实例时被调用一次。</li>
</ol>
<h3 id="call"><strong>call</strong></h3>
<p>作用:让模型实例能够像函数一样被调用，例如 output = model(input_data)。</p>
<ol>
<li>nn.Module 是所有神经网络单元（neural network modules）的基类,pytorch在nn.Module中，实现了__call__方法，而在__call__方法中调用了forward函数。</li>
<li>你不需要自己重写 <strong>call</strong> 方法。</li>
<li>你必须自己定义 forward 方法。 forward 才是真正定义数据如何在前向传播中流动的地方。</li>
<li><strong>call</strong> 是 forward 的封装器和触发器。永远使用 model(x) 而不是直接调用 model.forward(x)，因为前者能确保 PyTorch 的所有机制（如钩子）正常工作。</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyNeuralNetwork</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, input_size, hidden_size, output_size</span>):</span><br><span class="line">        <span class="built_in">super</span>(MyNeuralNetwork, self).__init__()</span><br><span class="line">        self.layer1 = torch.nn.Linear(input_size, hidden_size)</span><br><span class="line">        self.activation = torch.nn.ReLU()</span><br><span class="line">        self.layer2 = torch.nn.Linear(hidden_size, output_size)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 定义数据的前向传播路径</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x 是输入数据</span></span><br><span class="line">        x = self.layer1(x)       <span class="comment"># 数据通过第一层</span></span><br><span class="line">        x = self.activation(x)   <span class="comment"># 数据通过激活函数</span></span><br><span class="line">        x = self.layer2(x)       <span class="comment"># 数据通过第二层</span></span><br><span class="line">        <span class="keyword">return</span> x                 <span class="comment"># 返回最终输出</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用模型</span></span><br><span class="line">model = MyNeuralNetwork(input_size=<span class="number">784</span>, hidden_size=<span class="number">128</span>, output_size=<span class="number">10</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一些虚拟输入数据 (batch_size=4, feature_size=784)</span></span><br><span class="line">dummy_input = torch.randn(<span class="number">4</span>, <span class="number">784</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 将数据传入模型进行前向传播</span></span><br><span class="line"><span class="comment"># 这里触发了 __call__，进而调用了你定义的 forward 方法</span></span><br><span class="line">output = model(dummy_input)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(output.shape) <span class="comment"># 输出：torch.Size([4, 10])</span></span><br></pre></td></tr></table></figure>
<pre><code>torch.Size([4, 10])
</code></pre>
<h2 id="nn-module中的api">nn.module中的API</h2>
<ol>
<li>loss.backward()：计算梯度,即进行反向传播。</li>
<li>optimizer.step()：执行更新（“实施修改”）。</li>
<li>optimizer.zero_grad()：准备下一次计算（“重置记录”）。</li>
</ol>
<h2 id="实现线性回归">实现线性回归</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">x_data = torch.tensor([[<span class="number">1.0</span>],[<span class="number">2.0</span>],[<span class="number">3.0</span>]])<span class="comment"># 3*1</span></span><br><span class="line">y_data = torch.tensor([[<span class="number">2.0</span>],[<span class="number">4.0</span>],[<span class="number">6.0</span>]])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LinearModel</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(LinearModel,self).__init__()<span class="comment">#调用父类 nn.Module 的初始化方法</span></span><br><span class="line">        self.linear = torch.nn.Linear(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">        <span class="comment">#创建一个线性层（全连接层）</span></span><br><span class="line">        <span class="comment"># (1,1)是指输入x和输出y的特征维度，这里数据集中的x和y的特征都是1维的</span></span><br><span class="line">        <span class="comment"># 该线性层需要学习的参数是w和b  获取w/b的方式分别是~linear.weight/linear.bias</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        y_pred = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">model = LinearModel()</span><br><span class="line"></span><br><span class="line">criterion = torch.nn.MSELoss(reduction=<span class="string">&#x27;sum&#x27;</span>) <span class="comment">#损失函数</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(),lr=<span class="number">0.01</span>)<span class="comment">#优化器</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#训练过程</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    y_pred = model(x_data)</span><br><span class="line">    loss = criterion(y_pred,y_data)</span><br><span class="line">    <span class="built_in">print</span>(epoch,loss.item())</span><br><span class="line"></span><br><span class="line">    loss.backward()       <span class="comment">#计算梯度</span></span><br><span class="line">    optimizer.step()  <span class="comment">#更新w和b的值</span></span><br><span class="line">    optimizer.zero_grad()  <span class="comment">#清空</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;w=&#x27;</span>,model.linear.weight.item())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;b=&#x27;</span>,model.linear.bias.item())</span><br><span class="line"></span><br><span class="line">x_test = torch.tensor([[<span class="number">4.0</span>]])</span><br><span class="line">y_test = model(x_test)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;y_pred = &#x27;</span>, y_test.data)</span><br></pre></td></tr></table></figure>
<h2 id="常用的优化器与损失函数">常用的优化器与损失函数</h2>
<h3 id="回归任务损失函数">回归任务损失函数</h3>
<h4 id="mseloss-均方误差损失"><strong>MSELoss（均方误差损失）</strong></h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 基本用法</span></span><br><span class="line">criterion = nn.MSELoss()  <span class="comment"># reduction=&#x27;mean&#x27; 为默认值</span></span><br><span class="line"><span class="comment"># 或者指定 reduction</span></span><br><span class="line">criterion = nn.MSELoss(reduction=<span class="string">&#x27;mean&#x27;</span>)  <span class="comment"># 平均值</span></span><br><span class="line">criterion = nn.MSELoss(reduction=<span class="string">&#x27;sum&#x27;</span>)   <span class="comment"># 求和</span></span><br><span class="line">criterion = nn.MSELoss(reduction=<span class="string">&#x27;none&#x27;</span>)  <span class="comment"># 不聚合，返回每个样本的损失</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用示例</span></span><br><span class="line">y_pred = torch.tensor([[<span class="number">2.0</span>], [<span class="number">4.0</span>], [<span class="number">6.0</span>], [<span class="number">8.0</span>]])</span><br><span class="line">y_true = torch.tensor([[<span class="number">1.8</span>], [<span class="number">4.2</span>], [<span class="number">5.9</span>], [<span class="number">8.1</span>]])</span><br><span class="line"></span><br><span class="line">loss = criterion(y_pred, y_true)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;MSE Loss: <span class="subst">&#123;loss.item()&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数学公式: loss = (1/n) * Σ(y_pred - y_true)²</span></span><br></pre></td></tr></table></figure>
<h4 id="l1loss-平均绝对误差损失"><strong>L1Loss（平均绝对误差损失）</strong></h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.L1Loss()  <span class="comment"># MAE损失</span></span><br><span class="line"><span class="comment"># 可选参数: reduction=&#x27;mean&#x27; | &#x27;sum&#x27; | &#x27;none&#x27;</span></span><br><span class="line"></span><br><span class="line">loss = criterion(y_pred, y_true)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;L1 Loss: <span class="subst">&#123;loss.item()&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数学公式: loss = (1/n) * Σ|y_pred - y_true|</span></span><br><span class="line"><span class="comment"># 特点：对异常值不如MSE敏感</span></span><br></pre></td></tr></table></figure>
<h4 id="smoothl1loss-平滑l1损失"><strong>SmoothL1Loss（平滑L1损失）</strong></h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.SmoothL1Loss()</span><br><span class="line"><span class="comment"># 可选参数: reduction=&#x27;mean&#x27; | &#x27;sum&#x27; | &#x27;none&#x27;, beta=1.0</span></span><br><span class="line"></span><br><span class="line">loss = criterion(y_pred, y_true)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Smooth L1 Loss: <span class="subst">&#123;loss.item()&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数学公式: </span></span><br><span class="line"><span class="comment"># 如果 |y_pred - y_true| &lt; beta: 0.5 * (y_pred - y_true)² / beta</span></span><br><span class="line"><span class="comment"># 否则: |y_pred - y_true| - 0.5 * beta</span></span><br><span class="line"><span class="comment"># 特点：在0附近更平滑，常用于目标检测（如Faster R-CNN）</span></span><br></pre></td></tr></table></figure>
<h4 id="huberloss"><strong>HuberLoss</strong></h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.HuberLoss()</span><br><span class="line"><span class="comment"># 可选参数: reduction=&#x27;mean&#x27; | &#x27;sum&#x27; | &#x27;none&#x27;, delta=1.0</span></span><br><span class="line"></span><br><span class="line">loss = criterion(y_pred, y_true)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Huber Loss: <span class="subst">&#123;loss.item()&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数学公式与SmoothL1Loss类似，是L1和L2损失的结合</span></span><br></pre></td></tr></table></figure>
<h3 id="分类任务损失函数">分类任务损失函数</h3>
<h4 id="bceloss-二分类交叉熵损失"><strong>BCELoss（二分类交叉熵损失）</strong></h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用于二分类问题，输出需要经过sigmoid激活</span></span><br><span class="line">criterion = nn.BCELoss()</span><br><span class="line"><span class="comment"># 可选参数: reduction=&#x27;mean&#x27; | &#x27;sum&#x27; | &#x27;none&#x27;, weight=None</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用示例（输出经过sigmoid，值在0-1之间）</span></span><br><span class="line">y_pred = torch.tensor([[<span class="number">0.9</span>], [<span class="number">0.1</span>], [<span class="number">0.8</span>], [<span class="number">0.2</span>]])  <span class="comment"># 预测概率</span></span><br><span class="line">y_true = torch.tensor([[<span class="number">1.0</span>], [<span class="number">0.0</span>], [<span class="number">1.0</span>], [<span class="number">0.0</span>]])  <span class="comment"># 真实标签</span></span><br><span class="line"></span><br><span class="line">loss = criterion(y_pred, y_true)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;BCE Loss: <span class="subst">&#123;loss.item()&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数学公式: loss = -[y_true * log(y_pred) + (1-y_true) * log(1-y_pred)]</span></span><br></pre></td></tr></table></figure>
<h4 id="bcewithlogitsloss-带logits的bce损失"><strong>BCEWithLogitsLoss（带logits的BCE损失）</strong></h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 推荐使用，数值稳定性更好</span></span><br><span class="line">criterion = nn.BCEWithLogitsLoss()</span><br><span class="line"><span class="comment"># 可选参数: reduction=&#x27;mean&#x27; | &#x27;sum&#x27; | &#x27;none&#x27;, weight=None, pos_weight=None</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用示例（输出不需要sigmoid）</span></span><br><span class="line">y_pred_logits = torch.tensor([[<span class="number">2.0</span>], [-<span class="number">2.0</span>], [<span class="number">1.5</span>], [-<span class="number">1.5</span>]])  <span class="comment"># 原始logits</span></span><br><span class="line">y_true = torch.tensor([[<span class="number">1.0</span>], [<span class="number">0.0</span>], [<span class="number">1.0</span>], [<span class="number">0.0</span>]])</span><br><span class="line"></span><br><span class="line">loss = criterion(y_pred_logits, y_true)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;BCEWithLogits Loss: <span class="subst">&#123;loss.item()&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># pos_weight参数用于处理类别不平衡</span></span><br><span class="line">pos_weight = torch.tensor([<span class="number">3.0</span>])  <span class="comment"># 正样本权重</span></span><br><span class="line">criterion_balanced = nn.BCEWithLogitsLoss(pos_weight=pos_weight)</span><br></pre></td></tr></table></figure>
<h4 id="crossentropyloss-多分类交叉熵损失"><strong>CrossEntropyLoss（多分类交叉熵损失）</strong></h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用于多分类问题，输出不需要softmax</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line"><span class="comment"># 可选参数: reduction=&#x27;mean&#x27; | &#x27;sum&#x27; | &#x27;none&#x27;, weight=None, ignore_index=-100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用示例（3分类问题，4个样本）</span></span><br><span class="line">y_pred = torch.tensor([</span><br><span class="line">    [<span class="number">2.0</span>, <span class="number">1.0</span>, <span class="number">0.1</span>],  <span class="comment"># 预测为第0类</span></span><br><span class="line">    [<span class="number">0.5</span>, <span class="number">2.0</span>, <span class="number">0.3</span>],  <span class="comment"># 预测为第1类  </span></span><br><span class="line">    [<span class="number">0.2</span>, <span class="number">0.1</span>, <span class="number">3.0</span>],  <span class="comment"># 预测为第2类</span></span><br><span class="line">    [<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">0.5</span>]   <span class="comment"># 预测为第1类</span></span><br><span class="line">])</span><br><span class="line">y_true = torch.tensor([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">1</span>])  <span class="comment"># 真实类别索引</span></span><br><span class="line"></span><br><span class="line">loss = criterion(y_pred, y_true)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;CrossEntropy Loss: <span class="subst">&#123;loss.item()&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># weight参数用于处理类别不平衡</span></span><br><span class="line">class_weights = torch.tensor([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">1.0</span>])  <span class="comment"># 第1类的权重为2</span></span><br><span class="line">criterion_weighted = nn.CrossEntropyLoss(weight=class_weights)</span><br></pre></td></tr></table></figure>
<h4 id="nllloss-负对数似然损失"><strong>NLLLoss（负对数似然损失）</strong></h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 需要与LogSoftmax配合使用</span></span><br><span class="line">criterion = nn.NLLLoss()</span><br><span class="line"><span class="comment"># 可选参数: reduction=&#x27;mean&#x27; | &#x27;sum&#x27; | &#x27;none&#x27;, weight=None, ignore_index=-100</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用示例（输出需要先经过LogSoftmax）</span></span><br><span class="line">log_softmax = nn.LogSoftmax(dim=<span class="number">1</span>)</span><br><span class="line">y_pred_log = log_softmax(y_pred)  <span class="comment"># 先取log softmax</span></span><br><span class="line">loss = criterion(y_pred_log, y_true)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;NLL Loss: <span class="subst">&#123;loss.item()&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="其他损失函数">其他损失函数</h3>
<h4 id="cosineembeddingloss-余弦相似度损失"><strong>CosineEmbeddingLoss（余弦相似度损失）</strong></h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.CosineEmbeddingLoss()</span><br><span class="line"><span class="comment"># 可选参数: margin=0.0, reduction=&#x27;mean&#x27; | &#x27;sum&#x27; | &#x27;none&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用示例（用于度量学习、相似度计算）</span></span><br><span class="line">x1 = torch.tensor([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>]])</span><br><span class="line">x2 = torch.tensor([[<span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>]])  <span class="comment"># 相同向量</span></span><br><span class="line">target = torch.tensor([<span class="number">1</span>, <span class="number">1</span>])  <span class="comment"># 1表示相似，-1表示不相似</span></span><br><span class="line"></span><br><span class="line">loss = criterion(x1, x2, target)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Cosine Embedding Loss: <span class="subst">&#123;loss.item()&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h4 id="tripletmarginloss-三元组损失"><strong>TripletMarginLoss（三元组损失）</strong></h4>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">criterion = nn.TripletMarginLoss()</span><br><span class="line"><span class="comment"># 可选参数: margin=1.0, p=2, eps=1e-6, reduction=&#x27;mean&#x27; | &#x27;sum&#x27; | &#x27;none&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用示例（用于度量学习）</span></span><br><span class="line">anchor = torch.tensor([[<span class="number">1.0</span>, <span class="number">2.0</span>]])</span><br><span class="line">positive = torch.tensor([[<span class="number">1.1</span>, <span class="number">2.1</span>]])  <span class="comment"># 与anchor相似</span></span><br><span class="line">negative = torch.tensor([[<span class="number">5.0</span>, <span class="number">6.0</span>]])  <span class="comment"># 与anchor不相似</span></span><br><span class="line"></span><br><span class="line">loss = criterion(anchor, positive, negative)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;Triplet Margin Loss: <span class="subst">&#123;loss.item()&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="sgd-随机梯度下降">SGD（随机梯度下降）</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># 基本SGD</span></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 带动量的SGD（推荐）</span></span><br><span class="line">optimizer = optim.SGD(</span><br><span class="line">    model.parameters(), </span><br><span class="line">    lr=<span class="number">0.01</span>, </span><br><span class="line">    momentum=<span class="number">0.9</span>,</span><br><span class="line">    weight_decay=<span class="number">1e-4</span>,  <span class="comment"># L2正则化</span></span><br><span class="line">    nesterov=<span class="literal">False</span>      <span class="comment"># 是否使用Nesterov动量</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用示例</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">    <span class="keyword">for</span> inputs, targets <span class="keyword">in</span> dataloader:</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        outputs = model(inputs)</span><br><span class="line">        loss = criterion(outputs, targets)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br></pre></td></tr></table></figure>
<h3 id="adam-自适应矩估计">Adam（自适应矩估计）</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 基本Adam</span></span><br><span class="line">optimizer = optim.Adam(model.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 完整参数Adam</span></span><br><span class="line">optimizer = optim.Adam(</span><br><span class="line">    model.parameters(),</span><br><span class="line">    lr=<span class="number">0.001</span>,           <span class="comment"># 学习率</span></span><br><span class="line">    betas=(<span class="number">0.9</span>, <span class="number">0.999</span>), <span class="comment"># 一阶和二阶矩估计的衰减率</span></span><br><span class="line">    eps=<span class="number">1e-8</span>,           <span class="comment"># 数值稳定性常数</span></span><br><span class="line">    weight_decay=<span class="number">0</span>,     <span class="comment"># L2正则化</span></span><br><span class="line">    amsgrad=<span class="literal">False</span>       <span class="comment"># 是否使用AMSGrad变体</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 特点：自适应学习率，通常收敛快，适合大多数情况</span></span><br></pre></td></tr></table></figure>
<h3 id="adamw-adam-with-weight-decay">AdamW（Adam with Weight Decay）</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Adam的改进版本，正确处理权重衰减</span></span><br><span class="line">optimizer = optim.AdamW(</span><br><span class="line">    model.parameters(),</span><br><span class="line">    lr=<span class="number">0.001</span>,</span><br><span class="line">    betas=(<span class="number">0.9</span>, <span class="number">0.999</span>),</span><br><span class="line">    eps=<span class="number">1e-8</span>,</span><br><span class="line">    weight_decay=<span class="number">0.01</span>,  <span class="comment"># 真正的权重衰减（不是L2正则化）</span></span><br><span class="line">    amsgrad=<span class="literal">False</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 特点：比Adam有更好的泛化能力，推荐使用</span></span><br></pre></td></tr></table></figure>
<h3 id="rmsprop">RMSprop</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optim.RMSprop(</span><br><span class="line">    model.parameters(),</span><br><span class="line">    lr=<span class="number">0.01</span>,</span><br><span class="line">    alpha=<span class="number">0.99</span>,         <span class="comment"># 平滑常数</span></span><br><span class="line">    eps=<span class="number">1e-8</span>,</span><br><span class="line">    weight_decay=<span class="number">0</span>,</span><br><span class="line">    momentum=<span class="number">0</span>,         <span class="comment"># 动量因子</span></span><br><span class="line">    centered=<span class="literal">False</span>      <span class="comment"># 是否计算中心化的梯度方差</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 特点：适合非平稳目标，常用于RNN</span></span><br></pre></td></tr></table></figure>
<h3 id="adagrad">Adagrad</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optim.Adagrad(</span><br><span class="line">    model.parameters(),</span><br><span class="line">    lr=<span class="number">0.01</span>,</span><br><span class="line">    lr_decay=<span class="number">0</span>,         <span class="comment"># 学习率衰减</span></span><br><span class="line">    weight_decay=<span class="number">0</span>,</span><br><span class="line">    initial_accumulator_value=<span class="number">0</span>,</span><br><span class="line">    eps=<span class="number">1e-10</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 特点：自适应学习率，适合稀疏数据</span></span><br></pre></td></tr></table></figure>
<h3 id="adadelta">Adadelta</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optim.Adadelta(</span><br><span class="line">    model.parameters(),</span><br><span class="line">    lr=<span class="number">1.0</span>,             <span class="comment"># 默认1.0</span></span><br><span class="line">    rho=<span class="number">0.9</span>,            <span class="comment"># 衰减率</span></span><br><span class="line">    eps=<span class="number">1e-6</span>,</span><br><span class="line">    weight_decay=<span class="number">0</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 特点：不需要手动设置学习率，是Adagrad的改进</span></span><br></pre></td></tr></table></figure>
<h3 id="adamax">Adamax</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optim.Adamax(</span><br><span class="line">    model.parameters(),</span><br><span class="line">    lr=<span class="number">0.002</span>,</span><br><span class="line">    betas=(<span class="number">0.9</span>, <span class="number">0.999</span>),</span><br><span class="line">    eps=<span class="number">1e-8</span>,</span><br><span class="line">    weight_decay=<span class="number">0</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 特点：Adam的基于无穷范数的变体</span></span><br></pre></td></tr></table></figure>
<h3 id="损失函数选择指南：">损失函数选择指南：</h3>
<ul>
<li><strong>回归任务</strong>: <code>MSELoss</code>, <code>L1Loss</code>, <code>SmoothL1Loss</code></li>
<li><strong>二分类</strong>: <code>BCEWithLogitsLoss</code>（推荐）, <code>BCELoss</code></li>
<li><strong>多分类</strong>: <code>CrossEntropyLoss</code></li>
<li><strong>相似度学习</strong>: <code>CosineEmbeddingLoss</code>, <code>TripletMarginLoss</code></li>
</ul>
<h3 id="优化器选择指南：">优化器选择指南：</h3>
<ul>
<li><strong>推荐默认</strong>: <code>AdamW</code> 或 <code>Adam</code></li>
<li><strong>需要精细调参</strong>: <code>SGD</code> + momentum</li>
<li><strong>RNN/LSTM</strong>: <code>RMSprop</code></li>
<li><strong>稀疏数据</strong>: <code>Adagrad</code></li>
</ul>
<h1 id="逻辑回归">逻辑回归</h1>
<ol>
<li>在线性层的后面加上了激活函数(非线性变换)</li>
<li>解决分类问题</li>
<li>分类问题一般采用交叉熵作为损失函数,交叉熵越小说明二者越接近,分类越好</li>
</ol>
<p><strong>BCELoss 是二分类交叉熵损失，而 CrossEntropyLoss 是交叉熵损失，它更通用，可以同时处理二分类和多分类问题。</strong></p>
<p><strong>核心区别对比表</strong></p>
<table>
<thead>
<tr>
<th style="text-align:left">特性</th>
<th style="text-align:left">BCELoss</th>
<th style="text-align:left">CrossEntropyLoss</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>全称</strong></td>
<td style="text-align:left">Binary Cross-Entropy Loss</td>
<td style="text-align:left">Cross-Entropy Loss</td>
</tr>
<tr>
<td style="text-align:left"><strong>任务类型</strong></td>
<td style="text-align:left"><strong>二分类</strong></td>
<td style="text-align:left"><strong>多分类</strong>，也可用于二分类</td>
</tr>
<tr>
<td style="text-align:left"><strong>输出层要求</strong></td>
<td style="text-align:left">每个类别一个** Sigmoid 函数**</td>
<td style="text-align:left">整个输出层一个 <strong>Softmax 函数</strong></td>
</tr>
<tr>
<td style="text-align:left"><strong>模型输出含义</strong></td>
<td style="text-align:left">每个输出节点是一个<strong>独立的概率</strong>，表示属于该类的可能性。所有节点概率之和不等于1。</td>
<td style="text-align:left">所有输出节点经过Softmax，表示一个<strong>概率分布</strong>。所有节点概率之和为1。</td>
</tr>
<tr>
<td style="text-align:left"><strong>标签格式</strong></td>
<td style="text-align:left"><strong>浮点数</strong>（Float），通常是0或1。可以是单个值（单标签）或多个值（多标签）。</td>
<td style="text-align:left"><strong>整数</strong>（Long），是类别的索引。例如，3分类任务，标签是0, 1, 2。</td>
</tr>
<tr>
<td style="text-align:left"><strong>损失计算</strong></td>
<td style="text-align:left">对<strong>每个输出节点</strong>分别计算二元交叉熵，然后可以求平均或求和。</td>
<td style="text-align:left">将模型输出的<strong>整个分布</strong>与真实标签的 one-hot 编码计算交叉熵。</td>
</tr>
<tr>
<td style="text-align:left"><strong>PyTorch 实现</strong></td>
<td style="text-align:left"><code>torch.nn.BCELoss()</code></td>
<td style="text-align:left"><code>torch.nn.CrossEntropyLoss()</code></td>
</tr>
</tbody>
</table>
<hr>
<h2 id="bceloss">BCELoss</h2>
<p>BCELoss 用于<strong>二分类</strong>或<strong>多标签分类</strong>问题。在二分类中，虽然只有一个输出节点（例如，用Sigmoid输出一个概率值），但它也可以有多个输出节点，每个节点代表一个独立的“是/否”问题。</p>
<p><strong>核心思想</strong>：对输出的<strong>每一个维度（节点）</strong> 都应用一个 Sigmoid 函数，将其压缩到 (0, 1) 区间，然后与目标值（0或1）计算二元交叉熵。</p>
<p><strong>计算公式（对于单个样本的一个输出节点）</strong>： <code>Loss = - [y * log(p) + (1 - y) * log(1 - p)]</code> 其中 <code>y</code> 是真实标签（0或1），<code>p</code> 是模型预测的概率。</p>
<p><strong>PyTorch 示例</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假设一个3标签的分类问题（多标签分类）</span></span><br><span class="line"><span class="comment"># 模型有3个输出节点，每个节点用Sigmoid激活</span></span><br><span class="line">model_output = torch.tensor([[ <span class="number">0.8</span>, -<span class="number">0.6</span>,  <span class="number">1.2</span>]]) <span class="comment"># 原始logits</span></span><br><span class="line">sigmoid = nn.Sigmoid()</span><br><span class="line">probabilities = sigmoid(model_output) <span class="comment"># 转换为概率，如 [0.69, 0.35, 0.77]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 真实标签：第一个标签为1，第二个为0，第三个为1（可以同时为1）</span></span><br><span class="line">true_labels = torch.tensor([[<span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">1.0</span>]])</span><br><span class="line"></span><br><span class="line">criterion = nn.BCELoss()</span><br><span class="line">loss = criterion(probabilities, true_labels)</span><br><span class="line"><span class="built_in">print</span>(loss)</span><br></pre></td></tr></table></figure>
<p><strong>注意</strong>：在使用 <code>nn.BCELoss</code> 之前，<strong>必须</strong>手动使用 Sigmoid 将输出转换为概率。PyTorch 也提供了 <code>nn.BCEWithLogitsLoss</code>，它结合了 Sigmoid 和 BCELoss，数值计算上更稳定，是更推荐的做法。</p>
<h2 id="crossentropyloss">CrossEntropyLoss</h2>
<p>CrossEntropyLoss 是深度学习中<strong>最常用</strong>的分类损失函数，主要用于<strong>单标签多分类</strong>问题（一个样本只属于一个类别）。</p>
<p><strong>核心思想</strong>：对输出的<strong>整个向量</strong>应用 Softmax 函数，将其转换为一个概率分布（所有值之和为1），然后计算这个预测分布与真实分布（one-hot 编码）的交叉熵。</p>
<p><strong>关键点</strong>：</p>
<ul>
<li><strong>内部已包含 Softmax</strong>：你不需要在模型的最后一层显式地添加 Softmax。<code>nn.CrossEntropyLoss</code> 会在内部自动为你计算 Softmax。</li>
<li><strong>标签是类别索引</strong>：你不需要将标签手动转换为 one-hot 编码，直接提供类别索引（整数）即可。</li>
</ul>
<p><strong>计算公式（对于单个样本）</strong>： <code>Loss = - log( p_class )</code> 其中 <code>p_class</code> 是模型预测出的、在<strong>真实类别</strong>上的那个概率。</p>
<p><strong>PyTorch 示例</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设一个3分类问题</span></span><br><span class="line"><span class="comment"># 模型有3个输出节点，不需要最后一层用Softmax</span></span><br><span class="line">model_output = torch.tensor([[ <span class="number">2.0</span>,  <span class="number">1.0</span>, -<span class="number">0.5</span>]]) <span class="comment"># 原始logits</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 真实标签：这个样本属于第0类（注意是LongTensor，是索引值，不是one-hot）</span></span><br><span class="line">true_label = torch.tensor([<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">loss = criterion(model_output, true_label)</span><br><span class="line"><span class="built_in">print</span>(loss)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 上面的计算等价于：</span></span><br><span class="line"><span class="comment"># 1. 对model_output做Softmax： [0.70, 0.26, 0.04]</span></span><br><span class="line"><span class="comment"># 2. 取真实类别0对应的概率：0.70</span></span><br><span class="line"><span class="comment"># 3. 计算损失：-log(0.70) ≈ 0.356</span></span><br></pre></td></tr></table></figure>
<hr>
<h2 id="如何选择？">如何选择？</h2>
<ul>
<li><strong>如果你的问题是标准的单标签多分类</strong>（如图像分类到10个类别、手写数字识别等）：
<ul>
<li><strong>毫不犹豫地使用 <code>CrossEntropyLoss</code></strong>。</li>
<li>模型最后一层<strong>不要</strong>用任何激活函数（如 Softmax），直接输出 logits。</li>
</ul>
</li>
<li><strong>如果你的问题是二分类或多标签分类</strong>（如判断一张图片中是否同时包含“猫”和“狗”，一个样本可以有多个标签）：
<ul>
<li>使用 <code>BCEWithLogitsLoss</code>（或 <code>BCELoss</code> + 手动Sigmoid）。</li>
<li>模型的输出节点数等于标签数，每个节点使用 Sigmoid 函数。</li>
</ul>
</li>
<li><strong>如果你的问题是二分类，但想把它当作多分类来处理</strong>：
<ul>
<li>你也可以使用 <code>CrossEntropyLoss</code>，此时模型需要<strong>两个输出节点</strong>（分别对应“类别0”和“类别1”）。</li>
<li>标签相应地是0或1。</li>
<li>这种方法与使用 <code>BCEWithLogitsLoss</code> 和一个输出节点在数学上是等价的，但具体实现和数值稳定性上可能有细微差别。通常 <code>BCEWithLogitsLoss</code> 对于二分类更直接。</li>
</ul>
</li>
</ul>
<h2 id="总结">总结</h2>
<table>
<thead>
<tr>
<th style="text-align:left">场景</th>
<th style="text-align:left">推荐损失函数</th>
<th style="text-align:left">模型输出层</th>
<th style="text-align:left">标签格式</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left"><strong>多分类（单标签）</strong></td>
<td style="text-align:left"><code>CrossEntropyLoss</code></td>
<td style="text-align:left">无激活（Raw Logits）</td>
<td style="text-align:left">整数索引（如 <code>[0]</code>）</td>
</tr>
<tr>
<td style="text-align:left"><strong>二分类/多标签</strong></td>
<td style="text-align:left"><code>BCEWithLogitsLoss</code></td>
<td style="text-align:left">无激活（Raw Logits）</td>
<td style="text-align:left">浮点数（如 <code>[1.0, 0.0, 1.0]</code>）</td>
</tr>
</tbody>
</table>
<p>核心区别：<strong><code>CrossEntropyLoss</code> 内部用 Softmax 处理整个输出向量，适用于“多选一”；而 <code>BCELoss</code> 对每个输出节点用 Sigmoid，适用于“每个节点独立判断是/否”。</strong></p>
<h2 id="pytorch实现逻辑回归">pytorch实现逻辑回归</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">x_data = torch.Tensor([[<span class="number">1.0</span>], [<span class="number">2.0</span>], [<span class="number">3.0</span>]])</span><br><span class="line">y_data = torch.Tensor([[<span class="number">0</span>], [<span class="number">0</span>], [<span class="number">1</span>]])</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">LogisticRegressionModel</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(LogisticRegressionModel,self).__init__()</span><br><span class="line">        self.linear = torch.nn.Linear(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        y_pred=torch.sigmoid(self.linear(x))</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model = LogisticRegressionModel()</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">criterion = torch.nn.BCELoss(size_average= <span class="literal">False</span>)</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(),lr=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure>
<pre><code>d:\Environment\Anaconda3\envs\deep\lib\site-packages\torch\nn\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.
  warnings.warn(warning.format(ret))
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    y_pred = model(x_data)</span><br><span class="line">    loss = criterion(y_pred,y_data)</span><br><span class="line">    <span class="built_in">print</span>(epoch, loss.item())</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;w = &#x27;</span>, model.linear.weight.item())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;b = &#x27;</span>, model.linear.bias.item())</span><br><span class="line"> </span><br><span class="line">x_test = torch.Tensor([[<span class="number">4.0</span>]])</span><br><span class="line">y_test = model(x_test)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;y_pred = &#x27;</span>, y_test.data)</span><br></pre></td></tr></table></figure>
<h1 id="多维特征输入">多维特征输入</h1>
<ol>
<li>本算法中torch.nn.Sigmoid() # 将其看作是网络的一层，而不是简单的函数使用</li>
</ol>
<h2 id="np-loadtxt">np.loadtxt</h2>
<p><code>np.loadtxt</code> 是 NumPy 中用于从文本文件加载数据的函数，特别适合读取结构化的数值数据。</p>
<h3 id="基本语法">基本语法</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">numpy.loadtxt(fname, dtype=<span class="built_in">float</span>, delimiter=<span class="literal">None</span>, skiprows=<span class="number">0</span>, usecols=<span class="literal">None</span>, ...)</span><br></pre></td></tr></table></figure>
<h3 id="主要参数">主要参数</h3>
<ul>
<li><strong><code>fname</code></strong>: 文件名或文件对象</li>
<li><strong><code>dtype</code></strong>: 数据类型，默认为 <code>float</code></li>
<li><strong><code>delimiter</code></strong>: 分隔符，如 <code>,</code>（CSV）、空格、<code>\t</code>（制表符）</li>
<li><strong><code>skiprows</code></strong>: 跳过的行数（如标题行）</li>
<li><strong><code>usecols</code></strong>: 指定要读取的列</li>
<li><strong><code>unpack</code></strong>: 如果为 True，返回转置后的数组</li>
</ul>
<h3 id="使用示例">使用示例</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 示例3：跳过标题行，指定数据类型</span></span><br><span class="line"><span class="comment"># data_with_header.txt 内容：</span></span><br><span class="line"><span class="comment"># x y z</span></span><br><span class="line"><span class="comment"># 1 2 3</span></span><br><span class="line"><span class="comment"># 4 5 6</span></span><br><span class="line">data_with_header = np.loadtxt(<span class="string">&#x27;data_with_header.txt&#x27;</span>, </span><br><span class="line">                             skiprows=<span class="number">1</span>,  <span class="comment"># 跳过标题行</span></span><br><span class="line">                             dtype=<span class="built_in">int</span>)   <span class="comment"># 指定整数类型</span></span><br><span class="line"><span class="built_in">print</span>(data_with_header)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例4：只读取特定列</span></span><br><span class="line">data_selected = np.loadtxt(<span class="string">&#x27;data.txt&#x27;</span>, usecols=(<span class="number">0</span>, <span class="number">2</span>))  <span class="comment"># 只读取第0和第2列</span></span><br><span class="line"><span class="built_in">print</span>(data_selected)</span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="comment"># [[1. 3.]</span></span><br><span class="line"><span class="comment">#  [4. 6.]</span></span><br><span class="line"><span class="comment">#  [7. 9.]]</span></span><br></pre></td></tr></table></figure>
<h2 id="torch-from-numpy">torch.from_numpy</h2>
<p><code>torch.from_numpy</code> 是 PyTorch 中将 NumPy 数组转换为 PyTorch 张量的函数。</p>
<h3 id="基本语法">基本语法</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.from_numpy(ndarray) → Tensor</span><br></pre></td></tr></table></figure>
<h3 id="重要特性">重要特性</h3>
<ul>
<li><strong>共享内存</strong>：转换后的张量与原始 NumPy 数组共享内存</li>
<li><strong>自动类型映射</strong>：保持数据类型对应关系</li>
<li><strong>单向转换</strong>：只能从 NumPy 到 PyTorch，不能反向</li>
</ul>
<h2 id="数据类型映射">数据类型映射</h2>
<p>NumPy 和 PyTorch 数据类型对应关系：</p>
<table>
<thead>
<tr>
<th style="text-align:center">NumPy dtype</th>
<th style="text-align:center">PyTorch dtype</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center"><code>np.float32</code></td>
<td style="text-align:center"><code>torch.float32</code></td>
</tr>
<tr>
<td style="text-align:center"><code>np.float64</code></td>
<td style="text-align:center"><code>torch.float64</code></td>
</tr>
<tr>
<td style="text-align:center"><code>np.int32</code></td>
<td style="text-align:center"><code>torch.int32</code></td>
</tr>
<tr>
<td style="text-align:center"><code>np.int64</code></td>
<td style="text-align:center"><code>torch.int64</code></td>
</tr>
<tr>
<td style="text-align:center"><code>np.bool_</code></td>
<td style="text-align:center"><code>torch.bool</code></td>
</tr>
</tbody>
</table>
<h2 id="实际工作流程示例">实际工作流程示例</h2>
<p>典型的从文件加载数据到 PyTorch 的流程：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"><span class="comment"># 步骤1：使用 np.loadtxt 从文件加载数据</span></span><br><span class="line"><span class="comment"># 假设我们有一个CSV文件，包含特征和标签</span></span><br><span class="line"><span class="comment"># 格式：特征1,特征2,特征3,标签</span></span><br><span class="line">data_np = np.loadtxt(<span class="string">&#x27;dataset.csv&#x27;</span>, delimiter=<span class="string">&#x27;,&#x27;</span>, skiprows=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 步骤2：分离特征和标签</span></span><br><span class="line">features_np = data_np[:, :-<span class="number">1</span>]  <span class="comment"># 所有行，除了最后一列</span></span><br><span class="line">labels_np = data_np[:, -<span class="number">1</span>]     <span class="comment"># 所有行，只取最后一列</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 步骤3：转换为 PyTorch 张量</span></span><br><span class="line">features_tensor = torch.from_numpy(features_np).<span class="built_in">float</span>()  <span class="comment"># 特征通常是浮点数</span></span><br><span class="line">labels_tensor = torch.from_numpy(labels_np).long()       <span class="comment"># 分类标签需要long类型</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;特征张量形状: <span class="subst">&#123;features_tensor.shape&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;标签张量形状: <span class="subst">&#123;labels_tensor.shape&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 步骤4：用于模型训练</span></span><br><span class="line">model = nn.Linear(features_tensor.shape[<span class="number">1</span>], <span class="number">3</span>)  <span class="comment"># 假设3分类</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练循环示例</span></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    outputs = model(features_tensor)</span><br><span class="line">    loss = criterion(outputs, labels_tensor)</span><br><span class="line">    </span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>
<h2 id="注意事项">注意事项</h2>
<h3 id="内存共享的优缺点">内存共享的优缺点</h3>
<p><strong>优点</strong>：</p>
<ul>
<li>内存高效，不需要复制数据</li>
<li>转换速度快</li>
</ul>
<p><strong>缺点</strong>：</p>
<ul>
<li>意外的相互影响</li>
<li>需要小心处理原地操作</li>
</ul>
<h3 id="断开内存共享">断开内存共享</h3>
<p>如果需要独立的张量：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">np_array = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">torch_tensor = torch.from_numpy(np_array).clone()  <span class="comment"># 创建副本，断开共享</span></span><br></pre></td></tr></table></figure>
<h2 id="总结">总结</h2>
<ul>
<li><strong><code>np.loadtxt</code></strong>：从文本文件加载结构化数据到 NumPy 数组</li>
<li><strong><code>torch.from_numpy</code></strong>：将 NumPy 数组转换为 PyTorch 张量（共享内存）</li>
<li><strong>典型流程</strong>：文件 → <code>np.loadtxt</code> → NumPy 数组 → <code>torch.from_numpy</code> → PyTorch 张量 → 模型训练</li>
</ul>
<p>这两个函数构成了数据科学和深度学习项目中数据加载和预处理的基础环节。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">xy = np.loadtxt(<span class="string">&#x27;diabetes.csv.gz&#x27;</span>,delimiter=<span class="string">&#x27;,&#x27;</span>,dtype=np.float32)</span><br><span class="line"><span class="comment">#设置分隔符为逗号  转换为float32类型</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x_data = torch.from_numpy(xy[:,:-<span class="number">1</span>])</span><br><span class="line">y_data = torch.from_numpy(xy[:,[-<span class="number">1</span>]])<span class="comment">#保持二维形状</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Model,self).__init__()</span><br><span class="line">        self.linear1 = torch.nn.Linear(<span class="number">8</span>,<span class="number">6</span>)</span><br><span class="line">        self.linear2 = torch.nn.Linear(<span class="number">6</span>,<span class="number">4</span>)</span><br><span class="line">        self.linear3 = torch.nn.Linear(<span class="number">4</span>,<span class="number">1</span>)</span><br><span class="line">        self.sigmiod = torch.nn.Sigmoid()<span class="comment"># 将其看作是网络的一层，而不是简单的函数使用</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x=self.sigmiod(self.linear1(x))</span><br><span class="line">        x=self.sigmiod(self.linear2(x))</span><br><span class="line">        x=self.sigmiod(self.linear3(x))</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = Model()</span><br><span class="line">criterion = torch.nn.BCELoss(reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(),lr=<span class="number">0.1</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">epoch_list = []</span><br><span class="line">loss_list = []</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    y_pred = model(x_data)</span><br><span class="line">    loss = criterion(y_pred, y_data)</span><br><span class="line">    <span class="built_in">print</span>(epoch, loss.item())</span><br><span class="line">    epoch_list.append(epoch)</span><br><span class="line">    loss_list.append(loss.item())</span><br><span class="line"> </span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line"> </span><br><span class="line">    optimizer.step()</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line">plt.plot(epoch_list, loss_list)</span><br><span class="line">plt.ylabel(<span class="string">&#x27;loss&#x27;</span>)</span><br><span class="line">plt.xlabel(<span class="string">&#x27;epoch&#x27;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<pre><code>0 0.748360276222229
1 0.7387034893035889
2 0.7299509048461914
3 0.7220205068588257
4 0.7148370742797852
5 0.708331823348999
6 0.7024414539337158
7 0.6971085071563721
8 0.6922804713249207
9 0.6879096627235413
10 0.6839527487754822
11 0.6803703308105469
12 0.6771267652511597
13 0.6741898059844971
14 0.67153000831604
15 0.6691210865974426
16 0.6669390797615051
17 0.6649622321128845
18 0.6631709933280945
19 0.6615477204322815
20 0.660076379776001
21 0.6587424874305725
22 0.6575331091880798
23 0.6564363837242126
24 0.6554415225982666
25 0.6545389890670776
26 0.6537201404571533
27 0.652976930141449
28 0.6523025631904602
29 0.6516901254653931
30 0.6511341333389282
31 0.6506292819976807
32 0.650170624256134
33 0.6497540473937988
34 0.6493754386901855
35 0.6490315198898315
36 0.6487187743186951
37 0.648434579372406
38 0.6481761932373047
39 0.6479411721229553
40 0.6477274894714355
41 0.6475330591201782
42 0.6473562121391296
43 0.6471953392028809
44 0.6470488905906677
45 0.64691561460495
46 0.6467941999435425
47 0.6466837525367737
48 0.6465831398963928
49 0.6464914679527283
50 0.646407961845398
51 0.6463318467140198
52 0.6462624073028564
53 0.6461992263793945
54 0.646141529083252
55 0.6460889577865601
56 0.6460409760475159
57 0.645997166633606
58 0.6459571719169617
59 0.6459206938743591
60 0.6458873748779297
61 0.6458569765090942
62 0.6458291411399841
63 0.6458036303520203
64 0.6457803249359131
65 0.6457589268684387
66 0.6457394361495972
67 0.6457215547561646
68 0.6457050442695618
69 0.6456900238990784
70 0.6456762552261353
71 0.6456635594367981
72 0.6456518173217773
73 0.6456410884857178
74 0.6456312537193298
75 0.6456220746040344
76 0.6456136107444763
77 0.6456058621406555
78 0.6455986499786377
79 0.6455920338630676
80 0.645585834980011
81 0.6455800533294678
82 0.6455748081207275
83 0.6455698013305664
84 0.6455651521682739
85 0.6455608010292053
86 0.6455566883087158
87 0.6455529928207397
88 0.6455493569374084
89 0.645546019077301
90 0.6455428004264832
91 0.6455397605895996
92 0.6455368995666504
93 0.6455342769622803
94 0.6455316543579102
95 0.6455292701721191
96 0.6455268859863281
97 0.6455246210098267
98 0.6455225348472595
99 0.6455204486846924
</code></pre>
<p>![png](06_%E5%A4%9A%E7%BB%B4%E7%89%B9%E5%BE%81_files/06_%E5%A4%9A%E7%BB%B4%E7%89%B9%E5%BE%81_8_1.png</p>
<h1 id="加载数据集">加载数据集</h1>
<h2 id="dataset-数据集">Dataset（数据集）</h2>
<p><code>Dataset</code> 是一个抽象类，用来表示数据集。我们需要继承这个类并实现几个关键方法来创建自定义数据集。</p>
<h3 id="核心方法">核心方法</h3>
<ul>
<li><code>__init__()</code>: 初始化数据集（如读取文件、设置路径等）</li>
<li><code>__len__()</code>: 返回数据集的大小</li>
<li><code>__getitem__()</code>: 根据索引返回一个样本</li>
</ul>
<h3 id="基本示例">基本示例</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义数据集类</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CustomDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, csv_file, transform=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">        初始化数据集</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            csv_file: 数据文件的路径</span></span><br><span class="line"><span class="string">            transform: 数据预处理变换</span></span><br><span class="line"><span class="string">        &quot;&quot;&quot;</span></span><br><span class="line">        self.data_frame = pd.read_csv(csv_file)</span><br><span class="line">        self.transform = transform</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;返回数据集大小&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.data_frame)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="string">&quot;&quot;&quot;根据索引返回样本&quot;&quot;&quot;</span></span><br><span class="line">        <span class="keyword">if</span> torch.is_tensor(idx):</span><br><span class="line">            idx = idx.tolist()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 获取特征和标签</span></span><br><span class="line">        features = self.data_frame.iloc[idx, :-<span class="number">1</span>].values.astype(np.float32)</span><br><span class="line">        label = self.data_frame.iloc[idx, -<span class="number">1</span>].astype(np.int64)</span><br><span class="line">        </span><br><span class="line">        sample = &#123;<span class="string">&#x27;features&#x27;</span>: features, <span class="string">&#x27;label&#x27;</span>: label&#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 应用变换（如果有）</span></span><br><span class="line">        <span class="keyword">if</span> self.transform:</span><br><span class="line">            sample = self.transform(sample)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> sample</span><br></pre></td></tr></table></figure>
<h3 id="更简单的示例">更简单的示例</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SimpleDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, x, y</span>):</span><br><span class="line">        self.x = x</span><br><span class="line">        self.y = y</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.x)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, idx</span>):</span><br><span class="line">        <span class="keyword">return</span> self.x[idx], self.y[idx]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用示例</span></span><br><span class="line">x_data = torch.randn(<span class="number">100</span>, <span class="number">5</span>)  <span class="comment"># 100个样本，每个5个特征</span></span><br><span class="line">y_data = torch.randint(<span class="number">0</span>, <span class="number">3</span>, (<span class="number">100</span>,))  <span class="comment"># 100个标签，3分类</span></span><br><span class="line"></span><br><span class="line">dataset = SimpleDataset(x_data, y_data)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;数据集大小: <span class="subst">&#123;<span class="built_in">len</span>(dataset)&#125;</span>&quot;</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f&quot;第一个样本: <span class="subst">&#123;dataset[<span class="number">0</span>]&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h2 id="dataloader-数据加载器">DataLoader（数据加载器）</h2>
<p><code>DataLoader</code> 是一个迭代器，提供以下功能：</p>
<ul>
<li><strong>批量处理</strong> (batching)</li>
<li><strong>数据打乱</strong> (shuffling)</li>
<li><strong>多进程加载</strong> (multiprocessing)</li>
</ul>
<h3 id="主要参数">主要参数</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DataLoader(dataset, batch_size=<span class="number">1</span>, shuffle=<span class="literal">False</span>, num_workers=<span class="number">0</span>, ...)</span><br></pre></td></tr></table></figure>
<ul>
<li><code>dataset</code>: 要加载的数据集</li>
<li><code>batch_size</code>: 每个批次的样本数</li>
<li><code>shuffle</code>: 是否在每个epoch打乱数据</li>
<li><code>num_workers</code>: 用于数据加载的子进程数</li>
<li><code>drop_last</code>: 如果数据集大小不能被batch_size整除，是否丢弃最后一个不完整的batch</li>
</ul>
<h3 id="使用示例">使用示例</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建DataLoader</span></span><br><span class="line">dataloader = DataLoader(</span><br><span class="line">    dataset=dataset,      <span class="comment"># 上面创建的数据集</span></span><br><span class="line">    batch_size=<span class="number">32</span>,        <span class="comment"># 每批32个样本</span></span><br><span class="line">    shuffle=<span class="literal">True</span>,         <span class="comment"># 训练时打乱数据</span></span><br><span class="line">    num_workers=<span class="number">2</span>,        <span class="comment"># 使用2个子进程加载数据</span></span><br><span class="line">    drop_last=<span class="literal">False</span>       <span class="comment"># 不丢弃最后一个不完整的batch</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 遍历DataLoader</span></span><br><span class="line"><span class="keyword">for</span> batch_idx, (features, labels) <span class="keyword">in</span> <span class="built_in">enumerate</span>(dataloader):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;Batch <span class="subst">&#123;batch_idx&#125;</span>:&quot;</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;  特征形状: <span class="subst">&#123;features.shape&#125;</span>&quot;</span>)  <span class="comment"># torch.Size([32, 5])</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;  标签形状: <span class="subst">&#123;labels.shape&#125;</span>&quot;</span>)    <span class="comment"># torch.Size([32])</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> batch_idx == <span class="number">2</span>:  <span class="comment"># 只显示前3个batch</span></span><br><span class="line">        <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<h2 id="总结">总结</h2>
<ul>
<li><strong>Dataset</strong>：定义如何访问数据，实现 <code>__len__</code> 和 <code>__getitem__</code> 方法</li>
<li><strong>DataLoader</strong>：批量加载数据，提供打乱、并行加载等功能</li>
<li><strong>核心优势</strong>：代码模块化、内存高效、支持并行处理</li>
<li><strong>典型流程</strong>：创建 Dataset → 创建 DataLoader → 在训练循环中迭代</li>
</ul>
<p>这种设计使得数据加载与模型训练解耦，大大提高了代码的可读性和可维护性。</p>
<ol>
<li>需要mini_batch 就需要import DataSet和DataLoader</li>
<li>继承DataSet的类需要重写init，getitem,len魔法函数。分别是为了加载数据集，获取数据索引，获取数据总量。</li>
<li>DataLoader对数据集先打乱(shuffle)，然后划分成mini_batch。</li>
</ol>
<h2 id="代码示例">代码示例</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> Dataset</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DiabetesDataset</span>(<span class="title class_ inherited__">Dataset</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,filepath</span>):</span><br><span class="line">        xy = np.loadtxt(filepath,delimiter=<span class="string">&#x27;,&#x27;</span>,dtype=np.float32)</span><br><span class="line">        self.<span class="built_in">len</span> = xy.shape[<span class="number">0</span>]</span><br><span class="line">        self.x_data=torch.from_numpy(xy[:,:-<span class="number">1</span>])</span><br><span class="line">        self.y_data=torch.from_numpy(xy[:,[-<span class="number">1</span>]])</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self,index</span>):</span><br><span class="line">        <span class="keyword">return</span> self.x_data[index],self.y_data[index] <span class="comment">#返回元祖</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> self.<span class="built_in">len</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dataset=DiabetesDataset(<span class="string">&#x27;diabetes.csv.gz&#x27;</span>)</span><br><span class="line">train_loader = DataLoader(dataset=dataset,batch_size=<span class="number">32</span>,shuffle=<span class="literal">True</span>,num_workers=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Model</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Model,self).__init__()</span><br><span class="line">        self.linear1 = torch.nn.Linear(<span class="number">8</span>,<span class="number">6</span>)</span><br><span class="line">        self.linear2 = torch.nn.Linear(<span class="number">6</span>,<span class="number">4</span>)</span><br><span class="line">        self.linear3 = torch.nn.Linear(<span class="number">4</span>,<span class="number">1</span>)</span><br><span class="line">        self.sigmiod = torch.nn.Sigmoid()<span class="comment"># 将其看作是网络的一层，而不是简单的函数使用</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x=self.sigmiod(self.linear1(x))</span><br><span class="line">        x=self.sigmiod(self.linear2(x))</span><br><span class="line">        x=self.sigmiod(self.linear3(x))</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">model = Model()</span><br><span class="line">criterion = torch.nn.BCELoss(reduction=<span class="string">&#x27;mean&#x27;</span>)</span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(),lr=<span class="number">0.1</span>)</span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        <span class="keyword">for</span> i,data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader,<span class="number">0</span>):<span class="comment">#train_loader 是先shuffle后mini_batch 会调用 Dataset 的 __getitem__ 方法来获取每个样本，然后将它们组合成批次。</span></span><br><span class="line">            inputs,labels = data <span class="comment">#返回一个元组，包含两个元素：索引and数据 即一批(batch)数据</span></span><br><span class="line"></span><br><span class="line">            y_pred = model(inputs)</span><br><span class="line">            loss = criterion(y_pred, labels)</span><br><span class="line">            <span class="built_in">print</span>(epoch, i, loss.item())</span><br><span class="line"> </span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line"> </span><br><span class="line">            optimizer.step()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        </span><br></pre></td></tr></table></figure>
<h1 id="多分类问题">多分类问题</h1>
<p>在实践中，我们通常：</p>
<ol>
<li>对模型的原始输出（logits）应用 Softmax 得到概率分布</li>
<li>然后计算 交叉熵 损失</li>
</ol>
<p>CrossEntropyLoss  =  LogSoftmax + NLLLoss。也就是说使用CrossEntropyLoss最后一层(线性层)是不需要做其他变化的；使用NLLLoss之前，需要对最后一层(线性层)先进行SoftMax处理，再进行log操作。</p>
<h2 id="手写数字识别为例">手写数字识别为例</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> transforms</span><br><span class="line"><span class="keyword">from</span> torchvision <span class="keyword">import</span> datasets</span><br><span class="line"><span class="keyword">from</span> torch.utils.data <span class="keyword">import</span> DataLoader</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">transform = transforms.Compose([transforms.ToTensor(),transforms.Normalize((<span class="number">0.1307</span>,),(<span class="number">0.3081</span>,))])</span><br><span class="line"><span class="comment">#transforms.Compose 用于将多个变换组合在一起，按顺序执行。</span></span><br><span class="line"><span class="comment">#transforms.ToTensor()将 PIL 图像或 numpy 数组转换为 PyTorch 张量</span></span><br><span class="line"><span class="comment"># ToTensor 的转换过程</span></span><br><span class="line"><span class="comment"># 原始图像是 28x28 的灰度图，像素值 0-255</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换前：PIL.Image 或 numpy.ndarray</span></span><br><span class="line"><span class="comment"># 形状：H x W x C (28, 28, 1) </span></span><br><span class="line"><span class="comment"># 数值范围：0-255 (uint8)</span></span><br><span class="line"><span class="comment"># 转换后：torch.Tensor</span></span><br><span class="line"><span class="comment"># 形状：C x H x W (1, 28, 28) - PyTorch 要求的通道优先格式</span></span><br><span class="line"><span class="comment"># 数值范围：0.0-1.0 (float32)</span></span><br><span class="line"><span class="comment"># transforms.Normalize((0.1307,), (0.3081,))对张量进行标准化（归一化），使用指定的均值和标准差。</span></span><br><span class="line"><span class="comment"># (0.1307, 0.3081) 是 MNIST 数据集的全局统计值，通过对整个训练集计算得到：</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">train_dataset = datasets.MNIST(root=<span class="string">&#x27;./dataset/mniset&#x27;</span>,train=<span class="literal">True</span>,download=<span class="literal">True</span>,transform=transform)</span><br><span class="line">train_loader = DataLoader(train_dataset,shuffle=<span class="literal">True</span>,batch_size=batch_size)</span><br><span class="line">test_dataset = datasets.MNIST(root=<span class="string">&#x27;./dataset/mniset&#x27;</span>,train=<span class="literal">False</span>,download=<span class="literal">True</span>,transform=transform)</span><br><span class="line">test_loader = DataLoader(test_dataset,shuffle=<span class="literal">True</span>,batch_size=batch_size)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Net</span>(torch.nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(Net,self).__init__()</span><br><span class="line">        self.l1 = torch.nn.Linear(<span class="number">784</span>,<span class="number">512</span>)</span><br><span class="line">        self.l2 = torch.nn.Linear(<span class="number">512</span>,<span class="number">256</span>)</span><br><span class="line">        self.l3 = torch.nn.Linear(<span class="number">256</span>,<span class="number">128</span>)</span><br><span class="line">        self.l4 = torch.nn.Linear(<span class="number">128</span>,<span class="number">64</span>)</span><br><span class="line">        self.l5 = torch.nn.Linear(<span class="number">64</span>,<span class="number">10</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x = x.view(-<span class="number">1</span>,<span class="number">784</span>)</span><br><span class="line">        x = F.relu(self.l1(x))</span><br><span class="line">        x = F.relu(self.l2(x))</span><br><span class="line">        x = F.relu(self.l3(x))</span><br><span class="line">        x = F.relu(self.l4(x))</span><br><span class="line">        <span class="keyword">return</span> self.l5(x)  <span class="comment">#最后一层不激活</span></span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = Net()</span><br><span class="line">criterion = torch.nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(model.parameters(),lr=<span class="number">0.01</span>,momentum=<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">epoch</span>):</span><br><span class="line">    running_loss=<span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> batch_idx,data <span class="keyword">in</span> <span class="built_in">enumerate</span>(train_loader):</span><br><span class="line">        inputs,targets = data</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        outputs = model(inputs)</span><br><span class="line">        loss = criterion(outputs,targets)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        running_loss+=loss.item()</span><br><span class="line">        <span class="keyword">if</span> batch_idx % <span class="number">300</span> == <span class="number">299</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;[%d, %5d] loss: %.3f&#x27;</span> % (epoch+<span class="number">1</span>, batch_idx+<span class="number">1</span>, running_loss/<span class="number">300</span>))</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">test</span>():</span><br><span class="line">    correct = <span class="number">0</span></span><br><span class="line">    total = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="comment">#with torch.no_grad(): 创建一个上下文管理器，在这个上下文中的所有操作都不会计算梯度。</span></span><br><span class="line">        <span class="keyword">for</span> data <span class="keyword">in</span> test_loader:</span><br><span class="line">            images,labels = data</span><br><span class="line">            outputs = model(images)</span><br><span class="line">            _,predicted = torch.<span class="built_in">max</span>(outputs.data,dim=<span class="number">1</span>)</span><br><span class="line">            <span class="comment">#dim=1 即每一行求max</span></span><br><span class="line">            total +=labels.size(<span class="number">0</span>)</span><br><span class="line">            correct+=(predicted==labels).<span class="built_in">sum</span>().item()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;accuracy on test set: %d %% &#x27;</span> % (<span class="number">100</span>*correct/total))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">        train(epoch)</span><br><span class="line">        test()</span><br></pre></td></tr></table></figure>
<h1 id="cnn卷积神经网络">CNN卷积神经网络</h1>
<ol>
<li>一个卷积核的通道数量等于输入数据的通道数</li>
<li>卷积核的数目等于输出数据的通道数</li>
</ol>
<p><img src="image-20251207170314211.png" alt="image-20251207170314211"></p>
<p><img src="image-20251207170451727.png" alt="image-20251207170451727"></p>
<p><img src="image-20251207170629400.png" alt="image-20251207170629400"></p>
<p><img src="image-2.png" alt="image-2"></p>
<p><img src="image-4.png" alt="image-4"><img src="image-3.png" alt="image-3"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">corr2d</span>(<span class="params">X, K</span>):  </span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算二维互相关运算。&quot;&quot;&quot;</span></span><br><span class="line">    h, w = K.shape</span><br><span class="line">    Y = torch.zeros((X.shape[<span class="number">0</span>] - h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - w + <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            Y[i, j] = (X[i:i + h, j:j + w] * K).<span class="built_in">sum</span>()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br><span class="line"><span class="comment">#实现二维卷积层</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Conv2D</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, kernel_size</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.weight = nn.Parameter(torch.rand(kernel_size))</span><br><span class="line">        self.bias = nn.Parameter(torch.zeros(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="keyword">return</span> corr2d(x, self.weight) + self.bias</span><br></pre></td></tr></table></figure>
<h2 id="填充和步幅">填充和步幅</h2>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#在所有侧边填充1个像素</span></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">comp_conv2d</span>(<span class="params">conv2d, X</span>):</span><br><span class="line">    X = X.reshape((<span class="number">1</span>, <span class="number">1</span>) + X.shape)</span><br><span class="line">    Y = conv2d(X)</span><br><span class="line">    <span class="keyword">return</span> Y.reshape(Y.shape[<span class="number">2</span>:])<span class="comment">#卷积层nn.Conv2d的输入要求是4D张量</span></span><br><span class="line"></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)<span class="comment">#输入通道数 输出通道数 核大小3*3 填充为1</span></span><br><span class="line">X = torch.rand(size=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#填充不同的高度和宽度</span></span><br><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=(<span class="number">5</span>, <span class="number">3</span>), padding=(<span class="number">2</span>, <span class="number">1</span>))<span class="comment">#输入通道数 输出通道数 核大小5*3 填充为上下各为2 左右各为1</span></span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conv2d = nn.Conv2d(<span class="number">1</span>, <span class="number">1</span>, kernel_size=(<span class="number">3</span>, <span class="number">5</span>), padding=(<span class="number">0</span>, <span class="number">1</span>), stride=(<span class="number">3</span>, <span class="number">4</span>))<span class="comment">#stride规定了步幅,左右3上下4</span></span><br><span class="line">comp_conv2d(conv2d, X).shape</span><br></pre></td></tr></table></figure>
<h2 id="池化">池化</h2>
<p>卷积对位置敏感,因此引入池化提供一定的平移不变性</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">from</span> d2l <span class="keyword">import</span> torch <span class="keyword">as</span> d2l</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pool2d</span>(<span class="params">X, pool_size, mode=<span class="string">&#x27;max&#x27;</span></span>):</span><br><span class="line">    p_h, p_w = pool_size</span><br><span class="line">    Y = torch.zeros((X.shape[<span class="number">0</span>] - p_h + <span class="number">1</span>, X.shape[<span class="number">1</span>] - p_w + <span class="number">1</span>))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">0</span>]):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(Y.shape[<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">if</span> mode == <span class="string">&#x27;max&#x27;</span>:</span><br><span class="line">                Y[i, j] = X[i:i + p_h, j:j + p_w].<span class="built_in">max</span>()</span><br><span class="line">            <span class="keyword">elif</span> mode == <span class="string">&#x27;avg&#x27;</span>:</span><br><span class="line">                Y[i, j] = X[i:i + p_h, j:j + p_w].mean()</span><br><span class="line">    <span class="keyword">return</span> Y</span><br><span class="line">X = torch.tensor([[<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">2.0</span>], [<span class="number">3.0</span>, <span class="number">4.0</span>, <span class="number">5.0</span>], [<span class="number">6.0</span>, <span class="number">7.0</span>, <span class="number">8.0</span>]])</span><br><span class="line">pool2d(X, (<span class="number">2</span>, <span class="number">2</span>))</span><br></pre></td></tr></table></figure>
<p>深度学习框架中的池化层的步幅与池化窗口的大小相同</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X = torch.arange(<span class="number">16</span>, dtype=torch.float32).reshape((<span class="number">1</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">4</span>))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pool2d = nn.MaxPool2d(<span class="number">3</span>)</span><br><span class="line">pool2d(X)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pool2d = nn.MaxPool2d((<span class="number">2</span>, <span class="number">3</span>), padding=(<span class="number">1</span>, <span class="number">1</span>), stride=(<span class="number">2</span>, <span class="number">3</span>))<span class="comment">#手动设定大小 填充  步幅</span></span><br><span class="line">pool2d(X)</span><br></pre></td></tr></table></figure>
<h1 id="pytorch知识">Pytorch知识</h1>
<h2 id="sequential">sequential</h2>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://klklkl10086.github.io/klklkl10086.github.io">klklkl</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://klklkl10086.github.io/klklkl10086.github.io/2025/11/27/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/">https://klklkl10086.github.io/klklkl10086.github.io/2025/11/27/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://klklkl10086.github.io/klklkl10086.github.io" target="_blank">klklkl's blogs</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/Pytorch/">Pytorch</a></div><div class="post_share"><div class="social-share" data-image="https://pic2.zhimg.com/v2-f9b32b95385eedbdb58389bbbce5de39_r.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2025/12/07/%E4%BB%8E0%E5%AE%9E%E7%8E%B0LLM/" title="从0实现Transformer"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">从0实现Transformer</div></div></a></div><div class="next-post pull-right"><a href="/2025/11/18/cuda%E7%BC%96%E7%A8%8B%E5%85%A5%E9%97%A8/" title="cuda编程入门"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">cuda编程入门</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2025/12/07/%E4%BB%8E0%E5%AE%9E%E7%8E%B0LLM/" title="从0实现Transformer"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-12-07</div><div class="title">从0实现Transformer</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://pic2.zhimg.com/v2-f9b32b95385eedbdb58389bbbce5de39_r.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">klklkl</div><div class="author-info__description">student</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">22</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">26</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">8</div></a></div><div class="card-info-social-icons is-center"><a class="social-icon" href="mailto:2488926297@qq.com" target="_blank" title="Email"><i class="fa-regular fa-envelope" style="color: #000000;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Nice to meet you!</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B"><span class="toc-number">1.</span> <span class="toc-text">线性模型</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%A9%B7%E4%B8%BE%E6%B3%95"><span class="toc-number">1.1.</span> <span class="toc-text">穷举法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%BE%E5%90%8E%E4%BD%9C%E4%B8%9A"><span class="toc-number">1.2.</span> <span class="toc-text">课后作业</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">2.</span> <span class="toc-text">梯度下降</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">2.1.</span> <span class="toc-text">批量梯度下降</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">2.2.</span> <span class="toc-text">随机梯度下降</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B0%8F%E6%89%B9%E9%87%8F%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">2.3.</span> <span class="toc-text">小批量梯度下降</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">3.</span> <span class="toc-text">反向传播</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#tensor%E7%B1%BB%E4%B8%8E%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="toc-number">3.1.</span> <span class="toc-text">Tensor类与计算图</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-number">3.2.</span> <span class="toc-text">反向传播</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%9C%E4%B8%9A"><span class="toc-number">3.3.</span> <span class="toc-text">作业</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#pytorch%E5%AE%9E%E7%8E%B0%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">4.</span> <span class="toc-text">Pytorch实现线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#init%E4%B8%8Ecall%E5%87%BD%E6%95%B0"><span class="toc-number">4.1.</span> <span class="toc-text">init与call函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#init"><span class="toc-number">4.1.1.</span> <span class="toc-text">init</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#call"><span class="toc-number">4.1.2.</span> <span class="toc-text">call</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#nn-module%E4%B8%AD%E7%9A%84api"><span class="toc-number">4.2.</span> <span class="toc-text">nn.module中的API</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E7%8E%B0%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="toc-number">4.3.</span> <span class="toc-text">实现线性回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B8%B8%E7%94%A8%E7%9A%84%E4%BC%98%E5%8C%96%E5%99%A8%E4%B8%8E%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">4.4.</span> <span class="toc-text">常用的优化器与损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%9E%E5%BD%92%E4%BB%BB%E5%8A%A1%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">4.4.1.</span> <span class="toc-text">回归任务损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#mseloss-%E5%9D%87%E6%96%B9%E8%AF%AF%E5%B7%AE%E6%8D%9F%E5%A4%B1"><span class="toc-number">4.4.1.1.</span> <span class="toc-text">MSELoss（均方误差损失）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#l1loss-%E5%B9%B3%E5%9D%87%E7%BB%9D%E5%AF%B9%E8%AF%AF%E5%B7%AE%E6%8D%9F%E5%A4%B1"><span class="toc-number">4.4.1.2.</span> <span class="toc-text">L1Loss（平均绝对误差损失）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#smoothl1loss-%E5%B9%B3%E6%BB%91l1%E6%8D%9F%E5%A4%B1"><span class="toc-number">4.4.1.3.</span> <span class="toc-text">SmoothL1Loss（平滑L1损失）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#huberloss"><span class="toc-number">4.4.1.4.</span> <span class="toc-text">HuberLoss</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%86%E7%B1%BB%E4%BB%BB%E5%8A%A1%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">4.4.2.</span> <span class="toc-text">分类任务损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#bceloss-%E4%BA%8C%E5%88%86%E7%B1%BB%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1"><span class="toc-number">4.4.2.1.</span> <span class="toc-text">BCELoss（二分类交叉熵损失）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#bcewithlogitsloss-%E5%B8%A6logits%E7%9A%84bce%E6%8D%9F%E5%A4%B1"><span class="toc-number">4.4.2.2.</span> <span class="toc-text">BCEWithLogitsLoss（带logits的BCE损失）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#crossentropyloss-%E5%A4%9A%E5%88%86%E7%B1%BB%E4%BA%A4%E5%8F%89%E7%86%B5%E6%8D%9F%E5%A4%B1"><span class="toc-number">4.4.2.3.</span> <span class="toc-text">CrossEntropyLoss（多分类交叉熵损失）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#nllloss-%E8%B4%9F%E5%AF%B9%E6%95%B0%E4%BC%BC%E7%84%B6%E6%8D%9F%E5%A4%B1"><span class="toc-number">4.4.2.4.</span> <span class="toc-text">NLLLoss（负对数似然损失）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0"><span class="toc-number">4.4.3.</span> <span class="toc-text">其他损失函数</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#cosineembeddingloss-%E4%BD%99%E5%BC%A6%E7%9B%B8%E4%BC%BC%E5%BA%A6%E6%8D%9F%E5%A4%B1"><span class="toc-number">4.4.3.1.</span> <span class="toc-text">CosineEmbeddingLoss（余弦相似度损失）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#tripletmarginloss-%E4%B8%89%E5%85%83%E7%BB%84%E6%8D%9F%E5%A4%B1"><span class="toc-number">4.4.3.2.</span> <span class="toc-text">TripletMarginLoss（三元组损失）</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#sgd-%E9%9A%8F%E6%9C%BA%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D"><span class="toc-number">4.4.4.</span> <span class="toc-text">SGD（随机梯度下降）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#adam-%E8%87%AA%E9%80%82%E5%BA%94%E7%9F%A9%E4%BC%B0%E8%AE%A1"><span class="toc-number">4.4.5.</span> <span class="toc-text">Adam（自适应矩估计）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#adamw-adam-with-weight-decay"><span class="toc-number">4.4.6.</span> <span class="toc-text">AdamW（Adam with Weight Decay）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#rmsprop"><span class="toc-number">4.4.7.</span> <span class="toc-text">RMSprop</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#adagrad"><span class="toc-number">4.4.8.</span> <span class="toc-text">Adagrad</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#adadelta"><span class="toc-number">4.4.9.</span> <span class="toc-text">Adadelta</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#adamax"><span class="toc-number">4.4.10.</span> <span class="toc-text">Adamax</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E9%80%89%E6%8B%A9%E6%8C%87%E5%8D%97%EF%BC%9A"><span class="toc-number">4.4.11.</span> <span class="toc-text">损失函数选择指南：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E5%8C%96%E5%99%A8%E9%80%89%E6%8B%A9%E6%8C%87%E5%8D%97%EF%BC%9A"><span class="toc-number">4.4.12.</span> <span class="toc-text">优化器选择指南：</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="toc-number">5.</span> <span class="toc-text">逻辑回归</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#bceloss"><span class="toc-number">5.1.</span> <span class="toc-text">BCELoss</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#crossentropyloss"><span class="toc-number">5.2.</span> <span class="toc-text">CrossEntropyLoss</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A6%82%E4%BD%95%E9%80%89%E6%8B%A9%EF%BC%9F"><span class="toc-number">5.3.</span> <span class="toc-text">如何选择？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">5.4.</span> <span class="toc-text">总结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#pytorch%E5%AE%9E%E7%8E%B0%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92"><span class="toc-number">5.5.</span> <span class="toc-text">pytorch实现逻辑回归</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A4%9A%E7%BB%B4%E7%89%B9%E5%BE%81%E8%BE%93%E5%85%A5"><span class="toc-number">6.</span> <span class="toc-text">多维特征输入</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#np-loadtxt"><span class="toc-number">6.1.</span> <span class="toc-text">np.loadtxt</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95"><span class="toc-number">6.1.1.</span> <span class="toc-text">基本语法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E5%8F%82%E6%95%B0"><span class="toc-number">6.1.2.</span> <span class="toc-text">主要参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B"><span class="toc-number">6.1.3.</span> <span class="toc-text">使用示例</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#torch-from-numpy"><span class="toc-number">6.2.</span> <span class="toc-text">torch.from_numpy</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E8%AF%AD%E6%B3%95"><span class="toc-number">6.2.1.</span> <span class="toc-text">基本语法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%87%8D%E8%A6%81%E7%89%B9%E6%80%A7"><span class="toc-number">6.2.2.</span> <span class="toc-text">重要特性</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E6%98%A0%E5%B0%84"><span class="toc-number">6.3.</span> <span class="toc-text">数据类型映射</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%99%85%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%E7%A4%BA%E4%BE%8B"><span class="toc-number">6.4.</span> <span class="toc-text">实际工作流程示例</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9"><span class="toc-number">6.5.</span> <span class="toc-text">注意事项</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%85%E5%AD%98%E5%85%B1%E4%BA%AB%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="toc-number">6.5.1.</span> <span class="toc-text">内存共享的优缺点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%96%AD%E5%BC%80%E5%86%85%E5%AD%98%E5%85%B1%E4%BA%AB"><span class="toc-number">6.5.2.</span> <span class="toc-text">断开内存共享</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">6.6.</span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">7.</span> <span class="toc-text">加载数据集</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#dataset-%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="toc-number">7.1.</span> <span class="toc-text">Dataset（数据集）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E6%96%B9%E6%B3%95"><span class="toc-number">7.1.1.</span> <span class="toc-text">核心方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E7%A4%BA%E4%BE%8B"><span class="toc-number">7.1.2.</span> <span class="toc-text">基本示例</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9B%B4%E7%AE%80%E5%8D%95%E7%9A%84%E7%A4%BA%E4%BE%8B"><span class="toc-number">7.1.3.</span> <span class="toc-text">更简单的示例</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#dataloader-%E6%95%B0%E6%8D%AE%E5%8A%A0%E8%BD%BD%E5%99%A8"><span class="toc-number">7.2.</span> <span class="toc-text">DataLoader（数据加载器）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BB%E8%A6%81%E5%8F%82%E6%95%B0"><span class="toc-number">7.2.1.</span> <span class="toc-text">主要参数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8%E7%A4%BA%E4%BE%8B"><span class="toc-number">7.2.2.</span> <span class="toc-text">使用示例</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">7.3.</span> <span class="toc-text">总结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B"><span class="toc-number">7.4.</span> <span class="toc-text">代码示例</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%A4%9A%E5%88%86%E7%B1%BB%E9%97%AE%E9%A2%98"><span class="toc-number">8.</span> <span class="toc-text">多分类问题</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB%E4%B8%BA%E4%BE%8B"><span class="toc-number">8.1.</span> <span class="toc-text">手写数字识别为例</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#cnn%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="toc-number">9.</span> <span class="toc-text">CNN卷积神经网络</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%A1%AB%E5%85%85%E5%92%8C%E6%AD%A5%E5%B9%85"><span class="toc-number">9.1.</span> <span class="toc-text">填充和步幅</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%B1%A0%E5%8C%96"><span class="toc-number">9.2.</span> <span class="toc-text">池化</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#pytorch%E7%9F%A5%E8%AF%86"><span class="toc-number">10.</span> <span class="toc-text">Pytorch知识</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#sequential"><span class="toc-number">10.1.</span> <span class="toc-text">sequential</span></a></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/01/31/Linux%E7%8E%AF%E5%A2%83%E7%BC%96%E7%A8%8B/" title="Linux环境编程">Linux环境编程</a><time datetime="2026-01-31T12:51:35.000Z" title="发表于 2026-01-31 20:51:35">2026-01-31</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/01/31/STL%E6%8B%BE%E9%81%97/" title="STL拾遗">STL拾遗</a><time datetime="2026-01-31T12:50:18.000Z" title="发表于 2026-01-31 20:50:18">2026-01-31</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/12/07/%E4%BB%8E0%E5%AE%9E%E7%8E%B0LLM/" title="从0实现Transformer">从0实现Transformer</a><time datetime="2025-12-07T13:07:22.000Z" title="发表于 2025-12-07 21:07:22">2025-12-07</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/11/27/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/" title="Pytorch深度学习实践">Pytorch深度学习实践</a><time datetime="2025-11-27T14:15:20.000Z" title="发表于 2025-11-27 22:15:20">2025-11-27</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/11/18/cuda%E7%BC%96%E7%A8%8B%E5%85%A5%E9%97%A8/" title="cuda编程入门">cuda编程入门</a><time datetime="2025-11-18T12:46:44.000Z" title="发表于 2025-11-18 20:46:44">2025-11-18</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://pic1.zhimg.com/80/v2-908b61a41ec4bebe17a04468dcf5d834_720w.webp')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2026 By klklkl</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>