<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>cuda编程入门 | klklkl's blogs</title><meta name="author" content="klklkl"><meta name="copyright" content="klklkl"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="CUDA编程入门  在线cuda编程网站  基础知识 nvidia-smi指令  更多的命令问ai吧~ 从cpp到cuda编程 一般的程序: 12345678&#x2F;&#x2F;hello.cpp#include &lt;stdio.h&gt;int main()&amp;#123;    &#x2F;&#x2F;不能使用cout    printf(&quot;Hello world\n&quot;);    return 0;&amp;#125">
<meta property="og:type" content="article">
<meta property="og:title" content="cuda编程入门">
<meta property="og:url" content="https://klklkl10086.github.io/klklkl10086.github.io/2025/11/18/cuda%E7%BC%96%E7%A8%8B%E5%85%A5%E9%97%A8/index.html">
<meta property="og:site_name" content="klklkl&#39;s blogs">
<meta property="og:description" content="CUDA编程入门  在线cuda编程网站  基础知识 nvidia-smi指令  更多的命令问ai吧~ 从cpp到cuda编程 一般的程序: 12345678&#x2F;&#x2F;hello.cpp#include &lt;stdio.h&gt;int main()&amp;#123;    &#x2F;&#x2F;不能使用cout    printf(&quot;Hello world\n&quot;);    return 0;&amp;#125">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pic2.zhimg.com/v2-f9b32b95385eedbdb58389bbbce5de39_r.jpg">
<meta property="article:published_time" content="2025-11-18T12:46:44.000Z">
<meta property="article:modified_time" content="2025-11-27T14:16:41.352Z">
<meta property="article:author" content="klklkl">
<meta property="article:tag" content="CPP">
<meta property="article:tag" content="CUDA">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pic2.zhimg.com/v2-f9b32b95385eedbdb58389bbbce5de39_r.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://klklkl10086.github.io/klklkl10086.github.io/2025/11/18/cuda%E7%BC%96%E7%A8%8B%E5%85%A5%E9%97%A8/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: {"limitDay":500,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'cuda编程入门',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-11-27 22:16:41'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="klklkl's blogs" type="application/atom+xml">
<link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://pic2.zhimg.com/v2-f9b32b95385eedbdb58389bbbce5de39_r.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">22</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">26</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">8</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://pic1.zhimg.com/80/v2-908b61a41ec4bebe17a04468dcf5d834_720w.webp')"><nav id="nav"><span id="blog-info"><a href="/" title="klklkl's blogs"><img class="site-icon" src="https://pic2.zhimg.com/v2-f9b32b95385eedbdb58389bbbce5de39_r.jpg# image"/><span class="site-name">klklkl's blogs</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">cuda编程入门</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-11-18T12:46:44.000Z" title="发表于 2025-11-18 20:46:44">2025-11-18</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2025-11-27T14:16:41.352Z" title="更新于 2025-11-27 22:16:41">2025-11-27</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%A1%A5%E5%AE%8C/">研究生补完</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="cuda编程入门"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h1 id="cuda编程入门">CUDA编程入门</h1>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://leetgpu.com/">在线cuda编程网站</a></p>
</blockquote>
<h1 id="基础知识">基础知识</h1>
<h2 id="nvidia-smi指令">nvidia-smi指令</h2>
<p><img src="image-20251118210332910.png" alt="image-20251118210332910"></p>
<p>更多的命令问ai吧~</p>
<h2 id="从cpp到cuda编程">从cpp到cuda编程</h2>
<p>一般的程序:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//hello.cpp</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//不能使用cout</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Hello world\n&quot;</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">g++ hello.cpp -o hello</span><br><span class="line">./hello</span><br></pre></td></tr></table></figure>
<p>nvcc:</p>
<ul>
<li>安装cuda即可使用nvcc</li>
<li>nvcc支持纯c++代码编译</li>
<li>编译扩展名为.cu的cuda文件</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">nvcc hello.cu -o hello</span><br></pre></td></tr></table></figure>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//hello.cu</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//不能使用cout</span></span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Hello world\n&quot;</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">root@autodl-container-6bc24a9b46-e72ce521:~/cudacode/2.1lesson<span class="comment"># nvcc hello.cu -o hellocu</span></span><br><span class="line">root@autodl-container-6bc24a9b46-e72ce521:~/cudacode/2.1lesson<span class="comment"># ./hellocu</span></span><br><span class="line">Hello world</span><br></pre></td></tr></table></figure>
<h2 id="核函数">核函数</h2>
<blockquote>
<p><strong>核函数</strong> 是在GPU上并行执行的函数。它是CUDA编程模型的核心，允许你将计算任务分解成成千上万个并行线程，从而利用GPU的大规模并行计算能力。</p>
</blockquote>
<ol>
<li><strong>执行位置</strong>：在GPU上并行执行,具有异步性。</li>
<li><strong>并行方式</strong>：通过大量线程以“单指令多线程”的模式并行执行。</li>
<li><strong>调用方式</strong>：由CPU（主机）调用，但运行在GPU（设备）上。</li>
<li><strong>返回类型</strong>：必须返回 <code>void</code>。</li>
<li>只能访问GPU内存</li>
<li>不行使用变长参数 静态变量 函数指针</li>
<li>不支持c++的iostream</li>
</ol>
<h3 id="定义">定义</h3>
<p>使用 <code>__global__</code> 关键字来声明一个函数为核函数。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 使用 __global__ 关键字定义核函数</span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">myKernel</span><span class="params">(<span class="type">int</span> *array, <span class="type">int</span> value)</span> </span>&#123;<span class="comment">//global和void可以互换位置</span></span><br><span class="line">    <span class="comment">// 计算当前线程的全局索引</span></span><br><span class="line">    <span class="type">int</span> idx = threadIdx.x + blockIdx.x * blockDim.x;</span><br><span class="line">    <span class="comment">// 每个线程对数组中不同的元素进行操作</span></span><br><span class="line">    array[idx] += value;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>执行空间说明符</code>:</p>
<ul>
<li><code>__global__</code>：在GPU上执行，<strong>由CPU调用</strong>。这是我们定义核函数时使用的。</li>
<li><code>__device__</code>：在GPU上执行，<strong>由GPU调用</strong>（通常是从其他 <code>__device__</code> 函数或核函数中调用）。</li>
<li><code>__host__</code>：在CPU上执行，由CPU调用（就是普通的C++函数）。可以同时使用 <code>__host__ __device__</code>，使得该函数既能被CPU调用，也能被GPU编译。</li>
</ul>
<p><strong>cuda程序编写流程</strong>:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">(<span class="type">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	主机代码</span><br><span class="line">	核函数调用</span><br><span class="line">	主机代码</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="调用">调用</h3>
<p>调用核函数时，需要使用特殊的尖括号语法 <code>&lt;&lt;&lt; ... &gt;&gt;&gt;</code> 来指定<strong>执行配置</strong>，即如何组织线程来执行这个核函数。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">hello_from_gpu</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Hello from gpu\n&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 在主机(CPU)代码中调用核函数</span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">     <span class="comment">//执行配置 &lt;&lt;&lt;网格大小, 块大小, 动态共享内存大小(可选), 流(可选)&gt;&gt;&gt;</span></span><br><span class="line">    hello_from_gpu&lt;&lt;&lt;<span class="number">1</span>,<span class="number">1</span>&gt;&gt;&gt;();<span class="comment">//1个线程块 块中有1个线程</span></span><br><span class="line">    </span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();<span class="comment">//同步函数,等待gpu执行完毕,主机与gpu的同步</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;   </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>线程 线程块 网格</code></p>
<ul>
<li><strong>线程</strong>：最基本的执行单元。</li>
<li><strong>线程块</strong>：一组线程的集合。
<ul>
<li>一个块内的线程可以：
<ul>
<li>通过<strong>共享内存</strong>高效地交换数据。</li>
<li>使用 <code>__syncthreads()</code> 函数进行同步。</li>
</ul>
</li>
<li>线程块之间是<strong>相互独立</strong>的。它们可以以<strong>任何顺序</strong>、在<strong>任何SM（流多处理器）</strong> 上执行。</li>
<li>一个线程块的执行不应依赖于另一个线程块的结果。这是CUDA编程模型的一个基本假设。</li>
</ul>
</li>
<li><strong>网格</strong>：所有线程块的集合。一个核函数启动的所有线程块构成一个网格。</li>
</ul>
<h2 id="cuda线程模型">CUDA线程模型</h2>
<h3 id="线程模型结构">线程模型结构</h3>
<p><img src="kts7mm1vzn.png" alt="img"></p>
<blockquote>
<p>CUDA线程模型是一个<strong>分层的并行编程模型</strong>，它将并行任务分解为多个层次，从粗粒度到细粒度依次是：<br>
<strong>网格(grid) → 线程块(block) → 线程束 → 线程</strong></p>
</blockquote>
<p><code>线程</code>:最基本的执行单元</p>
<ul>
<li>每个线程是独立的执行路径</li>
<li>执行相同的核函数代码，但处理不同的数据</li>
<li>有自己的程序计数器、寄存器组和本地内存</li>
</ul>
<p><code>线程块</code>:协作的线程组</p>
<ul>
<li><strong>共享内存</strong>：块内所有线程共享一块快速片上内存</li>
<li><strong>同步能力</strong>：通过 <code>__syncthreads()</code> 实现块内线程同步</li>
<li><strong>独立性</strong>：不同线程块之间相互独立，可以乱序执行</li>
<li><strong>维度</strong>：可以是1D、2D或3D布局</li>
</ul>
<p><code>网格</code>: 完整的执行单元</p>
<ul>
<li>包含所有执行<strong>同一核函数</strong>的线程块</li>
<li>当核函数启动时，就定义了一个网格</li>
<li>网格中的线程块被调度到不同的SM上执行</li>
</ul>
<p><strong>注意</strong>:</p>
<ul>
<li>线程分块是逻辑划分,物理上线程不分块</li>
<li>配置线程: &lt;&lt;&lt;网格大小,线程块大小&gt;&gt;&gt;</li>
<li>最大允许线程块大小1024</li>
<li>最大允许网格大小<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>2</mn><mn>32</mn></msup><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">2^{32}-1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.897438em;vertical-align:-0.08333em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span><span class="mord mtight">2</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">1</span></span></span></span>(针对一维网格)</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">hello_from_gpu</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Hello from gpu\n&quot;</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    hello_from_gpu&lt;&lt;&lt;<span class="number">2</span>,<span class="number">4</span>&gt;&gt;&gt;();<span class="comment">//2线程块  一个块里有4个线程</span></span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;   </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">root@autodl-container-6bc24a9b46-e72ce521:~/cudacode/2.3lesson<span class="comment"># nvcc ex1.cu -o ex1</span></span><br><span class="line">root@autodl-container-6bc24a9b46-e72ce521:~/cudacode/2.3lesson<span class="comment"># ./ex1</span></span><br><span class="line">Hello from gpu</span><br><span class="line">Hello from gpu</span><br><span class="line">Hello from gpu</span><br><span class="line">Hello from gpu</span><br><span class="line">Hello from gpu</span><br><span class="line">Hello from gpu</span><br><span class="line">Hello from gpu</span><br><span class="line">Hello from gpu</span><br></pre></td></tr></table></figure>
<h3 id="一维线程模型">一维线程模型</h3>
<ol>
<li>每个线程在<code>核函数</code>中都有一个<strong>唯一的身份标识</strong></li>
<li>每个线程的唯一标识由&lt;&lt;&lt;grid_size,block_size&gt;&gt;&gt;确定,grid_size,block_size保存在内建变量(build-in variable) <strong>目前考虑的是一维情况</strong>
<ul>
<li>gridDim.x : 该数值等于执行配置中变量grid_size的值</li>
<li>blockDim.x : 该数值等于执行配置中变量block_size的值</li>
</ul>
</li>
<li>线程索引保存为内建变量
<ul>
<li>blockIdx.x : 该变量指定一个线程在一个网格中的线程块索引值,范围为0~gridDim.x-1</li>
<li>threadIdx.x : 该变量指定一个线程在一个线程块中的线程索引值,范围为0~blockDim.x-1</li>
</ul>
</li>
</ol>
<p><img src="image-20251119182816235.png" alt="image-20251119182816235"></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 一维网格和块 计算线程索引</span></span><br><span class="line"><span class="type">int</span> idx = blockIdx.x * blockDim.x + threadIdx.x;</span><br><span class="line"><span class="comment">//在上述例子中,idx的范围为0~7</span></span><br></pre></td></tr></table></figure>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">hello_from_gpu</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    </span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> bid = blockIdx.x;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> tid = threadIdx.x;</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> id = bid*blockDim.x+tid;</span><br><span class="line">    <span class="built_in">printf</span>(<span class="string">&quot;Hello from block %d and thread %d, global id %d\n&quot;</span>,bid,tid,id);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    hello_from_gpu&lt;&lt;&lt;<span class="number">2</span>,<span class="number">4</span>&gt;&gt;&gt;();<span class="comment">//2线程块  一个块里有4个线程</span></span><br><span class="line">    <span class="built_in">cudaDeviceSynchronize</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;   </span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">root@autodl-container-6bc24a9b46-e72ce521:~/cudacode/2.3lesson<span class="comment"># nvcc ex1.cu -o ex1</span></span><br><span class="line">root@autodl-container-6bc24a9b46-e72ce521:~/cudacode/2.3lesson<span class="comment"># ./ex1</span></span><br><span class="line">Hello from block 1 and thread 0, global <span class="built_in">id</span> 4</span><br><span class="line">Hello from block 1 and thread 1, global <span class="built_in">id</span> 5</span><br><span class="line">Hello from block 1 and thread 2, global <span class="built_in">id</span> 6</span><br><span class="line">Hello from block 1 and thread 3, global <span class="built_in">id</span> 7</span><br><span class="line">Hello from block 0 and thread 0, global <span class="built_in">id</span> 0</span><br><span class="line">Hello from block 0 and thread 1, global <span class="built_in">id</span> 1</span><br><span class="line">Hello from block 0 and thread 2, global <span class="built_in">id</span> 2</span><br><span class="line">Hello from block 0 and thread 3, global <span class="built_in">id</span> 3</span><br></pre></td></tr></table></figure>
<h3 id="多维线程">多维线程</h3>
<p><img src="image-20251119184229406.png" alt="image-20251119184229406"></p>
<p><img src="image-20251119184244577.png" alt="image-20251119184244577"></p>
<p><img src="image-20251119184425543.png" alt="image-20251119184425543"></p>
<p><img src="image-20251119184524180.png" alt="image-20251119184524180"></p>
<p><img src="image-20251119185219189.png" alt="image-20251119185219189"></p>
<p><strong>网格和线程块的限制条件</strong>:</p>
<p><img src="image-20251119185323653.png" alt="image-20251119185323653"></p>
<h3 id="线程全局索引计算方式">线程全局索引计算方式</h3>
<h4 id="线程全局索引">线程全局索引</h4>
<p><strong>一维网格 一维线程块:</strong></p>
<p><img src="image-20251119185717207.png" alt="image-20251119185717207"></p>
<p><strong>二维网格 二维线程块:</strong><br>
<img src="image-20251119191022263.png" alt="image-20251119191022263"></p>
<p><strong>三维网格  三维线程块:</strong></p>
<p><img src="image-20251119191154297.png" alt="image-20251119191154297"></p>
<h3 id="nvcc编译流程和gpu计算能力">NVCC编译流程和GPU计算能力</h3>
<h4 id="nvcc编译流程">NVCC编译流程</h4>
<blockquote>
<p>NVCC（NVIDIA CUDA Compiler）的编译流程分为多个阶段，主要处理主机端（Host，CPU）代码和设备端（Device，GPU）代码的混合编译。</p>
</blockquote>
<p><img src="image-20251119205631454.png" alt="image-20251119205631454"></p>
<p><img src="cuda-compilation-from-cu-to-executable.png" alt="CUDA Compilation Trajectory"></p>
<h3 id="ptx">PTX</h3>
<blockquote>
<p>PTX（Parallel Thread Execution）是CUDA平台为基于 GPU的通用计算而定义的虚拟机和指令集</p>
</blockquote>
<ul>
<li><strong>虚拟指令集</strong>：不直接对应具体GPU硬件，而是抽象中间表示</li>
<li><strong>跨架构兼容</strong>：可在不同代际的NVIDIA GPU上运行</li>
</ul>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">CUDA源码 (.cu) </span><br><span class="line">    ↓ NVCC编译</span><br><span class="line">PTX代码 (.<span class="built_in">ptx</span>)           ← 跨架构中间表示</span><br><span class="line">    ↓ GPU驱动JIT编译  </span><br><span class="line">具体架构SASS代码          ← 特定GPU机器码</span><br><span class="line">    ↓ GPU执行</span><br></pre></td></tr></table></figure>
<ol>
<li>nvcc编译命令总是使用两个体系结构:一个是虚拟的中间体系结构，另一个是实际的GPU体系结构</li>
<li>虚拟架构更像是对应用所需的GPU功能的声明</li>
<li>虚拟架构应该尽可能选择低----适配更多实际GPU</li>
<li>真实架构应该尽可能选择高----充分发挥GPU性能</li>
<li>虚拟架构应低于真实架构</li>
</ol>
<h3 id="gpu架构和计算能力">GPU架构和计算能力</h3>
<p><img src="image-20251119210546435.png" alt="image-20251119210546435"></p>
<p>并非GPU 的计算能力越高，性能就越高</p>
<h3 id="cuda程序兼容性问题">CUDA程序兼容性问题</h3>
<h4 id="虚拟架构计算能力">虚拟架构计算能力</h4>
<p><img src="image-20251119212759966.png" alt="image-20251119212759966"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nvcc helloworld.cu –o helloworld -<span class="built_in">arch</span>=compute_61</span><br><span class="line"><span class="comment">#编译出的可执行文件helloworld可以在计算能力&gt;=6.1的GPU上面执行，在计算能力小于6.1的GPU则不能执行。</span></span><br></pre></td></tr></table></figure>
<h4 id="真实架构计算能力">真实架构计算能力</h4>
<p><img src="image-20251119213013409.png" alt="image-20251119213013409"></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nvcc helloworld.cu –o helloworld -<span class="built_in">arch</span>=compute_61 -code=sm_60</span><br><span class="line"><span class="comment">#指定的真实架构能力为6.0虚拟架构为6.1违反(3)</span></span><br></pre></td></tr></table></figure>
<h4 id="多个gpu版本编译">多个GPU版本编译</h4>
<p><img src="image-20251119213142292.png" alt="image-20251119213142292"></p>
<h4 id="nvcc即时编译">NVCC即时编译</h4>
<ol>
<li>
<p>在运行可执行文件时，从保留的PTX代码临时编译出cubin文件</p>
</li>
<li>
<p>在可执行文件中保留PTX代码，nvcc编译指令指定所保留的PTX代码虚拟架构:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-gencode <span class="built_in">arch</span>=compute_XY ,code=compute_XY</span><br><span class="line"><span class="comment">#两个计算能力都是虚拟架构计算能力</span></span><br><span class="line"><span class="comment">#两个虚拟架构计算能力必须一致</span></span><br></pre></td></tr></table></figure>
</li>
</ol>
<h4 id="nvcc编译默认计算能力">NVCC编译默认计算能力</h4>
<p>不同版本CUDA编译器在编译CUDA代码时，都有一个默认计算能力</p>
<p><img src="image-20251119213458783.png" alt="image-20251119213458783"></p>
<h1 id="cuda编程简单实践">CUDA编程简单实践</h1>
<h2 id="cuda矩阵加法运算程序">CUDA矩阵加法运算程序</h2>
<h3 id="cuda程序基本框架">CUDA程序基本框架</h3>
<p><img src="image-20251119214814450.png" alt="image-20251119214814450"></p>
<h4 id="设置gpu设备">设置GPU设备</h4>
<p><strong>获取GPU数量:</strong></p>
<p><img src="image-20251119214943840.png" alt="image-20251119214943840"></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> iDeviceCount = <span class="number">0</span>;</span><br><span class="line"><span class="built_in">cudaGetDeviceCount</span>(&amp;iDeviceCount);</span><br></pre></td></tr></table></figure>
<p><strong>设置GPU执行时使用的设备</strong>:</p>
<p><img src="image-20251119215012498.png" alt="image-20251119215012498"></p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> iDev = <span class="number">0</span>;</span><br><span class="line"><span class="built_in">cudaSetDevice</span>(iDev)</span><br></pre></td></tr></table></figure>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//检查计算机GPU的数量</span></span><br><span class="line">    <span class="type">int</span> iDeviceCount=<span class="number">0</span>;</span><br><span class="line">    cudaError_t error = <span class="built_in">cudaGetDeviceCount</span>(&amp;iDeviceCount);</span><br><span class="line">    <span class="keyword">if</span>(error!= cudaSuccess || iDeviceCount ==<span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;NO CUDA campatable GPU found\n&quot;</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">-1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;The count of GPUs is %d \n&quot;</span>,iDeviceCount);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//设置执行</span></span><br><span class="line">    <span class="type">int</span> iDev = <span class="number">0</span>;</span><br><span class="line">    error = <span class="built_in">cudaSetDevice</span>(iDev);</span><br><span class="line">    <span class="keyword">if</span>(error!=cudaSuccess)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;fail to set GPU 0 for computing.\n&quot;</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">-1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;set GPU 0 for computing.\n&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="内存管理">内存管理</h3>
<blockquote>
<p>CUDA通过内存分配 数据传递 内存初始化 内存释放进行内存管理</p>
</blockquote>
<p><img src="image-20251119215530502.png" alt="image-20251119215530502"></p>
<h4 id="内存分配">内存分配</h4>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__host__ __device__ cudaError_t <span class="title">cudaMalloc</span><span class="params">(<span class="type">void</span>** devPtr, <span class="type">size_t</span> size)</span></span>;</span><br></pre></td></tr></table></figure>
<ul>
<li><code>devPtr</code>: 指向设备内存指针的指针。函数会将分配的设备内存地址存储在这个指针中。</li>
<li><code>size</code>: 要分配的内存大小（以字节为单位）。</li>
<li>返回 <code>cudaError_t</code> 类型值，表示函数执行的状态。如果成功，返回 <code>cudaSuccess</code></li>
</ul>
<p>使用 cudaMalloc 分配的内存必须使用 cudaFree 来释放。</p>
<h4 id="数据拷贝">数据拷贝</h4>
<p><code>cudaMemcpy</code> 用于在<strong>主机内存</strong>和<strong>设备内存</strong>之间复制数据。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__host__ cudaError_t <span class="title">cudaMemcpy</span><span class="params">(<span class="type">void</span>* dst, <span class="type">const</span> <span class="type">void</span>* src, <span class="type">size_t</span> count, cudaMemcpyKind kind)</span></span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>dst</code>: 目标内存地址</li>
<li><code>src</code>: 源内存地址</li>
<li><code>count</code>: 要复制的字节数</li>
<li><code>kind</code>: 复制方向，指定数据是从主机到设备，还是从设备到主机等。这是一个枚举类型，主要取值有：
<ul>
<li><code>cudaMemcpyHostToHost</code>： 主机 → 主机</li>
<li><code>cudaMemcpyHostToDevice</code>： 主机 → 设备</li>
<li><code>cudaMemcpyDeviceToHost</code>： 设备 → 主机</li>
<li><code>cudaMemcpyDeviceToDevice</code>： 设备 → 设备</li>
<li><code>cudaMemcpyDefault</code>： 根据指针地址自动判断方向（默认方式只允许在支持统一虚拟寻址的系统上使用）</li>
</ul>
</li>
</ul>
<h4 id="内存初始化">内存初始化</h4>
<p><code>cudaMemset</code> 用于设置设备内存的值，功能类似于标准 C 的 <code>memset</code> 函数。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__host__ cudaError_t <span class="title">cudaMemset</span><span class="params">(<span class="type">void</span>* devPtr, <span class="type">int</span> value, <span class="type">size_t</span> count)</span></span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>devPtr</code>: 指向设备内存的指针</li>
<li><code>value</code>: 要设置的值（以字节为单位设置）</li>
<li><code>count</code>: 要设置的字节数</li>
</ul>
<p><code>cudaMemset</code> 是按<strong>字节</strong>操作的，这与标准 <code>memset</code> 一致。这对于初始化 <code>char</code> 数组或清零内存非常有用，但对于设置非字节类型的特定值（如将所有 <code>int</code> 设置为 <code>1</code>）则不方便。</p>
<h4 id="内存释放">内存释放</h4>
<p><code>cudaFree</code> 用于释放由 <code>cudaMalloc</code>、<code>cudaMallocManaged</code> 等函数分配的<strong>设备内存</strong>。</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">__host__ __device__ cudaError_t <span class="title">cudaFree</span><span class="params">(<span class="type">void</span>* devPtr)</span></span></span><br></pre></td></tr></table></figure>
<ul>
<li>
<p><code>devPtr</code>: 指向要释放的设备内存的指针</p>
</li>
<li>
<p>只能释放由 CUDA 内存分配函数分配的内存。</p>
</li>
<li>
<p>试图释放无效的指针或已经释放的指针会导致未定义行为（通常是运行时错误）。</p>
</li>
<li>
<p>在主机程序退出前释放所有分配的设备内存是一个好习惯，但现代 CUDA 驱动在程序结束时也会自动清理。</p>
</li>
</ul>
<h3 id="自定义设备函数">自定义设备函数</h3>
<p><code>设备函数（device function）</code></p>
<ul>
<li>定义只能执行在GPU设备上的函数为设备函数</li>
<li>设备函数只能被核函数或其他设备函数调用</li>
<li>设备函数用device修饰</li>
</ul>
<p><code>核函数（kernel function）</code></p>
<ul>
<li>用global修饰的函数称为核函数，一般由主机调用，在设备中执行</li>
<li>global 修饰符既不能和host同时使用，也不可与device 同时使用</li>
</ul>
<p><code>主机函数（host function） </code></p>
<ul>
<li>主机端的普通 C++ 函数可用 <strong>host</strong> 修饰</li>
<li>对于主机端的函数， __host__修饰符可省略</li>
<li>可以用 <strong>host</strong> 和 <strong>device</strong> 同时修饰一个函数减少冗余代码。编译器会针对主机 和设备分别编译该函数。</li>
</ul>
<h3 id="一维矩阵加法">一维矩阵加法</h3>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">addFromGPU</span><span class="params">(<span class="type">float</span> *A ,<span class="type">float</span> *B ,<span class="type">float</span> *C,<span class="type">const</span> <span class="type">int</span> N)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> bid = blockIdx.x;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> tid = threadIdx.x;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> id = tid+bid*blockDim.x;</span><br><span class="line"></span><br><span class="line">    C[id]=A[id]+B[id];</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">initialData</span><span class="params">(<span class="type">float</span> *addr,<span class="type">int</span> elemCount)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;elemCount;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        addr[i]=(<span class="type">float</span>)(<span class="built_in">rand</span>()&amp; <span class="number">0xff</span>) / <span class="number">10.f</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">setGPU</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//1.检查计算机GPU的数量</span></span><br><span class="line">    <span class="type">int</span> iDeviceCount=<span class="number">0</span>;</span><br><span class="line">    cudaError_t error = <span class="built_in">cudaGetDeviceCount</span>(&amp;iDeviceCount);</span><br><span class="line">    <span class="keyword">if</span>(error!= cudaSuccess || iDeviceCount ==<span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;NO CUDA campatable GPU found\n&quot;</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">-1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;The count of GPUs is %d \n&quot;</span>,iDeviceCount);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.设置执行</span></span><br><span class="line">    <span class="type">int</span> iDev = <span class="number">0</span>;</span><br><span class="line">    error = <span class="built_in">cudaSetDevice</span>(iDev);</span><br><span class="line">    <span class="keyword">if</span>(error!=cudaSuccess)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;fail to set GPU 0 for computing.\n&quot;</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">-1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;set GPU 0 for computing.\n&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">   <span class="comment">// 1.设置GPU设备</span></span><br><span class="line">   <span class="built_in">setGPU</span>();</span><br><span class="line"></span><br><span class="line">   <span class="comment">//2.分配主机内存和设备内存,并初始化</span></span><br><span class="line">   <span class="type">int</span> iElemCount = <span class="number">512</span>;        <span class="comment">//一个矩阵的元素数目</span></span><br><span class="line">   <span class="type">size_t</span> stBytesCount = iElemCount * <span class="built_in">sizeof</span>(<span class="type">float</span>); <span class="comment">//字节数</span></span><br><span class="line"></span><br><span class="line">   <span class="comment">//(1)分配主机内存并初始化</span></span><br><span class="line">    <span class="type">float</span> *fpHost_A,*fpHost_B,*fpHost_C;</span><br><span class="line">    fpHost_A = (<span class="type">float</span>*) <span class="built_in">malloc</span>(stBytesCount);</span><br><span class="line">    fpHost_B = (<span class="type">float</span>*) <span class="built_in">malloc</span>(stBytesCount);</span><br><span class="line">    fpHost_C = (<span class="type">float</span>*) <span class="built_in">malloc</span>(stBytesCount);</span><br><span class="line">    <span class="keyword">if</span>(fpHost_A!=<span class="literal">NULL</span>&amp;&amp;fpHost_B!=<span class="literal">NULL</span>&amp;&amp;fpHost_C!=<span class="literal">NULL</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">//主机内存初始化为0</span></span><br><span class="line">        <span class="built_in">memset</span>(fpHost_A,<span class="number">0</span>,stBytesCount);</span><br><span class="line">        <span class="built_in">memset</span>(fpHost_B,<span class="number">0</span>,stBytesCount);</span><br><span class="line">        <span class="built_in">memset</span>(fpHost_C,<span class="number">0</span>,stBytesCount);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Fail to allocate host memory!\n&quot;</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">-1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// (2)分配设备内存 并初始化</span></span><br><span class="line">    <span class="type">float</span> *fpDevice_A,*fpDevice_B,*fpDevice_C;</span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">float</span>**)&amp;fpDevice_A,stBytesCount);</span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">float</span>**)&amp;fpDevice_B,stBytesCount);</span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">float</span>**)&amp;fpDevice_C,stBytesCount);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (fpDevice_A != <span class="literal">NULL</span> &amp;&amp; fpDevice_B != <span class="literal">NULL</span> &amp;&amp; fpDevice_C != <span class="literal">NULL</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">cudaMemset</span>(fpDevice_A,<span class="number">0</span>,stBytesCount);</span><br><span class="line">        <span class="built_in">cudaMemset</span>(fpDevice_B,<span class="number">0</span>,stBytesCount);</span><br><span class="line">        <span class="built_in">cudaMemset</span>(fpDevice_C,<span class="number">0</span>,stBytesCount);</span><br><span class="line">    &#125;<span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;fail to allocate memory\n&quot;</span>);</span><br><span class="line">        <span class="built_in">free</span>(fpHost_A);</span><br><span class="line">        <span class="built_in">free</span>(fpHost_B);</span><br><span class="line">        <span class="built_in">free</span>(fpHost_C);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">-1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3.初始化主机中的数据</span></span><br><span class="line">    <span class="built_in">srand</span>(<span class="number">666</span>);</span><br><span class="line">    <span class="built_in">initialData</span>(fpHost_A,iElemCount);</span><br><span class="line">    <span class="built_in">initialData</span>(fpHost_B,iElemCount);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//4.从主机复制数据到设备</span></span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(fpDevice_A,fpHost_A,stBytesCount,cudaMemcpyHostToDevice);</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(fpDevice_B,fpHost_B,stBytesCount,cudaMemcpyHostToDevice);</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(fpDevice_C,fpHost_C,stBytesCount,cudaMemcpyHostToDevice);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//5.调用核函数在设备上计算</span></span><br><span class="line">    <span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">32</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">grid</span><span class="params">(iElemCount/<span class="number">32</span>)</span></span>;<span class="comment">//保证每个线程负责一个数据</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    addFromGPU&lt;&lt;&lt;grid,block&gt;&gt;&gt;(fpDevice_A,fpDevice_B,fpDevice_C,iElemCount);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">//cudaDeviceSynchronize();//保证GPU执行完之后再执行以下语句</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">//6.将计算得到的数据从设备传给主机</span></span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(fpHost_C,fpDevice_C,stBytesCount,cudaMemcpyDeviceToHost);</span><br><span class="line">    <span class="comment">//隐式保证GPU执行完之后再执行 ,因此可以省略cudaDeviceSynchronize();</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>;i&lt;<span class="number">10</span>;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;idx=%2d\tmatrix_A:%.2f\tmatrix_B:%.2f\tresult=%.2f\n&quot;</span>, i+<span class="number">1</span>, fpHost_A[i], fpHost_B[i], fpHost_C[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//7.释放主机与设备内存</span></span><br><span class="line">    <span class="built_in">free</span>(fpHost_A);</span><br><span class="line">    <span class="built_in">free</span>(fpHost_B);</span><br><span class="line">    <span class="built_in">free</span>(fpHost_C);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaFree</span>(fpDevice_A);</span><br><span class="line">    <span class="built_in">cudaFree</span>(fpDevice_B);</span><br><span class="line">    <span class="built_in">cudaFree</span>(fpDevice_C);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaDeviceReset</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>Tips:</p>
<ol>
<li>
<p>上述代码人为设置一个线程可以负责一个数据,但当数据个数由512变化为513时,<code>dim3 grid(iElemCount/32);</code>就无法保证一个线程负责一个数据,因此要改为<code>dim3 grid((iElemCount + block.x - 1) / 32);</code>,即向上取整,此时线程个数会多于矩阵元素个数,因此在GPU上的运算函数要附加if条件。</p>
</li>
<li>
<p>上述核函数可以拆分为核函数调用设备函数的形式</p>
</li>
<li>
<p>结合上述两条,修改后的代码如下:</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="function">__device__  <span class="type">float</span> <span class="title">add</span><span class="params">(<span class="type">const</span> <span class="type">float</span> x,<span class="type">const</span> <span class="type">float</span> y)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> x+y;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">__global__ <span class="type">void</span> <span class="title">addFromGPU</span><span class="params">(<span class="type">float</span> *A ,<span class="type">float</span> *B ,<span class="type">float</span> *C,<span class="type">const</span> <span class="type">int</span> N)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> bid = blockIdx.x;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> tid = threadIdx.x;</span><br><span class="line">    <span class="type">const</span> <span class="type">int</span> id = tid+bid*blockDim.x;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(id&gt;=N) <span class="keyword">return</span> ;</span><br><span class="line">    C[id]=<span class="built_in">add</span>(A[id],B[id]);</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">initialData</span><span class="params">(<span class="type">float</span> *addr,<span class="type">int</span> elemCount)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i=<span class="number">0</span>;i&lt;elemCount;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        addr[i]=(<span class="type">float</span>)(<span class="built_in">rand</span>()&amp; <span class="number">0xff</span>) / <span class="number">10.f</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">setGPU</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">//1.检查计算机GPU的数量</span></span><br><span class="line">    <span class="type">int</span> iDeviceCount=<span class="number">0</span>;</span><br><span class="line">    cudaError_t error = <span class="built_in">cudaGetDeviceCount</span>(&amp;iDeviceCount);</span><br><span class="line">    <span class="keyword">if</span>(error!= cudaSuccess || iDeviceCount ==<span class="number">0</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;NO CUDA campatable GPU found\n&quot;</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">-1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;The count of GPUs is %d \n&quot;</span>,iDeviceCount);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//2.设置执行</span></span><br><span class="line">    <span class="type">int</span> iDev = <span class="number">0</span>;</span><br><span class="line">    error = <span class="built_in">cudaSetDevice</span>(iDev);</span><br><span class="line">    <span class="keyword">if</span>(error!=cudaSuccess)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;fail to set GPU 0 for computing.\n&quot;</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">-1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;set GPU 0 for computing.\n&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">   <span class="comment">// 1.设置GPU设备</span></span><br><span class="line">   <span class="built_in">setGPU</span>();</span><br><span class="line"></span><br><span class="line">   <span class="comment">//2.分配主机内存和设备内存,并初始化</span></span><br><span class="line">   <span class="type">int</span> iElemCount = <span class="number">513</span>;        <span class="comment">//一个矩阵的元素数目</span></span><br><span class="line">   <span class="type">size_t</span> stBytesCount = iElemCount * <span class="built_in">sizeof</span>(<span class="type">float</span>); <span class="comment">//字节数</span></span><br><span class="line"></span><br><span class="line">   <span class="comment">//(1)分配主机内存并初始化</span></span><br><span class="line">    <span class="type">float</span> *fpHost_A,*fpHost_B,*fpHost_C;</span><br><span class="line">    fpHost_A = (<span class="type">float</span>*) <span class="built_in">malloc</span>(stBytesCount);</span><br><span class="line">    fpHost_B = (<span class="type">float</span>*) <span class="built_in">malloc</span>(stBytesCount);</span><br><span class="line">    fpHost_C = (<span class="type">float</span>*) <span class="built_in">malloc</span>(stBytesCount);</span><br><span class="line">    <span class="keyword">if</span>(fpHost_A!=<span class="literal">NULL</span>&amp;&amp;fpHost_B!=<span class="literal">NULL</span>&amp;&amp;fpHost_C!=<span class="literal">NULL</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">//主机内存初始化为0</span></span><br><span class="line">        <span class="built_in">memset</span>(fpHost_A,<span class="number">0</span>,stBytesCount);</span><br><span class="line">        <span class="built_in">memset</span>(fpHost_B,<span class="number">0</span>,stBytesCount);</span><br><span class="line">        <span class="built_in">memset</span>(fpHost_C,<span class="number">0</span>,stBytesCount);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;Fail to allocate host memory!\n&quot;</span>);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">-1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// (2)分配设备内存 并初始化</span></span><br><span class="line">    <span class="type">float</span> *fpDevice_A,*fpDevice_B,*fpDevice_C;</span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">float</span>**)&amp;fpDevice_A,stBytesCount);</span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">float</span>**)&amp;fpDevice_B,stBytesCount);</span><br><span class="line">    <span class="built_in">cudaMalloc</span>((<span class="type">float</span>**)&amp;fpDevice_C,stBytesCount);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (fpDevice_A != <span class="literal">NULL</span> &amp;&amp; fpDevice_B != <span class="literal">NULL</span> &amp;&amp; fpDevice_C != <span class="literal">NULL</span>)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">cudaMemset</span>(fpDevice_A,<span class="number">0</span>,stBytesCount);</span><br><span class="line">        <span class="built_in">cudaMemset</span>(fpDevice_B,<span class="number">0</span>,stBytesCount);</span><br><span class="line">        <span class="built_in">cudaMemset</span>(fpDevice_C,<span class="number">0</span>,stBytesCount);</span><br><span class="line">    &#125;<span class="keyword">else</span></span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;fail to allocate memory\n&quot;</span>);</span><br><span class="line">        <span class="built_in">free</span>(fpHost_A);</span><br><span class="line">        <span class="built_in">free</span>(fpHost_B);</span><br><span class="line">        <span class="built_in">free</span>(fpHost_C);</span><br><span class="line">        <span class="built_in">exit</span>(<span class="number">-1</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//3.初始化主机中的数据</span></span><br><span class="line">    <span class="built_in">srand</span>(<span class="number">666</span>);</span><br><span class="line">    <span class="built_in">initialData</span>(fpHost_A,iElemCount);</span><br><span class="line">    <span class="built_in">initialData</span>(fpHost_B,iElemCount);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//4.从主机复制数据到设备</span></span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(fpDevice_A,fpHost_A,stBytesCount,cudaMemcpyHostToDevice);</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(fpDevice_B,fpHost_B,stBytesCount,cudaMemcpyHostToDevice);</span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(fpDevice_C,fpHost_C,stBytesCount,cudaMemcpyHostToDevice);</span><br><span class="line"></span><br><span class="line">    <span class="comment">//5.调用核函数在设备上计算</span></span><br><span class="line">    <span class="function">dim3 <span class="title">block</span><span class="params">(<span class="number">32</span>)</span></span>;</span><br><span class="line">    <span class="function">dim3 <span class="title">grid</span><span class="params">((iElemCount<span class="number">-1</span>+block.x)/block.x)</span></span>;<span class="comment">//保证每个线程负责一个数据</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    addFromGPU&lt;&lt;&lt;grid,block&gt;&gt;&gt;(fpDevice_A,fpDevice_B,fpDevice_C,iElemCount);</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    <span class="comment">//6.将计算得到的数据从设备传给主机</span></span><br><span class="line">    <span class="built_in">cudaMemcpy</span>(fpHost_C,fpDevice_C,stBytesCount,cudaMemcpyDeviceToHost);<span class="comment">//隐式保证GPU执行完之后再执行</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>;i&lt;<span class="number">10</span>;i++)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="built_in">printf</span>(<span class="string">&quot;idx=%2d\tmatrix_A:%.2f\tmatrix_B:%.2f\tresult=%.2f\n&quot;</span>, i+<span class="number">1</span>, fpHost_A[i], fpHost_B[i], fpHost_C[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">//7.释放主机与设备内存</span></span><br><span class="line">    <span class="built_in">free</span>(fpHost_A);</span><br><span class="line">    <span class="built_in">free</span>(fpHost_B);</span><br><span class="line">    <span class="built_in">free</span>(fpHost_C);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaFree</span>(fpDevice_A);</span><br><span class="line">    <span class="built_in">cudaFree</span>(fpDevice_B);</span><br><span class="line">    <span class="built_in">cudaFree</span>(fpDevice_C);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">cudaDeviceReset</span>();</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h2 id="cuda常用api">CUDA常用API</h2>
<h3 id="错误检测">错误检测</h3>
<h3 id="计时">计时</h3>
<h3 id="查询gpu信息">查询GPU信息</h3>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://klklkl10086.github.io/klklkl10086.github.io">klklkl</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://klklkl10086.github.io/klklkl10086.github.io/2025/11/18/cuda%E7%BC%96%E7%A8%8B%E5%85%A5%E9%97%A8/">https://klklkl10086.github.io/klklkl10086.github.io/2025/11/18/cuda%E7%BC%96%E7%A8%8B%E5%85%A5%E9%97%A8/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://klklkl10086.github.io/klklkl10086.github.io" target="_blank">klklkl's blogs</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/CPP/">CPP</a><a class="post-meta__tags" href="/tags/CUDA/">CUDA</a></div><div class="post_share"><div class="social-share" data-image="https://pic2.zhimg.com/v2-f9b32b95385eedbdb58389bbbce5de39_r.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2025/11/27/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/" title="Pytorch深度学习实践"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">Pytorch深度学习实践</div></div></a></div><div class="next-post pull-right"><a href="/2025/10/01/java%E5%9F%BA%E7%A1%80/" title="javaSE"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">javaSE</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2026/01/31/Linux%E7%8E%AF%E5%A2%83%E7%BC%96%E7%A8%8B/" title="Linux环境编程"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2026-01-31</div><div class="title">Linux环境编程</div></div></a></div><div><a href="/2026/01/31/STL%E6%8B%BE%E9%81%97/" title="STL拾遗"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2026-01-31</div><div class="title">STL拾遗</div></div></a></div><div><a href="/2025/10/01/cpp11-%E5%A4%9A%E7%BA%BF%E7%A8%8B/" title="c++11--thread库介绍"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-01</div><div class="title">c++11--thread库介绍</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://pic2.zhimg.com/v2-f9b32b95385eedbdb58389bbbce5de39_r.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">klklkl</div><div class="author-info__description">student</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">22</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">26</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">8</div></a></div><div class="card-info-social-icons is-center"><a class="social-icon" href="mailto:2488926297@qq.com" target="_blank" title="Email"><i class="fa-regular fa-envelope" style="color: #000000;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Nice to meet you!</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#cuda%E7%BC%96%E7%A8%8B%E5%85%A5%E9%97%A8"><span class="toc-number">1.</span> <span class="toc-text">CUDA编程入门</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86"><span class="toc-number">2.</span> <span class="toc-text">基础知识</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#nvidia-smi%E6%8C%87%E4%BB%A4"><span class="toc-number">2.1.</span> <span class="toc-text">nvidia-smi指令</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8Ecpp%E5%88%B0cuda%E7%BC%96%E7%A8%8B"><span class="toc-number">2.2.</span> <span class="toc-text">从cpp到cuda编程</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A0%B8%E5%87%BD%E6%95%B0"><span class="toc-number">2.3.</span> <span class="toc-text">核函数</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9A%E4%B9%89"><span class="toc-number">2.3.1.</span> <span class="toc-text">定义</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B0%83%E7%94%A8"><span class="toc-number">2.3.2.</span> <span class="toc-text">调用</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#cuda%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.4.</span> <span class="toc-text">CUDA线程模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B%E7%BB%93%E6%9E%84"><span class="toc-number">2.4.1.</span> <span class="toc-text">线程模型结构</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E7%BB%B4%E7%BA%BF%E7%A8%8B%E6%A8%A1%E5%9E%8B"><span class="toc-number">2.4.2.</span> <span class="toc-text">一维线程模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%9A%E7%BB%B4%E7%BA%BF%E7%A8%8B"><span class="toc-number">2.4.3.</span> <span class="toc-text">多维线程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BA%BF%E7%A8%8B%E5%85%A8%E5%B1%80%E7%B4%A2%E5%BC%95%E8%AE%A1%E7%AE%97%E6%96%B9%E5%BC%8F"><span class="toc-number">2.4.4.</span> <span class="toc-text">线程全局索引计算方式</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%BA%BF%E7%A8%8B%E5%85%A8%E5%B1%80%E7%B4%A2%E5%BC%95"><span class="toc-number">2.4.4.1.</span> <span class="toc-text">线程全局索引</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#nvcc%E7%BC%96%E8%AF%91%E6%B5%81%E7%A8%8B%E5%92%8Cgpu%E8%AE%A1%E7%AE%97%E8%83%BD%E5%8A%9B"><span class="toc-number">2.4.5.</span> <span class="toc-text">NVCC编译流程和GPU计算能力</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#nvcc%E7%BC%96%E8%AF%91%E6%B5%81%E7%A8%8B"><span class="toc-number">2.4.5.1.</span> <span class="toc-text">NVCC编译流程</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#ptx"><span class="toc-number">2.4.6.</span> <span class="toc-text">PTX</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#gpu%E6%9E%B6%E6%9E%84%E5%92%8C%E8%AE%A1%E7%AE%97%E8%83%BD%E5%8A%9B"><span class="toc-number">2.4.7.</span> <span class="toc-text">GPU架构和计算能力</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#cuda%E7%A8%8B%E5%BA%8F%E5%85%BC%E5%AE%B9%E6%80%A7%E9%97%AE%E9%A2%98"><span class="toc-number">2.4.8.</span> <span class="toc-text">CUDA程序兼容性问题</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%99%9A%E6%8B%9F%E6%9E%B6%E6%9E%84%E8%AE%A1%E7%AE%97%E8%83%BD%E5%8A%9B"><span class="toc-number">2.4.8.1.</span> <span class="toc-text">虚拟架构计算能力</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9C%9F%E5%AE%9E%E6%9E%B6%E6%9E%84%E8%AE%A1%E7%AE%97%E8%83%BD%E5%8A%9B"><span class="toc-number">2.4.8.2.</span> <span class="toc-text">真实架构计算能力</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E4%B8%AAgpu%E7%89%88%E6%9C%AC%E7%BC%96%E8%AF%91"><span class="toc-number">2.4.8.3.</span> <span class="toc-text">多个GPU版本编译</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#nvcc%E5%8D%B3%E6%97%B6%E7%BC%96%E8%AF%91"><span class="toc-number">2.4.8.4.</span> <span class="toc-text">NVCC即时编译</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#nvcc%E7%BC%96%E8%AF%91%E9%BB%98%E8%AE%A4%E8%AE%A1%E7%AE%97%E8%83%BD%E5%8A%9B"><span class="toc-number">2.4.8.5.</span> <span class="toc-text">NVCC编译默认计算能力</span></a></li></ol></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#cuda%E7%BC%96%E7%A8%8B%E7%AE%80%E5%8D%95%E5%AE%9E%E8%B7%B5"><span class="toc-number">3.</span> <span class="toc-text">CUDA编程简单实践</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#cuda%E7%9F%A9%E9%98%B5%E5%8A%A0%E6%B3%95%E8%BF%90%E7%AE%97%E7%A8%8B%E5%BA%8F"><span class="toc-number">3.1.</span> <span class="toc-text">CUDA矩阵加法运算程序</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#cuda%E7%A8%8B%E5%BA%8F%E5%9F%BA%E6%9C%AC%E6%A1%86%E6%9E%B6"><span class="toc-number">3.1.1.</span> <span class="toc-text">CUDA程序基本框架</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%AE%BE%E7%BD%AEgpu%E8%AE%BE%E5%A4%87"><span class="toc-number">3.1.1.1.</span> <span class="toc-text">设置GPU设备</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86"><span class="toc-number">3.1.2.</span> <span class="toc-text">内存管理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D"><span class="toc-number">3.1.2.1.</span> <span class="toc-text">内存分配</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%8B%B7%E8%B4%9D"><span class="toc-number">3.1.2.2.</span> <span class="toc-text">数据拷贝</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%86%85%E5%AD%98%E5%88%9D%E5%A7%8B%E5%8C%96"><span class="toc-number">3.1.2.3.</span> <span class="toc-text">内存初始化</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%86%85%E5%AD%98%E9%87%8A%E6%94%BE"><span class="toc-number">3.1.2.4.</span> <span class="toc-text">内存释放</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%87%AA%E5%AE%9A%E4%B9%89%E8%AE%BE%E5%A4%87%E5%87%BD%E6%95%B0"><span class="toc-number">3.1.3.</span> <span class="toc-text">自定义设备函数</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E7%BB%B4%E7%9F%A9%E9%98%B5%E5%8A%A0%E6%B3%95"><span class="toc-number">3.1.4.</span> <span class="toc-text">一维矩阵加法</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#cuda%E5%B8%B8%E7%94%A8api"><span class="toc-number">3.2.</span> <span class="toc-text">CUDA常用API</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%94%99%E8%AF%AF%E6%A3%80%E6%B5%8B"><span class="toc-number">3.2.1.</span> <span class="toc-text">错误检测</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%A1%E6%97%B6"><span class="toc-number">3.2.2.</span> <span class="toc-text">计时</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9F%A5%E8%AF%A2gpu%E4%BF%A1%E6%81%AF"><span class="toc-number">3.2.3.</span> <span class="toc-text">查询GPU信息</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/01/31/Linux%E7%8E%AF%E5%A2%83%E7%BC%96%E7%A8%8B/" title="Linux环境编程">Linux环境编程</a><time datetime="2026-01-31T12:51:35.000Z" title="发表于 2026-01-31 20:51:35">2026-01-31</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/01/31/STL%E6%8B%BE%E9%81%97/" title="STL拾遗">STL拾遗</a><time datetime="2026-01-31T12:50:18.000Z" title="发表于 2026-01-31 20:50:18">2026-01-31</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/12/07/%E4%BB%8E0%E5%AE%9E%E7%8E%B0LLM/" title="从0实现Transformer">从0实现Transformer</a><time datetime="2025-12-07T13:07:22.000Z" title="发表于 2025-12-07 21:07:22">2025-12-07</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/11/27/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/" title="Pytorch深度学习实践">Pytorch深度学习实践</a><time datetime="2025-11-27T14:15:20.000Z" title="发表于 2025-11-27 22:15:20">2025-11-27</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/11/18/cuda%E7%BC%96%E7%A8%8B%E5%85%A5%E9%97%A8/" title="cuda编程入门">cuda编程入门</a><time datetime="2025-11-18T12:46:44.000Z" title="发表于 2025-11-18 20:46:44">2025-11-18</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://pic1.zhimg.com/80/v2-908b61a41ec4bebe17a04468dcf5d834_720w.webp')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2026 By klklkl</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>