<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>从0实现Transformer | klklkl's blogs</title><meta name="author" content="klklkl"><meta name="copyright" content="klklkl"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="大模型《从零到一》长视频系列  框架基本知识  深度学习_0   深度学习_1   深度学习_2  代码  1.实现的是解码器结构的Transformer而非原始论文的encode-decode 2.和原始论文不太一样,并且存在许多隐含错误  12345678910111213141516171819202122232425262728293031323334">
<meta property="og:type" content="article">
<meta property="og:title" content="从0实现Transformer">
<meta property="og:url" content="https://klklkl10086.github.io/klklkl10086.github.io/2025/12/07/%E4%BB%8E0%E5%AE%9E%E7%8E%B0LLM/index.html">
<meta property="og:site_name" content="klklkl&#39;s blogs">
<meta property="og:description" content="大模型《从零到一》长视频系列  框架基本知识  深度学习_0   深度学习_1   深度学习_2  代码  1.实现的是解码器结构的Transformer而非原始论文的encode-decode 2.和原始论文不太一样,并且存在许多隐含错误  12345678910111213141516171819202122232425262728293031323334">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://pic2.zhimg.com/v2-f9b32b95385eedbdb58389bbbce5de39_r.jpg">
<meta property="article:published_time" content="2025-12-07T13:07:22.000Z">
<meta property="article:modified_time" content="2026-01-04T12:28:47.313Z">
<meta property="article:author" content="klklkl">
<meta property="article:tag" content="深度学习">
<meta property="article:tag" content="Transformer">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://pic2.zhimg.com/v2-f9b32b95385eedbdb58389bbbce5de39_r.jpg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://klklkl10086.github.io/klklkl10086.github.io/2025/12/07/%E4%BB%8E0%E5%AE%9E%E7%8E%B0LLM/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: {"limitDay":500,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '刚刚',
    min: '分钟前',
    hour: '小时前',
    day: '天前',
    month: '个月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/flickr-justified-gallery/dist/fjGallery.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '从0实现Transformer',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2026-01-04 20:28:47'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
    win.getCSS = (url,id = false) => new Promise((resolve, reject) => {
      const link = document.createElement('link')
      link.rel = 'stylesheet'
      link.href = url
      if (id) link.id = id
      link.onerror = reject
      link.onload = link.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        link.onload = link.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(link)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><meta name="generator" content="Hexo 6.3.0"><link rel="alternate" href="/atom.xml" title="klklkl's blogs" type="application/atom+xml">
<link href="https://cdn.bootcss.com/KaTeX/0.11.1/katex.min.css" rel="stylesheet" /></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="https://pic2.zhimg.com/v2-f9b32b95385eedbdb58389bbbce5de39_r.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">21</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">27</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('https://pic1.zhimg.com/80/v2-908b61a41ec4bebe17a04468dcf5d834_720w.webp')"><nav id="nav"><span id="blog-info"><a href="/" title="klklkl's blogs"><img class="site-icon" src="https://pic2.zhimg.com/v2-f9b32b95385eedbdb58389bbbce5de39_r.jpg# image"/><span class="site-name">klklkl's blogs</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 时间轴</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 标签</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分类</span></a></div><div class="menus_item"><a class="site-page" href="/Gallery/"><i class="fa-fw fas fa-images"></i><span> 照片</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fas fa-heart"></i><span> 关于</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">从0实现Transformer</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">发表于</span><time class="post-meta-date-created" datetime="2025-12-07T13:07:22.000Z" title="发表于 2025-12-07 21:07:22">2025-12-07</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">更新于</span><time class="post-meta-date-updated" datetime="2026-01-04T12:28:47.313Z" title="更新于 2026-01-04 20:28:47">2026-01-04</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/%E7%A0%94%E7%A9%B6%E7%94%9F%E8%A1%A5%E5%AE%8C/">研究生补完</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="从0实现Transformer"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">阅读量:</span><span id="busuanzi_value_page_pv"><i class="fa-solid fa-spinner fa-spin"></i></span></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><blockquote>
<p><a target="_blank" rel="noopener" href="https://space.bilibili.com/3546611527453161/lists/2386239?type=season">大模型《从零到一》长视频系列</a></p>
</blockquote>
<h1 id="框架基本知识">框架基本知识</h1>
<figure>
<img src="深度学习_0.jpg" alt="深度学习_0" /><figcaption aria-hidden="true">深度学习_0</figcaption>
</figure>
<figure>
<img src="深度学习_1.jpg" alt="深度学习_1" /><figcaption aria-hidden="true">深度学习_1</figcaption>
</figure>
<figure>
<img src="深度学习_2.jpg" alt="深度学习_2" /><figcaption aria-hidden="true">深度学习_2</figcaption>
</figure>
<h1 id="代码">代码</h1>
<blockquote>
<p>1.实现的是解码器结构的Transformer而非原始论文的encode-decode</p>
<p>2.和原始论文不太一样,并且存在许多隐含错误</p>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Decoder-Only  transformer</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> tiktoken</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 超参数</span></span><br><span class="line">batch_size = <span class="number">4</span></span><br><span class="line">context_len = <span class="number">16</span></span><br><span class="line">d_model = <span class="number">64</span>  <span class="comment"># 每个token的维度</span></span><br><span class="line">num_blocks = <span class="number">8</span> <span class="comment">#循环多少次</span></span><br><span class="line">num_heads = <span class="number">4</span> <span class="comment">#分为几个头</span></span><br><span class="line">learning_rate = <span class="number">1e-3</span></span><br><span class="line">dropout = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line">max_iters = <span class="number">500</span><span class="comment">#迭代多少次</span></span><br><span class="line">eval_interval = <span class="number">50</span></span><br><span class="line">eval_iters = <span class="number">100</span></span><br><span class="line">device = <span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line">TORCH_SEED = <span class="number">1337</span></span><br><span class="line">torch.manual_seed(TORCH_SEED)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># get dataset</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&#x27;/home/lizy/graduate/Transformer_learning/sales_textbook.txt&#x27;</span>):</span><br><span class="line">    url = <span class="string">&#x27;https://huggingface.co/datasets/goendalf666/sales-textbook_for_convincing_and_selling/raw/main/sales_textbook.txt&#x27;</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;/home/lizy/graduate/Transformer_learning/sales_textbook.txt&#x27;</span>,<span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(requests.get(url).content)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;/home/lizy/graduate/Transformer_learning/sales_textbook.txt&#x27;</span>,<span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    text = f.read()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">encoding = tiktoken.get_encoding(<span class="string">&quot;cl100k_base&quot;</span>)</span><br><span class="line">vocab_size = encoding.n_vocab  <span class="comment"># tiktoken的词汇表大小</span></span><br><span class="line"></span><br><span class="line">tokenized_text = encoding.encode(text)</span><br><span class="line"><span class="comment"># max_token_value = tokenized_text.max().item() + 1</span></span><br><span class="line">tokenized_text=torch.tensor(tokenized_text,dtype=torch.long,device=device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_idex = <span class="built_in">int</span>(<span class="built_in">len</span>(tokenized_text) * <span class="number">0.9</span>)</span><br><span class="line">train_data = tokenized_text[:train_idex]</span><br><span class="line">valid_data = tokenized_text[train_idex:]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FeedforwardNetwork</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,d_model,d_ff</span>):</span><br><span class="line">        <span class="built_in">super</span>(FeedforwardNetwork,self).__init__()</span><br><span class="line">        self.linear1 = nn.Linear(d_model,d_ff)</span><br><span class="line">        self.ReLU = nn.ReLU()</span><br><span class="line">        self.linear2 = nn.Linear(d_ff,d_model)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x=self.linear1(x)</span><br><span class="line">        x=self.ReLU(x)</span><br><span class="line">        x=self.linear2(x)</span><br><span class="line">        x=self.dropout(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">ScaledDotProductAttention</span>(nn.Module):  </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.Wq = nn.Linear(d_model,d_model//num_heads)</span><br><span class="line">        self.Wk = nn.Linear(d_model,d_model//num_heads)</span><br><span class="line">        self.Wv = nn.Linear(d_model,d_model//num_heads)</span><br><span class="line"></span><br><span class="line">        self.register_buffer(<span class="string">&#x27;mask&#x27;</span>,torch.tril(torch.ones(context_len,context_len)))</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line"></span><br><span class="line">        B, T, C = x.shape  <span class="comment"># Batch size, Time steps(current context_length), Channels(dimensions)</span></span><br><span class="line">        <span class="keyword">assert</span> T &lt;= context_len</span><br><span class="line">        <span class="keyword">assert</span> C == d_model</span><br><span class="line"></span><br><span class="line">        Q = self.Wq(x)</span><br><span class="line">        K = self.Wk(x)</span><br><span class="line">        V = self.Wv(x)</span><br><span class="line">        <span class="comment">#单头注意力</span></span><br><span class="line">        attention = Q @ K.transpose(-<span class="number">2</span>,-<span class="number">1</span>) / math.sqrt(d_model//num_heads)</span><br><span class="line">        attention = attention.masked_fill(self.mask[:T, :T] == <span class="number">0</span>, <span class="built_in">float</span>(<span class="string">&#x27;-inf&#x27;</span>))</span><br><span class="line">        attention = F.softmax(attention,dim=-<span class="number">1</span>)</span><br><span class="line">        attention = self.dropout(attention)</span><br><span class="line">        <span class="keyword">return</span> attention @ V  <span class="comment">#signal head output  (B,T,head_dim)</span></span><br><span class="line">    </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.heads = nn.ModuleList([ScaledDotProductAttention() <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_heads)]) <span class="comment">#多头</span></span><br><span class="line">        self.projection_layer = nn.Linear(d_model,d_model)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        heads_output =  [head(x) <span class="keyword">for</span> head <span class="keyword">in</span> self.heads]</span><br><span class="line">        out = torch.cat(heads_output,dim=-<span class="number">1</span>)</span><br><span class="line">        out = self.projection_layer(out)</span><br><span class="line">        out = self.dropout(out)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line">    </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.multi_head_attention_layer = MultiHeadAttention()</span><br><span class="line">        self.ffn = FeedforwardNetwork(d_model,d_model*<span class="number">4</span>)</span><br><span class="line">        self.layer_norm_1=nn.LayerNorm(d_model)</span><br><span class="line">        self.layer_norm_2=nn.LayerNorm(d_model)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x = x + self.multi_head_attention_layer(x)</span><br><span class="line">        x = self.layer_norm_1(x)</span><br><span class="line">        x = x + self.ffn(x)</span><br><span class="line">        x = self.layer_norm_2(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerLanguageModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment">#self.token_embedding_lookup_table = nn.Embedding(max_token_value+1,d_model)</span></span><br><span class="line">        <span class="comment"># 应该使用tokenizer的实际词汇表大小</span></span><br><span class="line">        self.token_embedding_lookup_table = nn.Embedding(vocab_size, d_model)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.transformer_blocks = nn.Sequential(*(</span><br><span class="line">            [TransformerBlock() <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_blocks)]</span><br><span class="line">           <span class="comment"># + [nn.LayerNorm(d_model)]#Different from original paper, here we add a final layer norm after all the blocks</span></span><br><span class="line">        ))</span><br><span class="line"></span><br><span class="line">        self.language_model_out_linear_layer = nn.Linear(d_model,vocab_size)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,idx,targets=<span class="literal">None</span></span>):</span><br><span class="line">        B , T = idx.shape</span><br><span class="line"></span><br><span class="line">        position_encoding_lookup_table = torch.zeros(context_len,d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>,context_len,dtype=torch.<span class="built_in">float</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>() * (-math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        position_encoding_lookup_table[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        position_encoding_lookup_table[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line"></span><br><span class="line">        position_embedding = position_encoding_lookup_table[:T, :].to(device)</span><br><span class="line"></span><br><span class="line">        x = self.token_embedding_lookup_table(idx) + position_embedding</span><br><span class="line">        x = self.transformer_blocks(x)</span><br><span class="line"></span><br><span class="line">        logits = self.language_model_out_linear_layer(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> targets <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            B, T, C = logits.shape</span><br><span class="line">            logits_reshaped = logits.view(B * T, C)</span><br><span class="line">            targets_reshaped = targets.view(B * T)</span><br><span class="line">            loss = F.cross_entropy(<span class="built_in">input</span>=logits_reshaped, target=targets_reshaped)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            loss = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">return</span> logits, loss</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generate</span>(<span class="params">self, idx, max_new_tokens, temperature=<span class="number">0.8</span>, top_k=<span class="number">50</span></span>):</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(max_new_tokens):</span><br><span class="line">            idx_crop = idx[:, -context_len:] <span class="keyword">if</span> idx.size(<span class="number">1</span>) &gt; context_len <span class="keyword">else</span> idx</span><br><span class="line">            </span><br><span class="line">            logits, _ = self(idx_crop)</span><br><span class="line">            logits = logits[:, -<span class="number">1</span>, :] / temperature</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 可选：top-k采样，提高质量</span></span><br><span class="line">            <span class="keyword">if</span> top_k <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                v, _ = torch.topk(logits, <span class="built_in">min</span>(top_k, logits.size(-<span class="number">1</span>)))</span><br><span class="line">                logits[logits &lt; v[:, [-<span class="number">1</span>]]] = <span class="built_in">float</span>(<span class="string">&#x27;-inf&#x27;</span>)</span><br><span class="line">            </span><br><span class="line">            probs = F.softmax(logits, dim=-<span class="number">1</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 采样</span></span><br><span class="line">            idx_next = torch.multinomial(probs, num_samples=<span class="number">1</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 确保token在有效范围内</span></span><br><span class="line">            idx_next = torch.clamp(idx_next, <span class="number">0</span>, vocab_size - <span class="number">1</span>)</span><br><span class="line">            </span><br><span class="line">            idx = torch.cat((idx, idx_next), dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> idx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = TransformerLanguageModel()</span><br><span class="line">model = model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_batch</span>(<span class="params">split</span>):</span><br><span class="line">    data = train_data <span class="keyword">if</span> split == <span class="string">&#x27;train&#x27;</span> <span class="keyword">else</span> valid_data</span><br><span class="line">    idxs = torch.randint(low=<span class="number">0</span>, high=<span class="built_in">len</span>(data) - context_len, size=(batch_size,))</span><br><span class="line">    x = torch.stack([data[idx:idx + context_len] <span class="keyword">for</span> idx <span class="keyword">in</span> idxs]).to(device)</span><br><span class="line">    y = torch.stack([data[idx + <span class="number">1</span>:idx + context_len + <span class="number">1</span>] <span class="keyword">for</span> idx <span class="keyword">in</span> idxs]).to(device)</span><br><span class="line">    <span class="keyword">return</span> x, y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate loss</span></span><br><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">estimate_loss</span>():</span><br><span class="line">    out = &#123;&#125;</span><br><span class="line">    model.<span class="built_in">eval</span>() <span class="comment"># 用于将模型设置为评估模式</span></span><br><span class="line">    <span class="keyword">for</span> split <span class="keyword">in</span> [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;valid&#x27;</span>]:</span><br><span class="line">        losses = torch.zeros(eval_iters)</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(eval_iters):</span><br><span class="line">            x_batch, y_batch = get_batch(split)</span><br><span class="line">            logits, loss = model(x_batch, y_batch)</span><br><span class="line">            losses[k] = loss.item()</span><br><span class="line">        out[split] = losses.mean()</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">optimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate)</span><br><span class="line">tracked_losses = <span class="built_in">list</span>()</span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(max_iters):</span><br><span class="line">    <span class="keyword">if</span> step % eval_iters == <span class="number">0</span> <span class="keyword">or</span> step == max_iters - <span class="number">1</span>:</span><br><span class="line">        losses = estimate_loss()</span><br><span class="line">        tracked_losses.append(losses)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Step:&#x27;</span>, step, <span class="string">&#x27;Training Loss:&#x27;</span>, <span class="built_in">round</span>(losses[<span class="string">&#x27;train&#x27;</span>].item(), <span class="number">3</span>), <span class="string">&#x27;Validation Loss:&#x27;</span>,</span><br><span class="line">              <span class="built_in">round</span>(losses[<span class="string">&#x27;valid&#x27;</span>].item(), <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">    xb, yb = get_batch(<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">    logits, loss = model(xb, yb)</span><br><span class="line">    optimizer.zero_grad(set_to_none=<span class="literal">True</span>)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Save the model state dictionary</span></span><br><span class="line">torch.save(model.state_dict(), <span class="string">&#x27;model-ckpt.pt&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">start = <span class="string">&#x27;The salesperson&#x27;</span></span><br><span class="line">start_ids = encoding.encode(start)</span><br><span class="line">x = (torch.tensor(start_ids, dtype=torch.long, device=device)[<span class="literal">None</span>, ...])</span><br><span class="line">y = model.generate(x, max_new_tokens=<span class="number">100</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;---------------&#x27;</span>)</span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    generated_text = encoding.decode(y[<span class="number">0</span>].tolist())</span><br><span class="line"><span class="keyword">except</span> KeyError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;解码时遇到无效token，尝试忽略: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="comment"># 忽略无效token</span></span><br><span class="line">    valid_tokens = []</span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> y[<span class="number">0</span>].tolist():</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="comment"># 检查token是否有效</span></span><br><span class="line">            <span class="keyword">if</span> <span class="number">0</span> &lt;= token &lt; vocab_size:</span><br><span class="line">                valid_tokens.append(token)</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">    generated_text = encoding.decode(valid_tokens)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(encoding.decode(y[<span class="number">0</span>].tolist()))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;---------------&#x27;</span>)</span><br></pre></td></tr></table></figure>
<h1 id="效果">效果</h1>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">Step: 0 Training Loss: 11.705 Validation Loss: 11.714</span><br><span class="line">Step: 100 Training Loss: 6.766 Validation Loss: 7.352</span><br><span class="line">Step: 200 Training Loss: 6.263 Validation Loss: 6.846</span><br><span class="line">Step: 300 Training Loss: 5.778 Validation Loss: 6.467</span><br><span class="line">Step: 400 Training Loss: 5.465 Validation Loss: 6.212</span><br><span class="line">Step: 500 Training Loss: 5.272 Validation Loss: 6.144</span><br><span class="line">Step: 600 Training Loss: 4.886 Validation Loss: 5.937</span><br><span class="line">Step: 700 Training Loss: 4.84 Validation Loss: 5.865</span><br><span class="line">Step: 800 Training Loss: 4.691 Validation Loss: 5.746</span><br><span class="line">Step: 900 Training Loss: 4.508 Validation Loss: 5.787</span><br><span class="line">Step: 1000 Training Loss: 4.504 Validation Loss: 5.763</span><br><span class="line">Step: 1100 Training Loss: 4.412 Validation Loss: 5.549</span><br><span class="line">Step: 1200 Training Loss: 4.237 Validation Loss: 5.498</span><br><span class="line">Step: 1300 Training Loss: 4.289 Validation Loss: 5.356</span><br><span class="line">Step: 1400 Training Loss: 4.156 Validation Loss: 5.582</span><br><span class="line">Step: 1500 Training Loss: 4.024 Validation Loss: 5.308</span><br><span class="line">Step: 1600 Training Loss: 4.047 Validation Loss: 5.403</span><br><span class="line">Step: 1700 Training Loss: 3.915 Validation Loss: 5.366</span><br><span class="line">Step: 1800 Training Loss: 3.909 Validation Loss: 5.254</span><br><span class="line">Step: 1900 Training Loss: 3.917 Validation Loss: 5.205</span><br><span class="line">Step: 2000 Training Loss: 3.836 Validation Loss: 5.26</span><br><span class="line">Step: 2100 Training Loss: 3.771 Validation Loss: 5.132</span><br><span class="line">Step: 2200 Training Loss: 3.793 Validation Loss: 5.269</span><br><span class="line">Step: 2300 Training Loss: 3.579 Validation Loss: 5.268</span><br><span class="line">Step: 2400 Training Loss: 3.661 Validation Loss: 5.207</span><br><span class="line">Step: 2500 Training Loss: 3.625 Validation Loss: 5.102</span><br><span class="line">Step: 2600 Training Loss: 3.6 Validation Loss: 4.921</span><br><span class="line">Step: 2700 Training Loss: 3.502 Validation Loss: 5.028</span><br><span class="line">Step: 2800 Training Loss: 3.503 Validation Loss: 4.943</span><br><span class="line">Step: 2900 Training Loss: 3.437 Validation Loss: 4.943</span><br><span class="line">Step: 3000 Training Loss: 3.441 Validation Loss: 4.992</span><br><span class="line">Step: 3100 Training Loss: 3.348 Validation Loss: 4.964</span><br><span class="line">Step: 3200 Training Loss: 3.338 Validation Loss: 4.93</span><br><span class="line">Step: 3300 Training Loss: 3.377 Validation Loss: 5.002</span><br><span class="line">Step: 3400 Training Loss: 3.205 Validation Loss: 4.944</span><br><span class="line">Step: 3500 Training Loss: 3.353 Validation Loss: 4.872</span><br><span class="line">Step: 3600 Training Loss: 3.279 Validation Loss: 4.988</span><br><span class="line">Step: 3700 Training Loss: 3.267 Validation Loss: 5.028</span><br><span class="line">Step: 3800 Training Loss: 3.307 Validation Loss: 4.805</span><br><span class="line">Step: 3900 Training Loss: 3.184 Validation Loss: 4.879</span><br><span class="line">Step: 4000 Training Loss: 3.231 Validation Loss: 4.911</span><br><span class="line">Step: 4100 Training Loss: 3.128 Validation Loss: 4.968</span><br><span class="line">Step: 4200 Training Loss: 3.089 Validation Loss: 4.928</span><br><span class="line">Step: 4300 Training Loss: 3.092 Validation Loss: 4.938</span><br><span class="line">Step: 4400 Training Loss: 3.136 Validation Loss: 4.978</span><br><span class="line">Step: 4500 Training Loss: 3.047 Validation Loss: 4.791</span><br><span class="line">Step: 4600 Training Loss: 2.983 Validation Loss: 4.931</span><br><span class="line">Step: 4700 Training Loss: 3.052 Validation Loss: 4.975</span><br><span class="line">Step: 4800 Training Loss: 3.027 Validation Loss: 4.828</span><br><span class="line">Step: 4900 Training Loss: 3.001 Validation Loss: 4.792</span><br><span class="line">Step: 4999 Training Loss: 2.933 Validation Loss: 4.921</span><br><span class="line">---------------</span><br><span class="line">The salesperson should create a connection with potential customer that your customers, ensuring the customer with your customers, and avoiding jargon. By actively listening, you can establish a solidify and build trust. For example, you can build trust, and credibility, credibility, build credibility and credibility, as them to make a more favorable, you can effectively communicate your unique circumstances, take action promptly to the customer&#x27;s perspective and understanding. By utilizing the ideal solution that your product or service limitations, you can build trust and increase</span><br><span class="line">---------------</span><br></pre></td></tr></table></figure>
<h1 id="post-layernorm-vs-pre-layernorm">Post-LayerNorm vs Pre-LayerNorm</h1>
<blockquote>
<p>后来的 Transformer 模型普遍从原始的 <strong>Post-LayerNorm</strong> 改为 <strong>Pre-LayerNorm</strong></p>
</blockquote>
<h2 id="两种-layernorm-位置对比">两种 LayerNorm 位置对比</h2>
<p><strong>原始 Transformer（Post-LayerNorm）</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 原始论文的顺序：子层 → LayerNorm → 残差连接</span></span><br><span class="line">x = x + Sublayer(x)      <span class="comment"># 先计算子层输出</span></span><br><span class="line">x = LayerNorm(x)         <span class="comment"># 再归一化</span></span><br></pre></td></tr></table></figure>
<p><strong>现代 Transformer（Pre-LayerNorm）</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 现代实现的顺序：LayerNorm → 子层 → 残差连接</span></span><br><span class="line">x_norm = LayerNorm(x)    <span class="comment"># 先归一化</span></span><br><span class="line">x = x + Sublayer(x_norm) <span class="comment"># 再计算子层并残差连接</span></span><br></pre></td></tr></table></figure>
<h2 id="为什么要改为-pre-layernorm">为什么要改为 Pre-LayerNorm？</h2>
<h3 id="训练稳定性大幅提升"><strong>训练稳定性大幅提升</strong></h3>
<p><strong>Post-LayerNorm 的问题：</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 梯度流经的路径：</span></span><br><span class="line">损失 → 层归一化 → 子层 → 输入</span><br><span class="line"><span class="comment"># 梯度必须先通过层归一化，这可能导致：</span></span><br><span class="line"><span class="comment"># 1. 梯度消失/爆炸（尤其深层网络）</span></span><br><span class="line"><span class="comment"># 2. 需要精细的初始化</span></span><br><span class="line"><span class="comment"># 3. 学习率需要小心调整</span></span><br></pre></td></tr></table></figure></p>
<p><strong>Pre-LayerNorm 的优势：</strong> <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 梯度流经的路径：</span></span><br><span class="line">损失 → 子层 → 层归一化 → 输入</span><br><span class="line"><span class="comment"># 梯度直接通过子层，然后才到归一化</span></span><br><span class="line"><span class="comment"># 梯度流动更平滑，训练更稳定</span></span><br></pre></td></tr></table></figure></p>
<h3 id="收敛速度更快"><strong>收敛速度更快</strong></h3>
<p><strong>实际效果对比：</strong> - <strong>Post-LayerNorm</strong>：可能需要更多训练步数才能收敛 - <strong>Pre-LayerNorm</strong>：通常收敛更快，需要的训练步数更少</p>
<h3 id="梯度传播更直接"><strong>梯度传播更直接</strong></h3>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Post-LayerNorm 梯度路径：</span><br><span class="line">损失 → LN → Attention/FFN → 输入</span><br><span class="line">      ↓</span><br><span class="line">  梯度先经过LN的缩放操作</span><br><span class="line">  可能放大或缩小梯度值</span><br><span class="line">  </span><br><span class="line">Pre-LayerNorm 梯度路径：</span><br><span class="line">损失 → Attention/FFN → LN → 输入</span><br><span class="line">      ↓</span><br><span class="line">  梯度直接传到子层</span><br><span class="line">  LN只影响前向传播，不影响梯度回传</span><br></pre></td></tr></table></figure>
<h2 id="梯度计算对比">梯度计算对比</h2>
<p><strong>Post-LayerNorm（层归一化在残差连接后）</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x = LayerNorm(x + Sublayer(x))</span><br></pre></td></tr></table></figure>
<p><strong>反向传播的路径:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">梯度流向：损失 → LayerNorm → (残差连接 + Sublayer) → 输入</span><br><span class="line">                 ↓</span><br><span class="line">             梯度必须先通过LayerNorm</span><br><span class="line">             它的导数可能放大或缩小梯度</span><br><span class="line">  </span><br><span class="line"><span class="comment"># 数学理解</span></span><br><span class="line">设：y = LN(x + f(x))，其中LN是LayerNorm</span><br><span class="line"></span><br><span class="line">梯度：∂L/∂x = ∂L/∂y × ∂LN/∂(x+f(x)) × (<span class="number">1</span> + ∂f/∂x)</span><br><span class="line"></span><br><span class="line">注意：∂LN/∂(x+f(x)) 包含：</span><br><span class="line"><span class="number">1.</span> <span class="number">1</span>/σ 缩放（标准差倒数）</span><br><span class="line"><span class="number">2.</span> 减去均值的影响</span><br><span class="line"><span class="number">3.</span> gamma参数的缩放</span><br></pre></td></tr></table></figure>
<ul>
<li><strong>当输入x+f(x)的方差σ很小时，1/σ很大 做完乘积→ 梯度爆炸</strong></li>
<li><strong>当方差σ很大时，1/σ很小 做完乘积→ 梯度消失</strong></li>
</ul>
<p><strong>Pre-LayerNorm（层归一化在残差连接前）</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">x = x + Sublayer(LayerNorm(x))</span><br></pre></td></tr></table></figure>
<p><strong>反向传播的路径:</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">梯度流向：损失 → (残差连接 + Sublayer) → LayerNorm → 输入</span><br><span class="line">                 ↓</span><br><span class="line">             梯度有两条路径：</span><br><span class="line">             <span class="number">1.</span> 直接通过残差连接（稳定）</span><br><span class="line">             <span class="number">2.</span> 通过Sublayer和LayerNorm（可能变化）</span><br><span class="line">             </span><br><span class="line"><span class="comment"># 数学理解</span></span><br><span class="line">Pre-LayerNorm: y = x + f(LN(x))</span><br><span class="line"></span><br><span class="line">∂L/∂x = ∂L/∂y × (∂y/∂x)</span><br><span class="line">      = ∂L/∂y × [<span class="number">1</span> + ∂f/∂LN(x) × ∂LN/∂x]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意这里的 &quot;1&quot; 来自残差连接</span></span><br><span class="line"><span class="comment"># 即使 ∂f/∂LN(x) × ∂LN/∂x 很小，</span></span><br><span class="line"><span class="comment"># 仍然有 ∂L/∂y × 1 这部分梯度直接传回</span></span><br></pre></td></tr></table></figure>
<p><strong>使用 Pre-LayerNorm 的模型</strong></p>
<ol type="1">
<li>GPT 系列（GPT-2, GPT-3, GPT-4）</li>
<li>BERT 及其变体</li>
<li>T5</li>
<li>RoBERTa</li>
<li>ALBERT</li>
<li>大部分现代 Transformer 变体</li>
</ol>
<p><strong>使用 Post-LayerNorm 的模型</strong></p>
<ol type="1">
<li>原始 Transformer（2017）</li>
<li>早期实验性模型</li>
<li>现在基本不再使用</li>
</ol>
<h2 id="总结">总结</h2>
<p><strong>从 Post-LayerNorm 改为 Pre-LayerNorm 的主要原因：</strong></p>
<ol type="1">
<li><strong>训练稳定性</strong>：Pre-LayerNorm 大大减少了梯度问题</li>
<li><strong>收敛速度</strong>：训练更快，需要更少的迭代次数</li>
<li><strong>调参友好</strong>：对初始化和学习率不那么敏感</li>
<li><strong>扩展性</strong>：更容易训练深层和超大模型</li>
<li><strong>实际效果</strong>：在几乎所有任务上都表现更好</li>
</ol>
<h1 id="改进">改进</h1>
<blockquote>
<ol type="1">
<li>将post-LayerNorm改为pre-LayerNorm</li>
<li>由于原始论文中位置编码固定,因此改变位置编码的位置,避免重复计算位置编码</li>
<li>修改ScaledDotProductAttention类和MultiHeadAttention类的职责分配,合成一个类</li>
<li>在残差连接前增加Dropout层</li>
</ol>
</blockquote>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">Decoder-Only  transformer</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">pre layerNorm</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"><span class="keyword">import</span> tiktoken</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 超参数</span></span><br><span class="line">batch_size = <span class="number">4</span></span><br><span class="line">context_len = <span class="number">16</span></span><br><span class="line">d_model = <span class="number">64</span>  <span class="comment"># 每个token的维度</span></span><br><span class="line">num_blocks = <span class="number">8</span> <span class="comment">#循环多少次</span></span><br><span class="line">num_heads = <span class="number">4</span> <span class="comment">#分为几个头</span></span><br><span class="line">learning_rate = <span class="number">1e-3</span></span><br><span class="line">dropout = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line">max_iters = <span class="number">5000</span> <span class="comment"># 一共迭代多少次</span></span><br><span class="line">eval_interval = <span class="number">100</span> <span class="comment"># 多久评估一次</span></span><br><span class="line">eval_iters = <span class="number">100</span>  <span class="comment"># 评估时的计算轮次</span></span><br><span class="line">device = <span class="string">&#x27;cuda&#x27;</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">&#x27;cpu&#x27;</span></span><br><span class="line">TORCH_SEED = <span class="number">1337</span></span><br><span class="line">torch.manual_seed(TORCH_SEED)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># get dataset</span></span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">&#x27;/home/lizy/graduate/Transformer_learning/sales_textbook.txt&#x27;</span>):</span><br><span class="line">    url = <span class="string">&#x27;https://huggingface.co/datasets/goendalf666/sales-textbook_for_convincing_and_selling/raw/main/sales_textbook.txt&#x27;</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;/home/lizy/graduate/Transformer_learning/sales_textbook.txt&#x27;</span>,<span class="string">&#x27;wb&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">        f.write(requests.get(url).content)</span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">&#x27;/home/lizy/graduate/Transformer_learning/sales_textbook.txt&#x27;</span>,<span class="string">&#x27;r&#x27;</span>, encoding=<span class="string">&#x27;utf-8&#x27;</span>) <span class="keyword">as</span> f:</span><br><span class="line">    text = f.read()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">encoding = tiktoken.get_encoding(<span class="string">&quot;cl100k_base&quot;</span>)</span><br><span class="line">vocab_size = encoding.n_vocab  <span class="comment"># tiktoken的词汇表大小</span></span><br><span class="line"></span><br><span class="line">tokenized_text = encoding.encode(text)</span><br><span class="line"><span class="comment"># max_token_value = tokenized_text.max().item() + 1</span></span><br><span class="line">tokenized_text=torch.tensor(tokenized_text,dtype=torch.long,device=device)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_idex = <span class="built_in">int</span>(<span class="built_in">len</span>(tokenized_text) * <span class="number">0.9</span>)</span><br><span class="line">train_data = tokenized_text[:train_idex]</span><br><span class="line">valid_data = tokenized_text[train_idex:]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">FeedforwardNetwork</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,d_model,d_ff</span>):</span><br><span class="line">        <span class="built_in">super</span>(FeedforwardNetwork,self).__init__()</span><br><span class="line">        self.linear1 = nn.Linear(d_model,d_ff)</span><br><span class="line">        self.ReLU = nn.ReLU()</span><br><span class="line">        self.linear2 = nn.Linear(d_ff,d_model)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x=self.linear1(x)</span><br><span class="line">        x=self.ReLU(x)</span><br><span class="line">        x=self.linear2(x)</span><br><span class="line">        x=self.dropout(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.Wqkv = nn.Linear(d_model,d_model*<span class="number">3</span>)  <span class="comment">#一次计算Q K V</span></span><br><span class="line">        self.projection_layer = nn.Linear(d_model,d_model)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        B,T,C = x.shape</span><br><span class="line">        <span class="comment">#一次计算所有头的QKV</span></span><br><span class="line">        qkv = self.Wqkv(x).reshape(B,T,<span class="number">3</span>,num_heads,C // num_heads)</span><br><span class="line">        q,k,v = qkv.unbind(dim=<span class="number">2</span>)  <span class="comment"># (B,T,num_heads,head_dim)</span></span><br><span class="line">        q, k, v = q.transpose(<span class="number">1</span>, <span class="number">2</span>), k.transpose(<span class="number">1</span>, <span class="number">2</span>), v.transpose(<span class="number">1</span>, <span class="number">2</span>) <span class="comment"># (B,num_heads,T,head_dim)</span></span><br><span class="line"> </span><br><span class="line">        <span class="comment"># 注意力计算</span></span><br><span class="line">        attn = (q @ k.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / math.sqrt(C // num_heads)  <span class="comment">#(B, num_heads, T, T)</span></span><br><span class="line">        mask = torch.tril(torch.ones(T, T)).to(x.device)</span><br><span class="line">        attn = attn.masked_fill(mask == <span class="number">0</span>, <span class="built_in">float</span>(<span class="string">&#x27;-inf&#x27;</span>))</span><br><span class="line">        attn = F.softmax(attn, dim=-<span class="number">1</span>)</span><br><span class="line">        attn = self.dropout(attn)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        out = (attn @ v).transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(B, T, C)</span><br><span class="line">        <span class="keyword">return</span> self.projection_layer(out)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.multi_head_attention_layer = MultiHeadAttention()</span><br><span class="line">        self.ffn = FeedforwardNetwork(d_model,d_model*<span class="number">4</span>)</span><br><span class="line">        self.layer_norm_1=nn.LayerNorm(d_model)</span><br><span class="line">        self.layer_norm_2=nn.LayerNorm(d_model)</span><br><span class="line"></span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x = x + self.dropout(self.multi_head_attention_layer(self.layer_norm_1(x)))</span><br><span class="line">        x = x + self.dropout(self.ffn(self.layer_norm_2(x)))</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TransformerLanguageModel</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        <span class="comment">#self.token_embedding_lookup_table = nn.Embedding(max_token_value+1,d_model)</span></span><br><span class="line">        <span class="comment"># 应该使用tokenizer的实际词汇表大小</span></span><br><span class="line">        self.token_embedding_lookup_table = nn.Embedding(vocab_size, d_model)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        self.transformer_blocks = nn.Sequential(*(</span><br><span class="line">            [TransformerBlock() <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_blocks)]</span><br><span class="line">            + [nn.LayerNorm(d_model)]<span class="comment">#Different from original paper, here we add a final layer norm after all the blocks</span></span><br><span class="line">        ))</span><br><span class="line"></span><br><span class="line">        self.language_model_out_linear_layer = nn.Linear(d_model,vocab_size)</span><br><span class="line"></span><br><span class="line">        <span class="comment">#预先计算位置编码</span></span><br><span class="line">        self.register_buffer(<span class="string">&#x27;position_embedding&#x27;</span>, self._create_position_embedding())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_create_position_embedding</span>(<span class="params">self</span>):</span><br><span class="line"></span><br><span class="line">        position_encoding_lookup_table = torch.zeros(context_len,d_model)</span><br><span class="line">        position = torch.arange(<span class="number">0</span>,context_len,dtype=torch.<span class="built_in">float</span>).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        div_term = torch.exp(torch.arange(<span class="number">0</span>, d_model, <span class="number">2</span>).<span class="built_in">float</span>() * (-math.log(<span class="number">10000.0</span>) / d_model))</span><br><span class="line">        position_encoding_lookup_table[:, <span class="number">0</span>::<span class="number">2</span>] = torch.sin(position * div_term)</span><br><span class="line">        position_encoding_lookup_table[:, <span class="number">1</span>::<span class="number">2</span>] = torch.cos(position * div_term)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> position_encoding_lookup_table</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,idx,targets=<span class="literal">None</span></span>):</span><br><span class="line">        B , T = idx.shape</span><br><span class="line">        position_embedding = self.position_embedding[:T, :].to(device)</span><br><span class="line"></span><br><span class="line">        x = self.token_embedding_lookup_table(idx) + position_embedding</span><br><span class="line">        x = self.transformer_blocks(x)</span><br><span class="line"></span><br><span class="line">        logits = self.language_model_out_linear_layer(x)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> targets <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            B, T, C = logits.shape</span><br><span class="line">            logits_reshaped = logits.view(B * T, C)</span><br><span class="line">            targets_reshaped = targets.view(B * T)</span><br><span class="line">            loss = F.cross_entropy(<span class="built_in">input</span>=logits_reshaped, target=targets_reshaped)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            loss = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">return</span> logits, loss</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">generate</span>(<span class="params">self, idx, max_new_tokens, temperature=<span class="number">0.8</span>, top_k=<span class="number">50</span></span>):</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(max_new_tokens):</span><br><span class="line">            idx_crop = idx[:, -context_len:] <span class="keyword">if</span> idx.size(<span class="number">1</span>) &gt; context_len <span class="keyword">else</span> idx</span><br><span class="line">            </span><br><span class="line">            logits, _ = self(idx_crop)</span><br><span class="line">            logits = logits[:, -<span class="number">1</span>, :] / temperature</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 可选：top-k采样，提高质量</span></span><br><span class="line">            <span class="keyword">if</span> top_k <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                v, _ = torch.topk(logits, <span class="built_in">min</span>(top_k, logits.size(-<span class="number">1</span>)))</span><br><span class="line">                logits[logits &lt; v[:, [-<span class="number">1</span>]]] = <span class="built_in">float</span>(<span class="string">&#x27;-inf&#x27;</span>)</span><br><span class="line">            </span><br><span class="line">            probs = F.softmax(logits, dim=-<span class="number">1</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 采样</span></span><br><span class="line">            idx_next = torch.multinomial(probs, num_samples=<span class="number">1</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 确保token在有效范围内</span></span><br><span class="line">            idx_next = torch.clamp(idx_next, <span class="number">0</span>, vocab_size - <span class="number">1</span>)</span><br><span class="line">            </span><br><span class="line">            idx = torch.cat((idx, idx_next), dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> idx</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">model = TransformerLanguageModel()</span><br><span class="line">model = model.to(device)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_batch</span>(<span class="params">split</span>):</span><br><span class="line">    data = train_data <span class="keyword">if</span> split == <span class="string">&#x27;train&#x27;</span> <span class="keyword">else</span> valid_data</span><br><span class="line">    idxs = torch.randint(low=<span class="number">0</span>, high=<span class="built_in">len</span>(data) - context_len, size=(batch_size,))</span><br><span class="line">    x = torch.stack([data[idx:idx + context_len] <span class="keyword">for</span> idx <span class="keyword">in</span> idxs]).to(device)</span><br><span class="line">    y = torch.stack([data[idx + <span class="number">1</span>:idx + context_len + <span class="number">1</span>] <span class="keyword">for</span> idx <span class="keyword">in</span> idxs]).to(device)</span><br><span class="line">    <span class="keyword">return</span> x, y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Calculate loss</span></span><br><span class="line"><span class="meta">@torch.no_grad()</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">estimate_loss</span>():</span><br><span class="line">    out = &#123;&#125;</span><br><span class="line">    model.<span class="built_in">eval</span>() <span class="comment"># 用于将模型设置为评估模式</span></span><br><span class="line">    <span class="keyword">for</span> split <span class="keyword">in</span> [<span class="string">&#x27;train&#x27;</span>, <span class="string">&#x27;valid&#x27;</span>]:</span><br><span class="line">        losses = torch.zeros(eval_iters)</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(eval_iters):</span><br><span class="line">            x_batch, y_batch = get_batch(split)</span><br><span class="line">            logits, loss = model(x_batch, y_batch)</span><br><span class="line">            losses[k] = loss.item()</span><br><span class="line">        out[split] = losses.mean()</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练</span></span><br><span class="line">optimizer = torch.optim.AdamW(params=model.parameters(), lr=learning_rate)</span><br><span class="line">tracked_losses = <span class="built_in">list</span>()</span><br><span class="line"><span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(max_iters):</span><br><span class="line">    <span class="keyword">if</span> step % eval_interval == <span class="number">0</span> <span class="keyword">or</span> step == max_iters - <span class="number">1</span>:</span><br><span class="line">        losses = estimate_loss()</span><br><span class="line">        tracked_losses.append(losses)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&#x27;Step:&#x27;</span>, step, <span class="string">&#x27;Training Loss:&#x27;</span>, <span class="built_in">round</span>(losses[<span class="string">&#x27;train&#x27;</span>].item(), <span class="number">3</span>), <span class="string">&#x27;Validation Loss:&#x27;</span>,</span><br><span class="line">              <span class="built_in">round</span>(losses[<span class="string">&#x27;valid&#x27;</span>].item(), <span class="number">3</span>))</span><br><span class="line"></span><br><span class="line">    xb, yb = get_batch(<span class="string">&#x27;train&#x27;</span>)</span><br><span class="line">    logits, loss = model(xb, yb)</span><br><span class="line">    optimizer.zero_grad(set_to_none=<span class="literal">True</span>)</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Save the model state dictionary</span></span><br><span class="line">torch.save(model.state_dict(), <span class="string">&#x27;./model_para/model_pre-ckpt.pt&#x27;</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># Generate</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">start = <span class="string">&#x27;The salesperson&#x27;</span></span><br><span class="line">start_ids = encoding.encode(start)</span><br><span class="line">x = (torch.tensor(start_ids, dtype=torch.long, device=device)[<span class="literal">None</span>, ...])</span><br><span class="line">y = model.generate(x, max_new_tokens=<span class="number">100</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;---------------&#x27;</span>)</span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    generated_text = encoding.decode(y[<span class="number">0</span>].tolist())</span><br><span class="line"><span class="keyword">except</span> KeyError <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;解码时遇到无效token，尝试忽略: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="comment"># 忽略无效token</span></span><br><span class="line">    valid_tokens = []</span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> y[<span class="number">0</span>].tolist():</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            <span class="comment"># 检查token是否有效</span></span><br><span class="line">            <span class="keyword">if</span> <span class="number">0</span> &lt;= token &lt; vocab_size:</span><br><span class="line">                valid_tokens.append(token)</span><br><span class="line">        <span class="keyword">except</span>:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">    generated_text = encoding.decode(valid_tokens)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(encoding.decode(y[<span class="number">0</span>].tolist()))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;---------------&#x27;</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">Step: 0 Training Loss: 11.68 Validation Loss: 11.693</span><br><span class="line">Step: 100 Training Loss: 6.739 Validation Loss: 7.328</span><br><span class="line">Step: 200 Training Loss: 6.219 Validation Loss: 6.781</span><br><span class="line">Step: 300 Training Loss: 5.725 Validation Loss: 6.413</span><br><span class="line">Step: 400 Training Loss: 5.387 Validation Loss: 6.186</span><br><span class="line">Step: 500 Training Loss: 5.22 Validation Loss: 6.108</span><br><span class="line">Step: 600 Training Loss: 4.842 Validation Loss: 5.907</span><br><span class="line">Step: 700 Training Loss: 4.76 Validation Loss: 5.817</span><br><span class="line">Step: 800 Training Loss: 4.651 Validation Loss: 5.722</span><br><span class="line">Step: 900 Training Loss: 4.466 Validation Loss: 5.766</span><br><span class="line">Step: 1000 Training Loss: 4.43 Validation Loss: 5.714</span><br><span class="line">Step: 1100 Training Loss: 4.353 Validation Loss: 5.496</span><br><span class="line">Step: 1200 Training Loss: 4.167 Validation Loss: 5.446</span><br><span class="line">Step: 1300 Training Loss: 4.227 Validation Loss: 5.336</span><br><span class="line">Step: 1400 Training Loss: 4.104 Validation Loss: 5.546</span><br><span class="line">Step: 1500 Training Loss: 3.981 Validation Loss: 5.33</span><br><span class="line">Step: 1600 Training Loss: 4.005 Validation Loss: 5.393</span><br><span class="line">Step: 1700 Training Loss: 3.869 Validation Loss: 5.364</span><br><span class="line">Step: 1800 Training Loss: 3.841 Validation Loss: 5.181</span><br><span class="line">Step: 1900 Training Loss: 3.87 Validation Loss: 5.169</span><br><span class="line">Step: 2000 Training Loss: 3.775 Validation Loss: 5.235</span><br><span class="line">Step: 2100 Training Loss: 3.738 Validation Loss: 5.118</span><br><span class="line">Step: 2200 Training Loss: 3.74 Validation Loss: 5.214</span><br><span class="line">Step: 2300 Training Loss: 3.522 Validation Loss: 5.215</span><br><span class="line">Step: 2400 Training Loss: 3.599 Validation Loss: 5.164</span><br><span class="line">Step: 2500 Training Loss: 3.573 Validation Loss: 5.105</span><br><span class="line">Step: 2600 Training Loss: 3.564 Validation Loss: 4.887</span><br><span class="line">Step: 2700 Training Loss: 3.45 Validation Loss: 5.05</span><br><span class="line">Step: 2800 Training Loss: 3.435 Validation Loss: 4.914</span><br><span class="line">Step: 2900 Training Loss: 3.374 Validation Loss: 4.98</span><br><span class="line">Step: 3000 Training Loss: 3.387 Validation Loss: 4.99</span><br><span class="line">Step: 3100 Training Loss: 3.292 Validation Loss: 4.934</span><br><span class="line">Step: 3200 Training Loss: 3.263 Validation Loss: 4.946</span><br><span class="line">Step: 3300 Training Loss: 3.309 Validation Loss: 5.0</span><br><span class="line">Step: 3400 Training Loss: 3.15 Validation Loss: 4.967</span><br><span class="line">Step: 3500 Training Loss: 3.27 Validation Loss: 4.829</span><br><span class="line">Step: 3600 Training Loss: 3.241 Validation Loss: 4.995</span><br><span class="line">Step: 3700 Training Loss: 3.195 Validation Loss: 5.018</span><br><span class="line">Step: 3800 Training Loss: 3.251 Validation Loss: 4.801</span><br><span class="line">Step: 3900 Training Loss: 3.132 Validation Loss: 4.875</span><br><span class="line">Step: 4000 Training Loss: 3.157 Validation Loss: 4.883</span><br><span class="line">Step: 4100 Training Loss: 3.062 Validation Loss: 4.917</span><br><span class="line">Step: 4200 Training Loss: 3.007 Validation Loss: 4.918</span><br><span class="line">Step: 4300 Training Loss: 3.032 Validation Loss: 4.943</span><br><span class="line">Step: 4400 Training Loss: 3.075 Validation Loss: 4.97</span><br><span class="line">Step: 4500 Training Loss: 2.999 Validation Loss: 4.808</span><br><span class="line">Step: 4600 Training Loss: 2.899 Validation Loss: 4.985</span><br><span class="line">Step: 4700 Training Loss: 2.993 Validation Loss: 4.971</span><br><span class="line">Step: 4800 Training Loss: 2.942 Validation Loss: 4.867</span><br><span class="line">Step: 4900 Training Loss: 2.941 Validation Loss: 4.835</span><br><span class="line">Step: 4999 Training Loss: 2.871 Validation Loss: 4.932</span><br><span class="line">---------------</span><br><span class="line">The salesperson can create a more likely in price and concise explanations. By asking follow-up questions, you create an environment where and clarifying their pain points. By showcasing the art of closing the sales process, such as the, and commitment to address their pain points, you offer tailored information or modifications to potential customers. By recognizing the time to product or service, salespeople can establish them see the sense of urgency and persuasion. This technique of clarifying the customer&#x27;s responses, further reinforce the price or budget.</span><br><span class="line">---------------</span><br></pre></td></tr></table></figure>
<blockquote>
<p>可能是有点提升,但我不知道是哪个发挥了作用</p>
</blockquote>
<h1 id="知识拾遗">知识拾遗</h1>
<blockquote>
<p>针对代码中不理解的位置进行学习</p>
</blockquote>
<h2 id="python-装饰器">Python 装饰器</h2>
<blockquote>
<p>它允许你在不修改原函数代码的情况下，为函数或类添加额外的功能。</p>
</blockquote>
<h3 id="基本概念">基本概念</h3>
<p>装饰器本质上是一个<strong>接受函数作为参数并返回一个新函数</strong>的函数。它使用 <code>@</code> 符号语法糖来应用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 装饰器的定义</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">decorator</span>(<span class="params">func</span>):           <span class="comment"># 接受一个函数</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">wrapper</span>():            <span class="comment"># 定义一个新函数</span></span><br><span class="line">        <span class="comment"># 添加额外功能</span></span><br><span class="line">        result = func()       <span class="comment"># 调用原函数</span></span><br><span class="line">        <span class="comment"># 添加额外功能</span></span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line">    <span class="keyword">return</span> wrapper            <span class="comment"># 返回新函数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用装饰器</span></span><br><span class="line"><span class="meta">@decorator</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">say_hello</span>():</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;Hello!&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 等价于：say_hello = decorator(say_hello)</span></span><br></pre></td></tr></table></figure>
<h2 id="装饰器的基本用法">装饰器的基本用法</h2>
<h3 id="最简单的装饰器">最简单的装饰器</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">my_decorator</span>(<span class="params">func</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">wrapper</span>():</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;函数执行前&quot;</span>)</span><br><span class="line">        func()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;函数执行后&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="meta">@my_decorator</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">greet</span>():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;你好！&quot;</span>)</span><br><span class="line"></span><br><span class="line">greet()</span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="comment"># 函数执行前</span></span><br><span class="line"><span class="comment"># 你好！</span></span><br><span class="line"><span class="comment"># 函数执行后</span></span><br></pre></td></tr></table></figure>
<h3 id="装饰带参数的函数">装饰带参数的函数</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">decorator</span>(<span class="params">func</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args, **kwargs</span>):  <span class="comment"># 接收任意参数</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;调用函数: <span class="subst">&#123;func.__name__&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;参数: <span class="subst">&#123;args&#125;</span>, <span class="subst">&#123;kwargs&#125;</span>&quot;</span>)</span><br><span class="line">        result = func(*args, **kwargs)  <span class="comment"># 传递参数给原函数</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;结果: <span class="subst">&#123;result&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="meta">@decorator</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">a, b</span>):</span><br><span class="line">    <span class="keyword">return</span> a + b</span><br><span class="line"></span><br><span class="line">result = add(<span class="number">3</span>, <span class="number">5</span>)</span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="comment"># 调用函数: add</span></span><br><span class="line"><span class="comment"># 参数: (3, 5), &#123;&#125;</span></span><br><span class="line"><span class="comment"># 结果: 8</span></span><br></pre></td></tr></table></figure>
<h2 id="装饰器的四种形式">装饰器的四种形式</h2>
<h3 id="形式1函数装饰器最常用">形式1：函数装饰器（最常用）</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">timer</span>(<span class="params">func</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算函数运行时间的装饰器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args, **kwargs</span>):</span><br><span class="line">        start_time = time.time()</span><br><span class="line">        result = func(*args, **kwargs)</span><br><span class="line">        end_time = time.time()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;func.__name__&#125;</span> 运行时间: <span class="subst">&#123;end_time - start_time:<span class="number">.4</span>f&#125;</span>秒&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="meta">@timer</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">slow_function</span>():</span><br><span class="line">    time.sleep(<span class="number">2</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;完成&quot;</span></span><br><span class="line"></span><br><span class="line">slow_function()</span><br></pre></td></tr></table></figure>
<h3 id="形式2带参数的装饰器"><strong>形式2：带参数的装饰器</strong></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">repeat</span>(<span class="params">num_times</span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;重复执行函数的装饰器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decorator_repeat</span>(<span class="params">func</span>):</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args, **kwargs</span>):</span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_times):</span><br><span class="line">                result = func(*args, **kwargs)</span><br><span class="line">            <span class="keyword">return</span> result</span><br><span class="line">        <span class="keyword">return</span> wrapper</span><br><span class="line">    <span class="keyword">return</span> decorator_repeat</span><br><span class="line"></span><br><span class="line"><span class="meta">@repeat(<span class="params">num_times=<span class="number">3</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">greet</span>(<span class="params">name</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;你好, <span class="subst">&#123;name&#125;</span>!&quot;</span>)</span><br><span class="line"></span><br><span class="line">greet(<span class="string">&quot;小明&quot;</span>)</span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="comment"># 你好, 小明!</span></span><br><span class="line"><span class="comment"># 你好, 小明!</span></span><br><span class="line"><span class="comment"># 你好, 小明!</span></span><br></pre></td></tr></table></figure>
<h3 id="形式3类装饰器">形式3：类装饰器</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">CountCalls</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;记录函数调用次数的装饰器（类实现）&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, func</span>):</span><br><span class="line">        self.func = func</span><br><span class="line">        self.num_calls = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__call__</span>(<span class="params">self, *args, **kwargs</span>):</span><br><span class="line">        self.num_calls += <span class="number">1</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;第<span class="subst">&#123;self.num_calls&#125;</span>次调用 <span class="subst">&#123;self.func.__name__&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> self.func(*args, **kwargs)</span><br><span class="line"></span><br><span class="line"><span class="meta">@CountCalls</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">say_hello</span>():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;你好！&quot;</span>)</span><br><span class="line"></span><br><span class="line">say_hello()  <span class="comment"># 第1次调用 say_hello</span></span><br><span class="line">say_hello()  <span class="comment"># 第2次调用 say_hello</span></span><br></pre></td></tr></table></figure>
<h3 id="形式4多个装饰器堆叠"><strong>形式4：多个装饰器堆叠</strong></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">bold</span>(<span class="params">func</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">wrapper</span>():</span><br><span class="line">        <span class="keyword">return</span> <span class="string">f&quot;&lt;b&gt;<span class="subst">&#123;func()&#125;</span>&lt;/b&gt;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">italic</span>(<span class="params">func</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">wrapper</span>():</span><br><span class="line">        <span class="keyword">return</span> <span class="string">f&quot;&lt;i&gt;<span class="subst">&#123;func()&#125;</span>&lt;/i&gt;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">underline</span>(<span class="params">func</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">wrapper</span>():</span><br><span class="line">        <span class="keyword">return</span> <span class="string">f&quot;&lt;u&gt;<span class="subst">&#123;func()&#125;</span>&lt;/u&gt;&quot;</span></span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="meta">@bold</span></span><br><span class="line"><span class="meta">@italic</span></span><br><span class="line"><span class="meta">@underline</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">hello</span>():</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;你好，世界！&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(hello())  <span class="comment"># &lt;b&gt;&lt;i&gt;&lt;u&gt;你好，世界！&lt;/u&gt;&lt;/i&gt;&lt;/b&gt;</span></span><br><span class="line"><span class="comment"># 装饰器应用顺序：从上到下</span></span><br><span class="line"><span class="comment"># 实际执行顺序：从下到上（underline → italic → bold）</span></span><br></pre></td></tr></table></figure>
<h2 id="装饰器在实际项目中的应用">装饰器在实际项目中的应用</h2>
<h3 id="日志记录">日志记录</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> functools</span><br><span class="line"><span class="keyword">import</span> logging</span><br><span class="line"></span><br><span class="line">logging.basicConfig(level=logging.INFO)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">log_decorator</span>(<span class="params">func</span>):</span><br><span class="line"><span class="meta">    @functools.wraps(<span class="params">func</span>)  </span><span class="comment"># 保留原函数信息</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args, **kwargs</span>):</span><br><span class="line">        logging.info(<span class="string">f&quot;调用函数: <span class="subst">&#123;func.__name__&#125;</span>&quot;</span>)</span><br><span class="line">        logging.info(<span class="string">f&quot;参数: args=<span class="subst">&#123;args&#125;</span>, kwargs=<span class="subst">&#123;kwargs&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            result = func(*args, **kwargs)</span><br><span class="line">            logging.info(<span class="string">f&quot;返回: <span class="subst">&#123;result&#125;</span>&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span> result</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            logging.error(<span class="string">f&quot;函数 <span class="subst">&#123;func.__name__&#125;</span> 出错: <span class="subst">&#123;e&#125;</span>&quot;</span>)</span><br><span class="line">            <span class="keyword">raise</span></span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="meta">@log_decorator</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">divide</span>(<span class="params">a, b</span>):</span><br><span class="line">    <span class="keyword">return</span> a / b</span><br><span class="line"></span><br><span class="line">divide(<span class="number">10</span>, <span class="number">2</span>)</span><br><span class="line">divide(<span class="number">10</span>, <span class="number">0</span>)  <span class="comment"># 会记录错误</span></span><br></pre></td></tr></table></figure>
<h3 id="权限验证"><strong>权限验证</strong></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">require_login</span>(<span class="params">role=<span class="string">&quot;user&quot;</span></span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decorator</span>(<span class="params">func</span>):</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">user, *args, **kwargs</span>):</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> user.get(<span class="string">&quot;authenticated&quot;</span>, <span class="literal">False</span>):</span><br><span class="line">                <span class="keyword">raise</span> PermissionError(<span class="string">&quot;需要登录&quot;</span>)</span><br><span class="line">            <span class="keyword">if</span> role == <span class="string">&quot;admin&quot;</span> <span class="keyword">and</span> user.get(<span class="string">&quot;role&quot;</span>) != <span class="string">&quot;admin&quot;</span>:</span><br><span class="line">                <span class="keyword">raise</span> PermissionError(<span class="string">&quot;需要管理员权限&quot;</span>)</span><br><span class="line">            <span class="keyword">return</span> func(user, *args, **kwargs)</span><br><span class="line">        <span class="keyword">return</span> wrapper</span><br><span class="line">    <span class="keyword">return</span> decorator</span><br><span class="line"></span><br><span class="line"><span class="meta">@require_login(<span class="params">role=<span class="string">&quot;admin&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">delete_user</span>(<span class="params">current_user, user_id</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;删除用户 <span class="subst">&#123;user_id&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line">admin_user = &#123;<span class="string">&quot;authenticated&quot;</span>: <span class="literal">True</span>, <span class="string">&quot;role&quot;</span>: <span class="string">&quot;admin&quot;</span>&#125;</span><br><span class="line">normal_user = &#123;<span class="string">&quot;authenticated&quot;</span>: <span class="literal">True</span>, <span class="string">&quot;role&quot;</span>: <span class="string">&quot;user&quot;</span>&#125;</span><br><span class="line"></span><br><span class="line">delete_user(admin_user, <span class="number">123</span>)  <span class="comment"># 成功</span></span><br><span class="line"><span class="comment"># delete_user(normal_user, 123)  # 报错：需要管理员权限</span></span><br></pre></td></tr></table></figure>
<h3 id="缓存记忆化"><strong>缓存/记忆化</strong></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> lru_cache</span><br><span class="line"></span><br><span class="line"><span class="comment"># 手动实现缓存装饰器</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cache</span>(<span class="params">func</span>):</span><br><span class="line">    cached_results = &#123;&#125;</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @functools.wraps(<span class="params">func</span>)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args, **kwargs</span>):</span><br><span class="line">        <span class="comment"># 创建缓存键</span></span><br><span class="line">        key = (args, <span class="built_in">tuple</span>(<span class="built_in">sorted</span>(kwargs.items())))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> key <span class="keyword">not</span> <span class="keyword">in</span> cached_results:</span><br><span class="line">            cached_results[key] = func(*args, **kwargs)</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;计算 <span class="subst">&#123;func.__name__&#125;</span><span class="subst">&#123;args&#125;</span> -&gt; 缓存&quot;</span>)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;从缓存获取 <span class="subst">&#123;func.__name__&#125;</span><span class="subst">&#123;args&#125;</span>&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> cached_results[key]</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="meta">@cache</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fibonacci</span>(<span class="params">n</span>):</span><br><span class="line">    <span class="keyword">if</span> n &lt; <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> n</span><br><span class="line">    <span class="keyword">return</span> fibonacci(n-<span class="number">1</span>) + fibonacci(n-<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(fibonacci(<span class="number">5</span>))  <span class="comment"># 大量计算被缓存</span></span><br></pre></td></tr></table></figure>
<h3 id="重试机制"><strong>重试机制</strong></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">retry</span>(<span class="params">max_attempts=<span class="number">3</span>, delay=<span class="number">1</span></span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decorator</span>(<span class="params">func</span>):</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args, **kwargs</span>):</span><br><span class="line">            last_exception = <span class="literal">None</span></span><br><span class="line">            <span class="keyword">for</span> attempt <span class="keyword">in</span> <span class="built_in">range</span>(max_attempts):</span><br><span class="line">                <span class="keyword">try</span>:</span><br><span class="line">                    <span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line">                <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                    last_exception = e</span><br><span class="line">                    <span class="keyword">if</span> attempt &lt; max_attempts - <span class="number">1</span>:</span><br><span class="line">                        <span class="built_in">print</span>(<span class="string">f&quot;尝试 <span class="subst">&#123;attempt+<span class="number">1</span>&#125;</span> 失败，<span class="subst">&#123;delay&#125;</span>秒后重试...&quot;</span>)</span><br><span class="line">                        time.sleep(delay)</span><br><span class="line">            <span class="keyword">raise</span> Exception(<span class="string">f&quot;所有 <span class="subst">&#123;max_attempts&#125;</span> 次尝试都失败&quot;</span>) <span class="keyword">from</span> last_exception</span><br><span class="line">        <span class="keyword">return</span> wrapper</span><br><span class="line">    <span class="keyword">return</span> decorator</span><br><span class="line"></span><br><span class="line"><span class="meta">@retry(<span class="params">max_attempts=<span class="number">3</span>, delay=<span class="number">2</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">unstable_network_request</span>():</span><br><span class="line">    <span class="keyword">import</span> random</span><br><span class="line">    <span class="keyword">if</span> random.random() &lt; <span class="number">0.7</span>:  <span class="comment"># 70%概率失败</span></span><br><span class="line">        <span class="keyword">raise</span> ConnectionError(<span class="string">&quot;网络错误&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">&quot;请求成功&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(unstable_network_request())</span><br></pre></td></tr></table></figure>
<h2 id="使用-functools.wraps"><strong>使用 <code>functools.wraps</code></strong></h2>
<h3 id="为什么需要它"><strong>为什么需要它？</strong></h3>
<p>装饰器会隐藏原函数的元信息（名字、文档字符串等），<code>functools.wraps</code> 可以解决这个问题。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> functools</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">my_decorator</span>(<span class="params">func</span>):</span><br><span class="line"><span class="meta">    @functools.wraps(<span class="params">func</span>)  </span><span class="comment"># 关键！保留原函数信息</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args, **kwargs</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;装饰器功能&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="meta">@my_decorator</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">example</span>():</span><br><span class="line">    <span class="string">&quot;&quot;&quot;这是一个示例函数&quot;&quot;&quot;</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;原函数功能&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(example.__name__)  <span class="comment"># 输出：example（没有wraps会输出wrapper）</span></span><br><span class="line"><span class="built_in">print</span>(example.__doc__)   <span class="comment"># 输出：这是一个示例函数</span></span><br><span class="line"><span class="built_in">help</span>(example)            <span class="comment"># 显示正确的帮助信息</span></span><br></pre></td></tr></table></figure>
<h2 id="装饰器的底层原理">装饰器的底层原理</h2>
<h3 id="装饰器的执行时机">装饰器的执行时机</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">decorator</span>(<span class="params">func</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f&quot;装饰器执行: 正在装饰 <span class="subst">&#123;func.__name__&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">wrapper</span>():</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;wrapper被调用&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> func()</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="meta">@decorator</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">my_function</span>():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;my_function被调用&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;定义完成&quot;</span>)</span><br><span class="line">my_function()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="comment"># 装饰器执行: 正在装饰 my_function  &lt;- 在函数定义时执行！</span></span><br><span class="line"><span class="comment"># 定义完成</span></span><br><span class="line"><span class="comment"># wrapper被调用</span></span><br><span class="line"><span class="comment"># my_function被调用</span></span><br></pre></td></tr></table></figure>
<h2 id="在机器学习中的实际应用"><strong>在机器学习中的实际应用</strong></h2>
<h3 id="模型训练装饰器">模型训练装饰器</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">training_decorator</span>(<span class="params">epochs=<span class="number">10</span></span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decorator</span>(<span class="params">train_func</span>):</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">model, dataloader, *args, **kwargs</span>):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;开始训练，共 <span class="subst">&#123;epochs&#125;</span> 个epoch&quot;</span>)</span><br><span class="line">            start_time = time.time()</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(epochs):</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;Epoch <span class="subst">&#123;epoch+<span class="number">1</span>&#125;</span>/<span class="subst">&#123;epochs&#125;</span>&quot;</span>)</span><br><span class="line">                epoch_loss = train_func(model, dataloader, *args, **kwargs)</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;  Loss: <span class="subst">&#123;epoch_loss:<span class="number">.4</span>f&#125;</span>&quot;</span>)</span><br><span class="line">            </span><br><span class="line">            training_time = time.time() - start_time</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f&quot;训练完成，用时 <span class="subst">&#123;training_time:<span class="number">.2</span>f&#125;</span>秒&quot;</span>)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> wrapper</span><br><span class="line">    <span class="keyword">return</span> decorator</span><br><span class="line"></span><br><span class="line"><span class="meta">@training_decorator(<span class="params">epochs=<span class="number">5</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train_one_epoch</span>(<span class="params">model, dataloader, optimizer, criterion</span>):</span><br><span class="line">    total_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> dataloader:</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        outputs = model(batch)</span><br><span class="line">        loss = criterion(outputs, batch.labels)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        total_loss += loss.item()</span><br><span class="line">    <span class="keyword">return</span> total_loss / <span class="built_in">len</span>(dataloader)</span><br></pre></td></tr></table></figure>
<h3 id="梯度检查装饰器">梯度检查装饰器</h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">check_gradients</span>(<span class="params">func</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">model, *args, **kwargs</span>):</span><br><span class="line">        <span class="comment"># 记录初始梯度</span></span><br><span class="line">        initial_grads = []</span><br><span class="line">        <span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">            <span class="keyword">if</span> param.grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                initial_grads.append(param.grad.clone())</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 执行前向传播和反向传播</span></span><br><span class="line">        loss = func(model, *args, **kwargs)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 检查梯度</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;梯度检查:&quot;</span>)</span><br><span class="line">        <span class="keyword">for</span> i, param <span class="keyword">in</span> <span class="built_in">enumerate</span>(model.parameters()):</span><br><span class="line">            <span class="keyword">if</span> param.grad <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                grad_norm = param.grad.norm().item()</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;  参数 <span class="subst">&#123;i&#125;</span>: 梯度范数 = <span class="subst">&#123;grad_norm:<span class="number">.6</span>f&#125;</span>&quot;</span>)</span><br><span class="line">                <span class="keyword">if</span> grad_norm &gt; <span class="number">100</span>:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">&quot;  ⚠️  梯度爆炸！&quot;</span>)</span><br><span class="line">                <span class="keyword">elif</span> grad_norm &lt; <span class="number">1e-7</span>:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">&quot;  ⚠️  梯度消失！&quot;</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> loss</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br></pre></td></tr></table></figure>
<h2 id="装饰器的常见问题"><strong>装饰器的常见问题</strong></h2>
<h3 id="问题1装饰器破坏了函数签名"><strong>问题1：装饰器破坏了函数签名</strong></h3>
<p><strong>解决方案</strong>：使用 <code>functools.wraps</code> 和 <code>inspect.signature</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> functools</span><br><span class="line"><span class="keyword">import</span> inspect</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preserve_signature</span>(<span class="params">func</span>):</span><br><span class="line"><span class="meta">    @functools.wraps(<span class="params">func</span>)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args, **kwargs</span>):</span><br><span class="line">        <span class="comment"># 检查参数数量</span></span><br><span class="line">        sig = inspect.signature(func)</span><br><span class="line">        bound = sig.bind(*args, **kwargs)</span><br><span class="line">        bound.apply_defaults()</span><br><span class="line">        </span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;调用 <span class="subst">&#123;func.__name__&#125;</span>，参数: <span class="subst">&#123;bound.arguments&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br></pre></td></tr></table></figure>
<h3 id="问题2装饰器不能装饰类方法"><strong>问题2：装饰器不能装饰类方法</strong></h3>
<p><strong>解决方案</strong>：正确处理 <code>self</code> 参数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">method_decorator</span>(<span class="params">func</span>):</span><br><span class="line"><span class="meta">    @functools.wraps(<span class="params">func</span>)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">self, *args, **kwargs</span>):  <span class="comment"># 注意第一个参数是self</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;调用方法: <span class="subst">&#123;self.__class__.__name__&#125;</span>.<span class="subst">&#123;func.__name__&#125;</span>&quot;</span>)</span><br><span class="line">        <span class="keyword">return</span> func(self, *args, **kwargs)</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MyClass</span>:</span><br><span class="line"><span class="meta">    @method_decorator</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">my_method</span>(<span class="params">self, value</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;值: <span class="subst">&#123;value&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>
<h3 id="问题3装饰器影响性能"><strong>问题3：装饰器影响性能</strong></h3>
<p><strong>解决方案</strong>：避免在装饰器内部做复杂操作</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 不好：每次调用都重新计算</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bad_decorator</span>(<span class="params">func</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args, **kwargs</span>):</span><br><span class="line">        <span class="comment"># 每次调用都创建新对象</span></span><br><span class="line">        cache = &#123;&#125;  <span class="comment"># ⚠️ 应该放在外层</span></span><br><span class="line">        <span class="comment"># ...</span></span><br><span class="line">        <span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"></span><br><span class="line"><span class="comment"># 好：初始化只做一次</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">good_decorator</span>(<span class="params">func</span>):</span><br><span class="line">    cache = &#123;&#125;  <span class="comment"># ✅ 在外层创建</span></span><br><span class="line">    </span><br><span class="line"><span class="meta">    @functools.wraps(<span class="params">func</span>)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args, **kwargs</span>):</span><br><span class="line">        <span class="comment"># 使用外层的cache</span></span><br><span class="line">        key = (args, <span class="built_in">tuple</span>(<span class="built_in">sorted</span>(kwargs.items())))</span><br><span class="line">        <span class="keyword">if</span> key <span class="keyword">not</span> <span class="keyword">in</span> cache:</span><br><span class="line">            cache[key] = func(*args, **kwargs)</span><br><span class="line">        <span class="keyword">return</span> cache[key]</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br></pre></td></tr></table></figure>
<h2 id="装饰器的最佳实践"><strong>装饰器的最佳实践</strong></h2>
<h3 id="始终使用-functools.wraps"><strong>始终使用 <code>functools.wraps</code></strong></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> functools</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">decorator</span>(<span class="params">func</span>):</span><br><span class="line"><span class="meta">    @functools.wraps(<span class="params">func</span>)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args, **kwargs</span>):</span><br><span class="line">        <span class="comment"># 装饰器逻辑</span></span><br><span class="line">        <span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br></pre></td></tr></table></figure>
<h3 id="编写可重用的装饰器"><strong>编写可重用的装饰器</strong></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> typing <span class="keyword">import</span> <span class="type">Callable</span>, <span class="type">Any</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">debug_decorator</span>(<span class="params">print_args: <span class="built_in">bool</span> = <span class="literal">True</span>, print_result: <span class="built_in">bool</span> = <span class="literal">True</span></span>):</span><br><span class="line">    <span class="string">&quot;&quot;&quot;可配置的调试装饰器&quot;&quot;&quot;</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decorator</span>(<span class="params">func: <span class="type">Callable</span></span>) -&gt; <span class="type">Callable</span>:</span><br><span class="line"><span class="meta">        @functools.wraps(<span class="params">func</span>)</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args, **kwargs</span>) -&gt; <span class="type">Any</span>:</span><br><span class="line">            <span class="keyword">if</span> print_args:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;func.__name__&#125;</span> 被调用，参数: <span class="subst">&#123;args&#125;</span>, <span class="subst">&#123;kwargs&#125;</span>&quot;</span>)</span><br><span class="line">            </span><br><span class="line">            result = func(*args, **kwargs)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> print_result:</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;func.__name__&#125;</span> 返回: <span class="subst">&#123;result&#125;</span>&quot;</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">return</span> result</span><br><span class="line">        <span class="keyword">return</span> wrapper</span><br><span class="line">    <span class="keyword">return</span> decorator</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用</span></span><br><span class="line"><span class="meta">@debug_decorator(<span class="params">print_args=<span class="literal">True</span>, print_result=<span class="literal">False</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">my_function</span>(<span class="params">x, y</span>):</span><br><span class="line">    <span class="keyword">return</span> x + y</span><br></pre></td></tr></table></figure>
<h3 id="装饰器工厂模式"><strong>装饰器工厂模式</strong></h3>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DecoratorFactory</span>:</span><br><span class="line">    <span class="string">&quot;&quot;&quot;装饰器工厂，管理多个装饰器&quot;&quot;&quot;</span></span><br><span class="line">    </span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">timer</span>():</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">decorator</span>(<span class="params">func</span>):</span><br><span class="line"><span class="meta">            @functools.wraps(<span class="params">func</span>)</span></span><br><span class="line">            <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args, **kwargs</span>):</span><br><span class="line">                <span class="keyword">import</span> time</span><br><span class="line">                start = time.time()</span><br><span class="line">                result = func(*args, **kwargs)</span><br><span class="line">                end = time.time()</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;<span class="subst">&#123;func.__name__&#125;</span> 耗时: <span class="subst">&#123;end-start:<span class="number">.4</span>f&#125;</span>s&quot;</span>)</span><br><span class="line">                <span class="keyword">return</span> result</span><br><span class="line">            <span class="keyword">return</span> wrapper</span><br><span class="line">        <span class="keyword">return</span> decorator</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">logger</span>(<span class="params">level=<span class="string">&quot;INFO&quot;</span></span>):</span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">decorator</span>(<span class="params">func</span>):</span><br><span class="line"><span class="meta">            @functools.wraps(<span class="params">func</span>)</span></span><br><span class="line">            <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args, **kwargs</span>):</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f&quot;[<span class="subst">&#123;level&#125;</span>] 调用: <span class="subst">&#123;func.__name__&#125;</span>&quot;</span>)</span><br><span class="line">                <span class="keyword">return</span> func(*args, **kwargs)</span><br><span class="line">            <span class="keyword">return</span> wrapper</span><br><span class="line">        <span class="keyword">return</span> decorator</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用</span></span><br><span class="line"><span class="meta">@DecoratorFactory.timer()</span></span><br><span class="line"><span class="meta">@DecoratorFactory.logger(<span class="params">level=<span class="string">&quot;DEBUG&quot;</span></span>)</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process_data</span>(<span class="params">data</span>):</span><br><span class="line">    <span class="comment"># 处理数据</span></span><br><span class="line">    <span class="keyword">return</span> data.upper()</span><br></pre></td></tr></table></figure>
<h2 id="总结-1"><strong>总结</strong></h2>
<p>装饰器是Python的<strong>超级武器</strong>，它让你能够：</p>
<ul>
<li><strong>添加功能</strong>而不修改原代码</li>
<li><strong>分离关注点</strong>（业务逻辑 vs 横切关注点）</li>
<li><strong>提高代码复用</strong>（装饰器可重复使用）</li>
<li><strong>保持代码简洁</strong>（避免重复代码）</li>
</ul>
<p><strong>关键要点</strong>：</p>
<ol type="1">
<li>装饰器在<strong>函数定义时</strong>执行，而不是调用时</li>
<li>使用 <code>@functools.wraps</code> 保留原函数信息</li>
<li>装饰器可以嵌套，执行顺序<strong>从内到外</strong></li>
<li>装饰器可以是函数，也可以是类（实现 <code>__call__</code> 方法）</li>
<li>装饰器参数需要额外包装一层</li>
</ol>
<h2 id="model.train-model.eval">model.train() model.eval()</h2>
<p><strong><code>model.train()</code> 和 <code>model.eval()</code> 是控制 PyTorch 模型行为的开关：</strong></p>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">特性</th>
<th style="text-align: left;"><code>model.train()</code></th>
<th style="text-align: left;"><code>model.eval()</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>用途</strong></td>
<td style="text-align: left;">训练</td>
<td style="text-align: left;">评估/推理</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>Dropout</strong></td>
<td style="text-align: left;">启用（随机丢弃）</td>
<td style="text-align: left;">禁用（全参与）</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>BatchNorm</strong></td>
<td style="text-align: left;">更新统计量</td>
<td style="text-align: left;">使用累积统计量</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>结果</strong></td>
<td style="text-align: left;">随机（训练需要）</td>
<td style="text-align: left;">确定（评估需要）</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>内存</strong></td>
<td style="text-align: left;">较大（保存梯度）</td>
<td style="text-align: left;">较小（无梯度）</td>
</tr>
</tbody>
</table>
<h2 id="广播机制broadcasting">广播机制（Broadcasting）</h2>
<p><code>unsqueeze()</code> 经常与广播机制一起使用：</p>
<h3 id="广播规则">广播规则</h3>
<p>两个张量运算时，PyTorch 会<strong>自动扩展维度</strong>使它们形状匹配：</p>
<h4 id="规则1维度对齐从右向左">规则1：维度对齐（从右向左）</h4>
<p>比较两个张量的形状，从<strong>最后一个维度（最右边）开始</strong>，向左逐个维度比较。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 示例</span></span><br><span class="line">a = torch.randn(<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)  <span class="comment"># 形状: (2, 3, 4, 5)</span></span><br><span class="line">b = torch.randn(    <span class="number">4</span>, <span class="number">5</span>)    <span class="comment"># 形状:      (4, 5)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 比较过程：</span></span><br><span class="line"><span class="comment"># 步骤1: 维度4比较: a的5 vs b的5 → 相等 ✓</span></span><br><span class="line"><span class="comment"># 步骤2: 维度3比较: a的4 vs b的4 → 相等 ✓</span></span><br><span class="line"><span class="comment"># 步骤3: 维度2比较: a的3 vs b无 → b缺失，视为1</span></span><br><span class="line"><span class="comment"># 步骤4: 维度1比较: a的2 vs b无 → b缺失，视为1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 最终b的形状变为: (1, 1, 4, 5)</span></span><br></pre></td></tr></table></figure>
<h4 id="规则2兼容性判断">规则2：兼容性判断</h4>
<p>两个维度兼容的条件：</p>
<ol type="1">
<li><strong>维度相等</strong>：如 5 和 5</li>
<li><strong>其中一个为1</strong>：如 5 和 1</li>
<li><strong>其中一个不存在（缺失）</strong>：视为1</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 兼容的例子</span></span><br><span class="line">(<span class="number">5</span>, <span class="number">3</span>) 和 (<span class="number">3</span>,)     → 兼容 ✓</span><br><span class="line">(<span class="number">5</span>, <span class="number">3</span>) 和 (<span class="number">1</span>, <span class="number">3</span>)   → 兼容 ✓</span><br><span class="line">(<span class="number">5</span>, <span class="number">3</span>) 和 (<span class="number">5</span>, <span class="number">1</span>)   → 兼容 ✓</span><br><span class="line">(<span class="number">5</span>, <span class="number">3</span>, <span class="number">4</span>) 和 (<span class="number">3</span>, <span class="number">4</span>) → 兼容 ✓</span><br><span class="line"></span><br><span class="line"><span class="comment"># 不兼容的例子</span></span><br><span class="line">(<span class="number">5</span>, <span class="number">3</span>) 和 (<span class="number">4</span>,)     → 不兼容 ✗ (<span class="number">3</span> ≠ <span class="number">4</span>)</span><br><span class="line">(<span class="number">5</span>, <span class="number">3</span>) 和 (<span class="number">5</span>, <span class="number">4</span>)   → 不兼容 ✗ (<span class="number">3</span> ≠ <span class="number">4</span> 且都不为<span class="number">1</span>)</span><br><span class="line">(<span class="number">5</span>, <span class="number">3</span>) 和 (<span class="number">6</span>, <span class="number">3</span>)   → 不兼容 ✗ (<span class="number">5</span> ≠ <span class="number">6</span> 且都不为<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<h4 id="规则3扩展执行">规则3：扩展执行</h4>
<p>将形状为1的维度扩展为对应维度的大小。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)  <span class="comment"># 形状: (3, 4, 5)</span></span><br><span class="line">b = torch.randn(    <span class="number">5</span>)    <span class="comment"># 形状:      (5)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 广播过程：</span></span><br><span class="line"><span class="comment"># 1. b 对齐为: (1, 1, 5)</span></span><br><span class="line"><span class="comment"># 2. b 扩展为: (3, 4, 5)  # 复制数据（逻辑上）</span></span><br><span class="line"><span class="comment"># 3. 执行运算: a + b</span></span><br></pre></td></tr></table></figure>
<h2 id="unsqueeze-squeeze">unsqueeze() squeeze()</h2>
<p><code>unsqueeze()</code> 的逆操作是 <code>squeeze()</code>，用于<strong>移除大小为1的维度</strong>：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 添加维度</span></span><br><span class="line">x = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])      <span class="comment"># 形状: (3,)</span></span><br><span class="line">x_expanded = x.unsqueeze(<span class="number">0</span>)      <span class="comment"># 形状: (1, 3)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 移除维度</span></span><br><span class="line">x_squeezed = x_expanded.squeeze(<span class="number">0</span>)  <span class="comment"># 形状: (3,)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># squeeze() 不指定维度时，移除所有大小为1的维度</span></span><br><span class="line">y = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line">y_squeezed = y.squeeze()  <span class="comment"># 形状: (3, 4)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 只移除特定维度</span></span><br><span class="line">y = torch.randn(<span class="number">1</span>, <span class="number">3</span>, <span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">y_squeezed_dim0 = y.squeeze(<span class="number">0</span>)  <span class="comment"># 形状: (3, 1, 4)</span></span><br><span class="line">y_squeezed_dim2 = y.squeeze(<span class="number">2</span>)  <span class="comment"># 形状: (1, 3, 4)</span></span><br></pre></td></tr></table></figure>
<h2 id="view-reshape"><code>view()</code> <code>reshape()</code></h2>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">特性</th>
<th style="text-align: left;"><code>torch.view()</code></th>
<th style="text-align: left;"><code>torch.reshape()</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong>内存连续性要求</strong></td>
<td style="text-align: left;">要求张量是连续的（contiguous）</td>
<td style="text-align: left;">不要求，会自动处理非连续张量</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>数据复制</strong></td>
<td style="text-align: left;">不复制数据，共享底层存储</td>
<td style="text-align: left;">必要时会复制数据（当张量不连续时）</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>错误情况</strong></td>
<td style="text-align: left;">如果张量不连续会报错</td>
<td style="text-align: left;">总是成功，但可能有性能损失</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>使用场景</strong></td>
<td style="text-align: left;">已知张量连续时的快速形状调整</td>
<td style="text-align: left;">不确定张量是否连续时的安全形状调整</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>性能</strong></td>
<td style="text-align: left;">更快（无数据复制）</td>
<td style="text-align: left;">可能较慢（可能需要复制）</td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong>返回值</strong></td>
<td style="text-align: left;">新视图，共享数据</td>
<td style="text-align: left;">可能的新张量，可能不共享数据</td>
</tr>
</tbody>
</table>
<h3 id="其他改变形状的api">其他改变形状的API</h3>
<table>
<thead>
<tr class="header">
<th style="text-align: left;">API</th>
<th style="text-align: left;">功能</th>
<th style="text-align: left;">是否改变存储</th>
<th style="text-align: left;">是否支持原地操作</th>
<th style="text-align: left;">示例</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;"><strong><code>view()</code></strong></td>
<td style="text-align: left;">改变形状（需连续）</td>
<td style="text-align: left;">❌ 共享存储</td>
<td style="text-align: left;">❌</td>
<td style="text-align: left;"><code>x.view(2, 3)</code></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong><code>reshape()</code></strong></td>
<td style="text-align: left;">改变形状（自动处理）</td>
<td style="text-align: left;">可能复制</td>
<td style="text-align: left;">❌</td>
<td style="text-align: left;"><code>x.reshape(2, 3)</code></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong><code>resize_()</code></strong></td>
<td style="text-align: left;">原地调整大小</td>
<td style="text-align: left;">✅ 可能改变存储</td>
<td style="text-align: left;">✅</td>
<td style="text-align: left;"><code>x.resize_(2, 3)</code></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong><code>flatten()</code></strong></td>
<td style="text-align: left;">展平为1D</td>
<td style="text-align: left;">可能复制</td>
<td style="text-align: left;">❌</td>
<td style="text-align: left;"><code>x.flatten()</code></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong><code>squeeze()</code></strong></td>
<td style="text-align: left;">移除维度为1的轴</td>
<td style="text-align: left;">❌ 共享存储</td>
<td style="text-align: left;">有原地版本</td>
<td style="text-align: left;"><code>x.squeeze()</code></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong><code>unsqueeze()</code></strong></td>
<td style="text-align: left;">添加维度为1的轴</td>
<td style="text-align: left;">❌ 共享存储</td>
<td style="text-align: left;">有原地版本</td>
<td style="text-align: left;"><code>x.unsqueeze(0)</code></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong><code>transpose()</code></strong></td>
<td style="text-align: left;">交换两个维度</td>
<td style="text-align: left;">❌ 共享存储</td>
<td style="text-align: left;">❌</td>
<td style="text-align: left;"><code>x.transpose(0, 1)</code></td>
</tr>
<tr class="even">
<td style="text-align: left;"><strong><code>permute()</code></strong></td>
<td style="text-align: left;">重新排列所有维度</td>
<td style="text-align: left;">❌ 共享存储</td>
<td style="text-align: left;">❌</td>
<td style="text-align: left;"><code>x.permute(1, 0, 2)</code></td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong><code>contiguous()</code></strong></td>
<td style="text-align: left;">使张量连续</td>
<td style="text-align: left;">✅ 复制数据</td>
<td style="text-align: left;">❌</td>
<td style="text-align: left;"><code>x.contiguous()</code></td>
</tr>
</tbody>
</table>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="https://klklkl10086.github.io/klklkl10086.github.io">klklkl</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="https://klklkl10086.github.io/klklkl10086.github.io/2025/12/07/%E4%BB%8E0%E5%AE%9E%E7%8E%B0LLM/">https://klklkl10086.github.io/klklkl10086.github.io/2025/12/07/%E4%BB%8E0%E5%AE%9E%E7%8E%B0LLM/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="https://klklkl10086.github.io/klklkl10086.github.io" target="_blank">klklkl's blogs</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a><a class="post-meta__tags" href="/tags/Transformer/">Transformer</a></div><div class="post_share"><div class="social-share" data-image="https://pic2.zhimg.com/v2-f9b32b95385eedbdb58389bbbce5de39_r.jpg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc/sharejs/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2026/01/31/STL%E6%8B%BE%E9%81%97/" title="STL拾遗"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">上一篇</div><div class="prev_info">STL拾遗</div></div></a></div><div class="next-post pull-right"><a href="/2025/11/27/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/" title="Pytorch深度学习实践"><div class="cover" style="background: var(--default-bg-color)"></div><div class="pagination-info"><div class="label">下一篇</div><div class="next_info">Pytorch深度学习实践</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>相关推荐</span></div><div class="relatedPosts-list"><div><a href="/2025/11/27/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/" title="Pytorch深度学习实践"><div class="cover" style="background: var(--default-bg-color)"></div><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-11-27</div><div class="title">Pytorch深度学习实践</div></div></a></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="https://pic2.zhimg.com/v2-f9b32b95385eedbdb58389bbbce5de39_r.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">klklkl</div><div class="author-info__description">student</div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">21</div></a><a href="/tags/"><div class="headline">标签</div><div class="length-num">27</div></a><a href="/categories/"><div class="headline">分类</div><div class="length-num">7</div></a></div><div class="card-info-social-icons is-center"><a class="social-icon" href="mailto:2488926297@qq.com" target="_blank" title="Email"><i class="fa-regular fa-envelope" style="color: #000000;"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">Nice to meet you!</div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>目录</span><span class="toc-percentage"></span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%A1%86%E6%9E%B6%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86"><span class="toc-number">1.</span> <span class="toc-text">框架基本知识</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E4%BB%A3%E7%A0%81"><span class="toc-number">2.</span> <span class="toc-text">代码</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%95%88%E6%9E%9C"><span class="toc-number">3.</span> <span class="toc-text">效果</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#post-layernorm-vs-pre-layernorm"><span class="toc-number">4.</span> <span class="toc-text">Post-LayerNorm vs Pre-LayerNorm</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%A4%E7%A7%8D-layernorm-%E4%BD%8D%E7%BD%AE%E5%AF%B9%E6%AF%94"><span class="toc-number">4.1.</span> <span class="toc-text">两种 LayerNorm 位置对比</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E6%94%B9%E4%B8%BA-pre-layernorm"><span class="toc-number">4.2.</span> <span class="toc-text">为什么要改为 Pre-LayerNorm？</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E7%A8%B3%E5%AE%9A%E6%80%A7%E5%A4%A7%E5%B9%85%E6%8F%90%E5%8D%87"><span class="toc-number">4.2.1.</span> <span class="toc-text">训练稳定性大幅提升</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%94%B6%E6%95%9B%E9%80%9F%E5%BA%A6%E6%9B%B4%E5%BF%AB"><span class="toc-number">4.2.2.</span> <span class="toc-text">收敛速度更快</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E4%BC%A0%E6%92%AD%E6%9B%B4%E7%9B%B4%E6%8E%A5"><span class="toc-number">4.2.3.</span> <span class="toc-text">梯度传播更直接</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E8%AE%A1%E7%AE%97%E5%AF%B9%E6%AF%94"><span class="toc-number">4.3.</span> <span class="toc-text">梯度计算对比</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93"><span class="toc-number">4.4.</span> <span class="toc-text">总结</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%94%B9%E8%BF%9B"><span class="toc-number">5.</span> <span class="toc-text">改进</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E7%9F%A5%E8%AF%86%E6%8B%BE%E9%81%97"><span class="toc-number">6.</span> <span class="toc-text">知识拾遗</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#python-%E8%A3%85%E9%A5%B0%E5%99%A8"><span class="toc-number">6.1.</span> <span class="toc-text">Python 装饰器</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="toc-number">6.1.1.</span> <span class="toc-text">基本概念</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A3%85%E9%A5%B0%E5%99%A8%E7%9A%84%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95"><span class="toc-number">6.2.</span> <span class="toc-text">装饰器的基本用法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%80%E7%AE%80%E5%8D%95%E7%9A%84%E8%A3%85%E9%A5%B0%E5%99%A8"><span class="toc-number">6.2.1.</span> <span class="toc-text">最简单的装饰器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A3%85%E9%A5%B0%E5%B8%A6%E5%8F%82%E6%95%B0%E7%9A%84%E5%87%BD%E6%95%B0"><span class="toc-number">6.2.2.</span> <span class="toc-text">装饰带参数的函数</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A3%85%E9%A5%B0%E5%99%A8%E7%9A%84%E5%9B%9B%E7%A7%8D%E5%BD%A2%E5%BC%8F"><span class="toc-number">6.3.</span> <span class="toc-text">装饰器的四种形式</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BD%A2%E5%BC%8F1%E5%87%BD%E6%95%B0%E8%A3%85%E9%A5%B0%E5%99%A8%E6%9C%80%E5%B8%B8%E7%94%A8"><span class="toc-number">6.3.1.</span> <span class="toc-text">形式1：函数装饰器（最常用）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BD%A2%E5%BC%8F2%E5%B8%A6%E5%8F%82%E6%95%B0%E7%9A%84%E8%A3%85%E9%A5%B0%E5%99%A8"><span class="toc-number">6.3.2.</span> <span class="toc-text">形式2：带参数的装饰器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BD%A2%E5%BC%8F3%E7%B1%BB%E8%A3%85%E9%A5%B0%E5%99%A8"><span class="toc-number">6.3.3.</span> <span class="toc-text">形式3：类装饰器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BD%A2%E5%BC%8F4%E5%A4%9A%E4%B8%AA%E8%A3%85%E9%A5%B0%E5%99%A8%E5%A0%86%E5%8F%A0"><span class="toc-number">6.3.4.</span> <span class="toc-text">形式4：多个装饰器堆叠</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A3%85%E9%A5%B0%E5%99%A8%E5%9C%A8%E5%AE%9E%E9%99%85%E9%A1%B9%E7%9B%AE%E4%B8%AD%E7%9A%84%E5%BA%94%E7%94%A8"><span class="toc-number">6.4.</span> <span class="toc-text">装饰器在实际项目中的应用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%97%A5%E5%BF%97%E8%AE%B0%E5%BD%95"><span class="toc-number">6.4.1.</span> <span class="toc-text">日志记录</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9D%83%E9%99%90%E9%AA%8C%E8%AF%81"><span class="toc-number">6.4.2.</span> <span class="toc-text">权限验证</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%93%E5%AD%98%E8%AE%B0%E5%BF%86%E5%8C%96"><span class="toc-number">6.4.3.</span> <span class="toc-text">缓存&#x2F;记忆化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%87%8D%E8%AF%95%E6%9C%BA%E5%88%B6"><span class="toc-number">6.4.4.</span> <span class="toc-text">重试机制</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BD%BF%E7%94%A8-functools.wraps"><span class="toc-number">6.5.</span> <span class="toc-text">使用 functools.wraps</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E5%AE%83"><span class="toc-number">6.5.1.</span> <span class="toc-text">为什么需要它？</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A3%85%E9%A5%B0%E5%99%A8%E7%9A%84%E5%BA%95%E5%B1%82%E5%8E%9F%E7%90%86"><span class="toc-number">6.6.</span> <span class="toc-text">装饰器的底层原理</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A3%85%E9%A5%B0%E5%99%A8%E7%9A%84%E6%89%A7%E8%A1%8C%E6%97%B6%E6%9C%BA"><span class="toc-number">6.6.1.</span> <span class="toc-text">装饰器的执行时机</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%9C%A8%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B8%AD%E7%9A%84%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8"><span class="toc-number">6.7.</span> <span class="toc-text">在机器学习中的实际应用</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E8%A3%85%E9%A5%B0%E5%99%A8"><span class="toc-number">6.7.1.</span> <span class="toc-text">模型训练装饰器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E6%A3%80%E6%9F%A5%E8%A3%85%E9%A5%B0%E5%99%A8"><span class="toc-number">6.7.2.</span> <span class="toc-text">梯度检查装饰器</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A3%85%E9%A5%B0%E5%99%A8%E7%9A%84%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98"><span class="toc-number">6.8.</span> <span class="toc-text">装饰器的常见问题</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%97%AE%E9%A2%981%E8%A3%85%E9%A5%B0%E5%99%A8%E7%A0%B4%E5%9D%8F%E4%BA%86%E5%87%BD%E6%95%B0%E7%AD%BE%E5%90%8D"><span class="toc-number">6.8.1.</span> <span class="toc-text">问题1：装饰器破坏了函数签名</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%97%AE%E9%A2%982%E8%A3%85%E9%A5%B0%E5%99%A8%E4%B8%8D%E8%83%BD%E8%A3%85%E9%A5%B0%E7%B1%BB%E6%96%B9%E6%B3%95"><span class="toc-number">6.8.2.</span> <span class="toc-text">问题2：装饰器不能装饰类方法</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%97%AE%E9%A2%983%E8%A3%85%E9%A5%B0%E5%99%A8%E5%BD%B1%E5%93%8D%E6%80%A7%E8%83%BD"><span class="toc-number">6.8.3.</span> <span class="toc-text">问题3：装饰器影响性能</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%A3%85%E9%A5%B0%E5%99%A8%E7%9A%84%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5"><span class="toc-number">6.9.</span> <span class="toc-text">装饰器的最佳实践</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A7%8B%E7%BB%88%E4%BD%BF%E7%94%A8-functools.wraps"><span class="toc-number">6.9.1.</span> <span class="toc-text">始终使用 functools.wraps</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BC%96%E5%86%99%E5%8F%AF%E9%87%8D%E7%94%A8%E7%9A%84%E8%A3%85%E9%A5%B0%E5%99%A8"><span class="toc-number">6.9.2.</span> <span class="toc-text">编写可重用的装饰器</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%A3%85%E9%A5%B0%E5%99%A8%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F"><span class="toc-number">6.9.3.</span> <span class="toc-text">装饰器工厂模式</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%80%BB%E7%BB%93-1"><span class="toc-number">6.10.</span> <span class="toc-text">总结</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#model.train-model.eval"><span class="toc-number">6.11.</span> <span class="toc-text">model.train() model.eval()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%B9%BF%E6%92%AD%E6%9C%BA%E5%88%B6broadcasting"><span class="toc-number">6.12.</span> <span class="toc-text">广播机制（Broadcasting）</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%B9%BF%E6%92%AD%E8%A7%84%E5%88%99"><span class="toc-number">6.12.1.</span> <span class="toc-text">广播规则</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%84%E5%88%991%E7%BB%B4%E5%BA%A6%E5%AF%B9%E9%BD%90%E4%BB%8E%E5%8F%B3%E5%90%91%E5%B7%A6"><span class="toc-number">6.12.1.1.</span> <span class="toc-text">规则1：维度对齐（从右向左）</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%84%E5%88%992%E5%85%BC%E5%AE%B9%E6%80%A7%E5%88%A4%E6%96%AD"><span class="toc-number">6.12.1.2.</span> <span class="toc-text">规则2：兼容性判断</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%A7%84%E5%88%993%E6%89%A9%E5%B1%95%E6%89%A7%E8%A1%8C"><span class="toc-number">6.12.1.3.</span> <span class="toc-text">规则3：扩展执行</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#unsqueeze-squeeze"><span class="toc-number">6.13.</span> <span class="toc-text">unsqueeze() squeeze()</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#view-reshape"><span class="toc-number">6.14.</span> <span class="toc-text">view() reshape()</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%85%B6%E4%BB%96%E6%94%B9%E5%8F%98%E5%BD%A2%E7%8A%B6%E7%9A%84api"><span class="toc-number">6.14.1.</span> <span class="toc-text">其他改变形状的API</span></a></li></ol></li></ol></li></ol></div></div><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2026/01/31/STL%E6%8B%BE%E9%81%97/" title="STL拾遗">STL拾遗</a><time datetime="2026-01-31T12:50:18.000Z" title="发表于 2026-01-31 20:50:18">2026-01-31</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/12/07/%E4%BB%8E0%E5%AE%9E%E7%8E%B0LLM/" title="从0实现Transformer">从0实现Transformer</a><time datetime="2025-12-07T13:07:22.000Z" title="发表于 2025-12-07 21:07:22">2025-12-07</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/11/27/pytorch%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%AE%9E%E8%B7%B5/" title="Pytorch深度学习实践">Pytorch深度学习实践</a><time datetime="2025-11-27T14:15:20.000Z" title="发表于 2025-11-27 22:15:20">2025-11-27</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/11/18/cuda%E7%BC%96%E7%A8%8B%E5%85%A5%E9%97%A8/" title="cuda编程入门">cuda编程入门</a><time datetime="2025-11-18T12:46:44.000Z" title="发表于 2025-11-18 20:46:44">2025-11-18</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/2025/10/01/java%E5%9F%BA%E7%A1%80/" title="javaSE">javaSE</a><time datetime="2025-10-01T14:43:52.000Z" title="发表于 2025-10-01 22:43:52">2025-10-01</time></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('https://pic1.zhimg.com/80/v2-908b61a41ec4bebe17a04468dcf5d834_720w.webp')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2026 By klklkl</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="单栏和双栏切换"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [['$', '$'], ['\\(', '\\)']],
      tags: 'ams'
    },
    chtml: {
      scale: 1.1
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, '']
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax/es5/tex-mml-chtml.min.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typesetPromise()
}</script></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>